[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Zoombinis - Allergic Cliffs\n\n\n\nFun\n\n\nZoombinis\n\n\nVroomy Code\n\n\n\n\nBailey Andrew\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKL-Divergence of the Dirichlet Distribution\n\n\n\nWork\n\n\nCompositional\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed antGLasso\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolyglot Programming 2\n\n\n\nUseful\n\n\nPolyglot Programming\n\n\n\n\nBailey Andrew\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Road to Polyglot Programming with Jupyter\n\n\n\nUseful\n\n\nPolyglot Programming\n\n\n\n\nBailey Andrew\n\n\nFeb 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo Victory\n\n\n\nFun\n\n\nGo\n\n\n\n\nBailey Andrew\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Chicken 3 Furious\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nFeb 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChicken 2: Cock-a-doodle-doo\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA tale of two chickens\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttempting to Customize The ls Command\n\n\n\nLife\n\n\n\n\nBailey Andrew\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning My Computer\n\n\n\nLife\n\n\n\n\nBailey Andrew\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKronecker Sum Projections\n\n\n\nWork\n\n\nUseful\n\n\nVroomy Code\n\n\n\n\nBailey Andrew\n\n\nJan 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeing Naughty with Numpy\n\n\n\nFun\n\n\n\n\nBailey Andrew\n\n\nJan 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlideshow\n\n\n\nBlogging\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nJan 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Living Biobank of Ovarian Cancer – Figure 5b-c\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Living Biobank of Ovarian Cancer – Figure 5a\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Living Biobank of Ovarian Cancer – Data Acqusition\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex Optimization Taster\n\n\n\nWork\n\n\nUseful\n\n\nOptimization\n\n\n\n\nBailey Andrew\n\n\nJan 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Replication on Sparse Microbial Networks – Measuring the Network\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper Replication on Sparse Microbial Networks – Creating the Network\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is 16s rRNA?\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnter the Simplex\n\n\n\nWork\n\n\nCompositional\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of scRNA-seq Data Creation – Sequencing Protocol\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing scRNA-seq Data – Exploration\n\n\n\nWork\n\n\nOmics\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Ensembl\n\n\n\nWork\n\n\nUseful\n\n\nOmics\n\n\n\n\nBailey Andrew\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollapsable Multi-Cell Blocks in Quarto Output of .ipynb\n\n\n\nLife\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing scRNA-seq Data – Dataset Acquisition\n\n\n\nWork\n\n\nUseful\n\n\nOmics\n\n\nPaper Replication\n\n\n\n\nBailey Andrew\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoulders in Valleys Tips and Tricks\n\n\n\nFun\n\n\n\n\nBailey Andrew\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of scRNA-seq Data Creation – Sequencing\n\n\n\nWork\n\n\nUseful\n\n\nOmics\n\n\n\n\nBailey Andrew\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of scRNA-seq Data Creation – Cell Isolation\n\n\n\nWork\n\n\nUseful\n\n\nOmics\n\n\n\n\nBailey Andrew\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConferences\n\n\n\nWork\n\n\n\n\nBailey Andrew\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a Slitherlink level generator\n\n\n\nFun\n\n\n\n\nBailey Andrew\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to add comments to your blog\n\n\n\nLife\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a blog\n\n\n\nLife\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nDec 13, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_002.html",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_002.html",
    "title": "Boulders in Valleys 002",
    "section": "",
    "text": "This puzzle was originally posted to the Puzzling Stack Exchange under the name “Balls in Pockets”"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_002.html#puzzle",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_002.html#puzzle",
    "title": "Boulders in Valleys 002",
    "section": "Puzzle",
    "text": "Puzzle\n\n\n\nThe Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_002.html#rules",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_002.html#rules",
    "title": "Boulders in Valleys 002",
    "section": "Rules:",
    "text": "Rules:\n\nCurve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints\n\n\n\nBoulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve.\n\n\n\nMechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_001.html",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_001.html",
    "title": "Boulders in Valleys 001",
    "section": "",
    "text": "This puzzle was originally posted to the Puzzling Stack Exchange under the name “Balls on Hills”"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_001.html#puzzle",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_001.html#puzzle",
    "title": "Boulders in Valleys 001",
    "section": "Puzzle",
    "text": "Puzzle\n\n\n\nThe Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_001.html#rules",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_001.html#rules",
    "title": "Boulders in Valleys 001",
    "section": "Rules:",
    "text": "Rules:\n\nCurve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints\n\n\n\nBoulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve.\n\n\n\nMechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_004.html",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_004.html",
    "title": "Boulders in Valleys 004",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_004.html#puzzle",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_004.html#puzzle",
    "title": "Boulders in Valleys 004",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_004.html#rules",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_004.html#rules",
    "title": "Boulders in Valleys 004",
    "section": "Rules:",
    "text": "Rules:\n\nCurve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints\n\n\n\nBoulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve.\n\n\n\nMechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section",
    "title": "Journal Club",
    "section": "",
    "text": "Many ‘omics’: genomics, transcriptomics, proteomics, etc…"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-1",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-1",
    "title": "Journal Club",
    "section": "",
    "text": "Cells divide and accrue mutations"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-2",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-2",
    "title": "Journal Club",
    "section": "",
    "text": "Things can go wrong at the omics level"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-3",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-3",
    "title": "Journal Club",
    "section": "",
    "text": "Many diseases are caused by an omics issue\n\n\n\nWhen so, other data is a proxy1\n\n\n\n\nheterogenous tumors?: single-cell omics\n\n\n\n\nSymptoms may be caused by omics issues\n\nProteomics enables better design of treatments?\n\n\n\n\n1) Well, environmental factors can be a cause, but limited ability to affect them"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#hands-on-learning-experience",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#hands-on-learning-experience",
    "title": "Journal Club",
    "section": "Hands-on Learning Experience™",
    "text": "Hands-on Learning Experience™\n\nhttps://www.ebi.ac.uk/gxa/sc/experiments/E-MTAB-8559/"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---where",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---where",
    "title": "Journal Club",
    "section": "Dataset Details - Where?",
    "text": "Dataset Details - Where?\n\nTo build a living biobank, we established a biopsy pipeline, collecting samples from patients diagnosed with epithelial ovarian cancer treated at the Christie Hospital.\n– Nelson et al. (2020)\n\n(Christie Hospital is in Manchester)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---who",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---who",
    "title": "Journal Club",
    "section": "Dataset Details - Who?",
    "text": "Dataset Details - Who?\n\nBetween May 2016 and June 2019, we collected 312 samples from patients with chemo-naïve and relapsed disease, either as solid biopsies or as ascites (Fig. 1a)\nTen patients had HGSOC while two had mucinous ovarian carcinoma. Longitudinal biopsies were collected from three patients.\n– Nelson et al. (2020)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---how",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#dataset-details---how",
    "title": "Journal Club",
    "section": "Dataset Details - How?",
    "text": "Dataset Details - How?\n\nThe primer contains:\n\nan Illumina TruSeq Read 1 (read 1 sequencing primer)\n16 nt 10x Barcode\n12 nt unique molecular identifier (UMI)\n30 nt poly(dT) sequence\n\nBarcoded, full-length cDNA is amplified via PCR to generate sufficient mass for library construction.\n– Nelson et al. (2020)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data",
    "title": "Journal Club",
    "section": "Load the Data",
    "text": "Load the Data\n\nfile.path = './localdata/E-MTAB-8559-quantification-raw-files/'\nraw.counts &lt;- as(Matrix::readMM(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx',\n        sep=''\n    )\n), 'CsparseMatrix')\ndim(raw.counts)\n\n\n2328419880"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data-1",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data-1",
    "title": "Journal Club",
    "section": "Load the Data",
    "text": "Load the Data\n\nrow.info &lt;- read.table(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx_rows',\n        sep=''\n    ),\n    header=FALSE,\n    col.names=c(\"Ensembl.ID\", \"Redundant\")\n)\n\n# Drop duplicate field in row.info\nrow.info &lt;- row.info['Ensembl.ID']\nrownames(raw.counts) &lt;- row.info$Ensembl.ID\n\n\n\n       Ensembl.ID\n1 ENSG00000000003\n2 ENSG00000000419\n3 ENSG00000000457\n4 ENSG00000000460\n5 ENSG00000000938\n6 ENSG00000000971"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data-2",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-data-2",
    "title": "Journal Club",
    "section": "Load the Data",
    "text": "Load the Data\n\ncol.info &lt;- read.table(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx_cols',\n        sep=''\n    ),\n    header=FALSE,\n    col.names=c('Cell.ID')\n)\ncolnames(raw.counts) &lt;- col.info$Cell.ID\n\n\n\n                        Cell.ID\n1 SAMEA6492740-AAACCCACAGTTAGGG\n2 SAMEA6492740-AAACCCACATGTGTCA\n3 SAMEA6492740-AAACCCAGTCGCATGC\n4 SAMEA6492740-AAACCCAGTCTTTCAT\n5 SAMEA6492740-AAACCCATCCGTGTCT\n6 SAMEA6492740-AAACCCATCCTCTCTT"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-metadata",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#load-the-metadata",
    "title": "Journal Club",
    "section": "Load the Metadata",
    "text": "Load the Metadata\n\n# Load in the experimental design matrix\nexp.design.table &lt;- read.table(\n    './localdata/ExpDesign-E-MTAB-8559.tsv',\n    header=TRUE,\n    sep='\\t'\n)\n\n# We can see we have four patients in our dataset\nprint(unique(exp.design.table$Sample.Characteristic.individual))\n\n[1] \"38b\"  \"59\"   \"74-1\" \"79\""
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-4",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-4",
    "title": "Journal Club",
    "section": "",
    "text": "# For reproducibility\nset.seed(0)\n\n# Convenient data container\nlibrary(SingleCellExperiment)\n\n# Libraries for scRNA analysis\nlibrary(scran)\nlibrary(scater)\n\n# Library for cluster analysis\nlibrary(bluster)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-5",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-5",
    "title": "Journal Club",
    "section": "",
    "text": "# Store as a SingleCellExperiment object for convenience\novarian.sce &lt;- SingleCellExperiment(\n    assays=list(counts=raw.counts),\n)\n\n# Add patient metadata\novarian.sce$patient &lt;- exp.design.table$Sample.Characteristic.individual.\n\n# Take a peak at internal data structure\novarian.sce\n\nclass: SingleCellExperiment \ndim: 23284 19880 \nmetadata(0):\nassays(1): counts\nrownames(23284): ENSG00000000003 ENSG00000000419 ... ENSG00000289701\n  ENSG00000289716\nrowData names(0):\ncolnames(19880): SAMEA6492740-AAACCCACAGTTAGGG\n  SAMEA6492740-AAACCCACATGTGTCA ... SAMEA6492743-TTTGTTGGTCCTGGTG\n  SAMEA6492743-TTTGTTGTCAGATTGC\ncolData names(1): patient\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#why-so-much-preprocessing",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#why-so-much-preprocessing",
    "title": "Journal Club",
    "section": "Why So Much Preprocessing?",
    "text": "Why So Much Preprocessing?\n\nELI5: Wet-lab work is hard"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-6",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-6",
    "title": "Journal Club",
    "section": "",
    "text": "ovarian.sce &lt;- scuttle::logNormCounts(ovarian.sce)\novarian.sce\n\nclass: SingleCellExperiment \ndim: 23284 19880 \nmetadata(0):\nassays(2): counts logcounts\nrownames(23284): ENSG00000000003 ENSG00000000419 ... ENSG00000289701\n  ENSG00000289716\nrowData names(0):\ncolnames(19880): SAMEA6492740-AAACCCACAGTTAGGG\n  SAMEA6492740-AAACCCACATGTGTCA ... SAMEA6492743-TTTGTTGGTCCTGGTG\n  SAMEA6492743-TTTGTTGTCAGATTGC\ncolData names(2): patient sizeFactor\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-7",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-7",
    "title": "Journal Club",
    "section": "",
    "text": "ovarian.sce &lt;- scran::fixedPCA(\n    ovarian.sce,\n    rank=14,\n    subset.row=NULL,\n    assay.type=\"logcounts\"\n)\n\n\n\novarian.sce &lt;- scater::runTSNE(\n    ovarian.sce,\n    dimred=\"PCA\"\n)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-8",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-8",
    "title": "Journal Club",
    "section": "",
    "text": "plotReducedDim(ovarian.sce, dimred=\"TSNE\", colour_by=\"patient\")"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#identifying-stromals",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#identifying-stromals",
    "title": "Journal Club",
    "section": "Identifying stromals",
    "text": "Identifying stromals\n\novarian.sce$tumor.stromal.clusters &lt;- scran::clusterCells(\n    ovarian.sce,\n    use.dimred=\"TSNE\",\n    BLUSPARAM=bluster::KmeansParam(centers=5)\n)\n\n\n\n# We'll assign clusters by eye since it's obvious\novarian.sce$tumor.or.stromal &lt;- \"Stromal\"\novarian.sce$tumor.or.stromal[ovarian.sce$tumor.stromal.clusters==1] &lt;- \"Tumor.1\"\novarian.sce$tumor.or.stromal[ovarian.sce$tumor.stromal.clusters==2] &lt;- \"Tumor.2\"\novarian.sce$tumor.or.stromal[ovarian.sce$tumor.stromal.clusters==4] &lt;- \"Tumor.3\"\novarian.sce$tumor.or.stromal[ovarian.sce$tumor.stromal.clusters==5] &lt;- \"Tumor.4\""
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-9",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-9",
    "title": "Journal Club",
    "section": "",
    "text": "plotReducedDim(\n    ovarian.sce,\n    \"TSNE\",\n    colour_by=\"tumor.stromal.clusters\",\n    text_by=\"tumor.or.stromal\",\n    shape_by=\"patient\"\n)"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-10",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-10",
    "title": "Journal Club",
    "section": "",
    "text": "marker.info &lt;- scran::scoreMarkers(\n    ovarian.sce,\n    ovarian.sce$tumor.stromal.clusters\n)\n\n\ncluster &lt;- \"3\" # the stromal cluster\ndiff.exp.gene &lt;- rownames(ovarian.sce)[\n    order(marker.info[[cluster]]$mean.AUC, decreasing=TRUE)[[1]]\n]\nprint(diff.exp.gene)\n\n[1] \"ENSG00000106366\"\n\n\n\nplotReducedDim(\n    ovarian.sce,\n    \"TSNE\",\n    colour_by=diff.exp.gene,\n    shape_by=\"patient\",\n    text_by=\"tumor.or.stromal\",\n    text_colour=\"cyan\"\n) +\n    ggtitle(\"Ovarian Cancer Cells by SERPINE1 (ENSG00000106366) Expression\") +\n    theme(\n        panel.background = element_rect(fill = \"darkblue\"),\n        plot.background = element_rect(fill = \"cyan\")\n    )"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#serpine1",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#serpine1",
    "title": "Journal Club",
    "section": "SERPINE1",
    "text": "SERPINE1\n\nPAI-1, the protein encoded by SERPINE1, is related to cancer!\n\nForm hypotheses to guide future experiments:\n\nCan SERPINE1 be a diagnostic factor?\nIs it a cause or symptom of cancer?\nIs it important for tumor health?\nDoes it affect patient wellbeing or outcomes?\nCan drugs be designed to target it?"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-12",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-12",
    "title": "Journal Club",
    "section": "",
    "text": "Go to the Single Cell Expression Atlas\n\n\n\nPick an interesting dataset\n\nDownload it\nRead the paper\n\n\n\n\n\nRead the Bioconductor scRNA ebooks!!!\n\nPerhaps the best educational resource I’ve used for anything\nPractical and conceptual"
  },
  {
    "objectID": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-13",
    "href": "900_Slideshows/Journal-Club-2023-Jan-27/journal-club.html#section-13",
    "title": "Journal Club",
    "section": "",
    "text": "Recreate their preprocessing steps\n\n\n\nPick a figure in the paper and recreate it\n\nWe recreated Figure 5a from Nelson et al. (2020)."
  },
  {
    "objectID": "700_Static/Puzzles/4-by-4-sudoku.html",
    "href": "700_Static/Puzzles/4-by-4-sudoku.html",
    "title": "All the 4x4 Sudoku Puzzles",
    "section": "",
    "text": "We can split a sudoku puzzle into three parts:\n\nThe solution\nThe clues\nThe grid\n\nA solution to a 4x4 might look like:\n12 34\n23 41\n\n34 12\n41 23\nThe clues are the subset of the solution that we are allowed to see.\nFinally, the grid is typically just a subdivision into 4 boxes covering 4 cells each.\n\n\n\nThe grid here is made up of two L and two Z shaped boxes."
  },
  {
    "objectID": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-solutions-are-there",
    "href": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-solutions-are-there",
    "title": "All the 4x4 Sudoku Puzzles",
    "section": "How many solutions are there?",
    "text": "How many solutions are there?\nIt’s not difficult, but hella annoying, to work out that there are 12. This corresponds to the following OEIS sequence. Ignoring the grid restrictions sudoku is just a “Latin Square” puzzle; fit the numbers 1 to n in an nxn square without having the same number twice in a column/row. There are many possible 4x4 latin squares (576, to be precise).\nHowever, it doesn’t really feel like rotating a latin square by 90° should produce a new latin square - the two squares are still essentially the same. More precisely, we want to account for symmetry. There are 4 types of symmetries that a latin square can have:\n\nRotational (4 different types)\nReflectional (2 different types)\n\nIt might feel like there are more, depending on your axis of reflection - but we can account for this using the rotations, so we really only need one axis of reflection\n\nPermutational (24 different types)\n\nWe could easily swap every occurance of 1 with 2 and vice versa, but the square is still “the same”. This is permutational symmetry, in which we change which numbers are which but not where they are.\n(24 = 4!)\n\nRow-permutational (24 different types)\n\nWe can change the order of the rows without changing the latin square\nColumn permutations can be done using a rotation first\n\n\nThe first three symmetries all still apply when we add a grid, but the fourth one could destroy our grid by splitting one of our boxes into two disconnected components. Typically we don’t allow this, so I’m treating puzzles that are row-permutationally equivalent to be different.\nIt turns out that there are only 4 different latin squares, accounting for all 4 symmetries - but when we ignore row-permutational symmetries, there are 12 different latin squares. To my knowledge, these are not enumerated anywhere on the internet.\n\n\n\nThe 12 Sudokus\n\n\nIn the above image, I wrote the solutions in a number-agnostic way to emphasize the permutation symmetry."
  },
  {
    "objectID": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-grids-are-there",
    "href": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-grids-are-there",
    "title": "All the 4x4 Sudoku Puzzles",
    "section": "How many grids are there?",
    "text": "How many grids are there?\nA connected box made of 4 cells is called a “tetromino” (tetra=4, omino as in domino for the 2-cell case). These are exactly the pieces that you use in tetris! The question of how many grids there are can be rephrased as “how many ways can a 4x4 square be decomposed into tetrominos”? For this, I refer the reader to the mayhematics site which has a nice writeup. These are the grids:\n\n\n\nThe Grids\n\n\nUnlike the solutions, we do need to take into account symmetric versions of the grids - this is because we want to combine solutions and grids, so we need to combine them in all orientations. We could have instead done this with the solutions, and only considered the 22 fundamental grids. Instead we have 117 grids, although I won’t write them all out."
  },
  {
    "objectID": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-solution-grid-pairs-are-there",
    "href": "700_Static/Puzzles/4-by-4-sudoku.html#how-many-solution-grid-pairs-are-there",
    "title": "All the 4x4 Sudoku Puzzles",
    "section": "How many solution-grid pairs are there?",
    "text": "How many solution-grid pairs are there?\nThis gives us 12 * 117 = 1404 possible pairs… except that not every grid is compatible with every solution. The first solution I gave is incompatible with the first grid, for example.\nTO BE CONTINUED"
  },
  {
    "objectID": "otherblogs.html",
    "href": "otherblogs.html",
    "title": "Other People’s Blogs",
    "section": "",
    "text": "Sam’s Blog\nMary’s Website"
  },
  {
    "objectID": "000_Blog/011_math/allergic-cliffs.html",
    "href": "000_Blog/011_math/allergic-cliffs.html",
    "title": "Zoombinis - Allergic Cliffs",
    "section": "",
    "text": "from dataclasses import dataclass\nfrom typing import Callable, Optional, Tuple\nimport random\nGrowing up, I played a lot of Zoombinis. Zoombinis is well-known for teaching maths, but I think most people talk more about the “high-concept” maths that it teaches, i.e. set theory. I haven’t seen much discussion on the algorithmics that it teaches.\nThe topic of this blog post is the first level of Zoombinis. In it, you have a set of Zoombinis and two paths (cliffs) in front of you. Each Zoombini can travel across exactly one path - but you don’t know which one until you try it! Which path they can traverse is based on the features of the Zoombinis - on the easiest difficulty, one of the cliffs is “allergic” to Zoombinis with exactly one type of trait, and the other is allergic to all other Zoombinis.\nWe can talk about how it is teaching basic set theory (complements and unions - depending on difficulty level), but that does not interest me that much. Rather, I’m interested in how to efficiently solve the level. There are multiple ways to define efficiency:\nSince the problem size for Zoombinis is fixed (at most 16 Zoombinis per level), considering asymptotic complexity is not of much interest to me currently. Also, I’m most interested in algorithms humans would actually implement, in which case the time sink is the number of input and output queries."
  },
  {
    "objectID": "000_Blog/011_math/allergic-cliffs.html#level-1---not-so-easy",
    "href": "000_Blog/011_math/allergic-cliffs.html#level-1---not-so-easy",
    "title": "Zoombinis - Allergic Cliffs",
    "section": "Level 1 - Not So Easy",
    "text": "Level 1 - Not So Easy\nOne cliff accepts all Zoombinis with a specific attribute (such as sunglasses), and the other cliff rejects them all."
  },
  {
    "objectID": "000_Blog/011_math/allergic-cliffs.html#level-2---oh-so-hard",
    "href": "000_Blog/011_math/allergic-cliffs.html#level-2---oh-so-hard",
    "title": "Zoombinis - Allergic Cliffs",
    "section": "Level 2 - Oh So Hard",
    "text": "Level 2 - Oh So Hard\nOne cliff accepts a Zoombini if it has one of two types of the same attribute (i.e. one cliff might accept either curly hair or a hat, but you won’t get a mix of accepting curly hair or sunglasses). It rejects all others."
  },
  {
    "objectID": "000_Blog/011_math/allergic-cliffs.html#level-3---very-hard",
    "href": "000_Blog/011_math/allergic-cliffs.html#level-3---very-hard",
    "title": "Zoombinis - Allergic Cliffs",
    "section": "Level 3 - Very Hard",
    "text": "Level 3 - Very Hard\nOne cliff accepts a Zoombini if it has one of two features, where the features are from different attributes (i.e. curly hair or sunglasses). It rejects all others."
  },
  {
    "objectID": "000_Blog/011_math/allergic-cliffs.html#level-4---very-very-hard",
    "href": "000_Blog/011_math/allergic-cliffs.html#level-4---very-very-hard",
    "title": "Zoombinis - Allergic Cliffs",
    "section": "Level 4 - Very, Very Hard",
    "text": "Level 4 - Very, Very Hard\nSame as before, but with three features instead of two. In the original game, it is apparently impossible to guarantee you will deduce the rule within the amount of guesses alloted to you - although I don’t know the details of this fact."
  },
  {
    "objectID": "000_Blog/2022mondec19.html",
    "href": "000_Blog/2022mondec19.html",
    "title": "How to make a Slitherlink level generator",
    "section": "",
    "text": "How to make a Slitherlink level generator\nSlitherlink is a fun logic puzzle played on a grid; a good website to try it is Krazydad, or if you have a tablet I would recommend the Slitherlink app by Conceptis.\n\n\n\nExample\n\n\nAbove is an in-progress example of a Slitherlink puzzle.\nIt has the following rules:\n\nIf there is a number X in a cell, there must be exactly X filled-in edges around the number.\nAll the filled-in edges in a puzzle must join together to form a single continuous loop.\n\nA good Slitherlink puzzle comes with the guarantee that there is a unique solution. The above example has the solution:\n\n\n\nSolved Example\n\n\nFrom the perspective of a Slitherlink generator, every puzzle has two components:\n\nThe solution\nThe hints\n\nGiven a solution, it is trivial to fill the grid with maximal hints - some process could be used to then remove hints iteratively to increase the difficulty of the puzzle, if desired.\nEven without such a process we would still have a valid, if easy, Slitherlink puzzle; thus we should focus on generating a solution first. Instead of trying to generate the loop itself, we can think of the loop as being the boundary between some shape and the outside of the grid.\nAs a first attempt, we will just generate a completely random ‘shape’. It will probably be disconnected and hence an invalid solution; this is just temporary to test the puzzle rendering.\nconda install conda-forge::numpy=1.23.5\nconda install conda-forge::matplotlib=3.6.2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef random_slitherlink(\n    shape: \"Two element tuple\"\n) -&gt; \"0/1 numpy array with shape `shape`\":\n    return np.random.randint(0, 2, shape)\nslither = random_slitherlink((7, 7))\nslither\n\narray([[1, 1, 0, 0, 1, 0, 0],\n       [1, 0, 1, 1, 0, 1, 0],\n       [0, 1, 1, 0, 0, 0, 1],\n       [1, 1, 0, 0, 1, 0, 0],\n       [0, 1, 0, 1, 1, 1, 1],\n       [0, 1, 1, 0, 1, 0, 1],\n       [0, 1, 0, 1, 1, 1, 0]])\n\n\n\ndef display_slitherlink_solution(\n    grid: \"0/1 numpy array\"\n) -&gt; \"(fig, ax) tuple for plotted figure\":\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(grid)\n    return (fig, ax)\ndisplay_slitherlink_solution(slither)\npass\n\n\n\n\nCreating a good solution turns out to be a bit difficult, and I made some false starts. Sam had the good idea of including false pathways in his blog as expandable dropdowns, which I will steal. This way, people can still follow my thought process if desired, but false pathways do not distract from the flow of the true solution.\n\n\nAttempt 1: Painting lines\n\nMy first idea was to choose a random cell, and draw a straight line at a random distance from it. Then, iteratively pick a random inside cell and create a new straight line (making sure to never intersect with an already created line).\nI didn’t quite finish the code for it (still some bugs - it’s possible for it to cut the outside into multiple parts diagonally), but I abandoned the path because I wasn’t happy with it - I had the idea for my next attempt, based on surface area, since I think it would be more elegant.\ndef random_slitherlink(\n    shape: \"Two element tuple\",\n    iters: int = 5\n) -&gt; \"Boolean numpy array with shape `shape`\":\n    # Initialize\n    slither = np.zeros(shape).astype(bool)\n    \n    # Maybe transpose it\n    transposed = np.random.randint(2)\n    if transposed:\n        slither = slither.T\n    \n    # Pick a random column to slither from\n    slither[\n        :np.random.randint(1, slither.shape[0]),\n        np.random.randint(slither.shape[1])\n    ] = True\n    \n    # Maybe flip it\n    if np.random.randint(2):\n        slither = slither[::-1]\n    \n    # Un-transpose it to preserve shape\n    if transposed:\n        slither = slither.T\n    \n    # Iteratively add slithers\n    for i in range(iters):\n        \n        # Choose cell to expand\n        choice = (\n            np.random.choice(\n                np.argwhere(slither.reshape(-1))[:, 0]\n            )\n        )\n        row = choice // shape[1]\n        col = choice % shape[1]\n        \n        # Always expand in the direction with the most space\n        column_slithers = np.argwhere(slither[row]).reshape(-1) - col\n        row_slithers = np.argwhere(slither[:, col]).reshape(-1) - row\n        column_slithers = column_slithers[column_slithers != 0]\n        row_slithers = row_slithers[row_slithers != 0]\n        \n        along_columns = True\n        \n        \n        # Find the direction with the largest possible extension\n        if len(row_slithers) == 0:\n            row_slithers = np.array([0, shape[0]]) - row\n        if len(column_slithers) == 0:\n            column_slithers = np.array([0, shape[1]]) - col\n        if np.abs(row_slithers).min() &lt; np.abs(column_slithers).min():\n            # More room to expand along columns\n            along_columns = True\n        else:\n            along_columns = False\n        \n        # Flip the solution if along columns\n        if along_columns:\n            slither = slither.T\n            row_slithers = column_slithers\n            temp = row\n            row = col\n            col = temp\n            \n        # Left or right\n        # Check if smallest-in-magnitude negative number\n        # is larger than smallest-in-magnitude positive number\n        # and if so, go left\n        left = np.random.randint(0, 2) == 0\n        if len(row_slithers) &gt; 0:\n            neg_sliths = -row_slithers[row_slithers &lt;= 0]\n            pos_sliths = row_slithers[row_slithers &gt;= 0]\n            \n            if len(neg_sliths) &gt; 0:\n                max_left = neg_sliths.min()\n            else:\n                max_left = 0\n                \n            if len(pos_sliths) &gt; 0:\n                max_right = pos_sliths.min()\n            else:\n                max_right = 0\n                \n            if (max_left &gt; max_right):\n                left = True\n            else:\n                left = False\n        else:\n            if along_columns:\n                slither = slither.T\n            continue\n                \n        max_slither = max_left if left else max_right\n        \n        offset = np.random.randint(0, max_slither)\n        if left:\n            slither[row:row-offset, col] = True\n        else:\n            slither[row:row+offset, col] = True\n            \n        # Reset the flipped solution\n        if along_columns:\n            slither = slither.T\n        \n        \n        \n    \n    return slither\ndisplay_slitherlink_solution(\n    random_slitherlink((7, 10), iters=100)\n)\npass\n\nMy next idea was based on circumference - large slitherlinks look very crumpled, and so intuitively they should have a large circumference compared to their area. Imagine the following process:\n\nStart with the entire grid enclosed in one loop\nPick a random cell to ‘fold’ inwards\n\nOnly pick cells that can be folded without creating two loops\nWeight the random pick by cells that add the most edges\n\nStop when we reach a prespecificed circumference-to-area ratio\n\nThis should, hopefully, produce random crumpled loops.\nTo elaborate on “only pick cells that won’t create two loops”, there are two scenarios we need to avoid:\n |1|        |1|_\n |x|        |x 1\n |1|        |1|‾\nIn neither scenario can we fold in the x - in the first, it cuts the loop in two, and in the second, it results in four edges at a corner:\n|1̲|_\n x̲|1̲\n|1|\nThe procedure I’ve described is conceptually simple, but it took quite a lot of code to implement 😅 There’re probably more elegant ways to program it\n\ndef get_neighbors(ck, shape):\n    neighbors = [\n        (ck[0], ck[1]-1),\n        (ck[0], ck[1]+1),\n        (ck[0]-1, ck[1]),\n        (ck[0]+1, ck[1])\n    ]\n    neighbors = [\n        neighbor for neighbor in neighbors\n        if neighbor[0] &gt;= 0\n        and neighbor[0] &lt; shape[0]\n        and neighbor[1] &gt;= 0\n        and neighbor[1] &lt; shape[1]\n    ]\n    return neighbors\n\ndef get_neighbors_with_diags(ck, shape):\n    neighbors = [\n        (\n            (ck[0]-1, ck[1]-1),\n            (\n                (ck[0], ck[1]-1),\n                (ck[0]-1, ck[1])\n            )\n        ),\n        (\n            (ck[0]-1, ck[1]+1),\n            (\n                (ck[0], ck[1]+1),\n                (ck[0]-1, ck[1])\n            )\n        ),\n        (\n            (ck[0]+1, ck[1]-1),\n            (\n                (ck[0], ck[1]-1),\n                (ck[0]+1, ck[1])\n            )\n        ),\n        (\n            (ck[0]+1, ck[1]+1),\n            (\n                (ck[0], ck[1]+1),\n                (ck[0]+1, ck[1])\n            )\n        )\n    ]\n    def valid(neigh, shape):\n        return (\n            neigh[0] &gt;= 0\n            and neigh[0] &lt; shape[0]\n            and neigh[1] &gt;= 0\n            and neigh[1] &lt; shape[1]\n        )\n        \n    neighbors = [\n        (dia, (neigh1, neigh2)) for (dia, (neigh1, neigh2)) in neighbors\n        if valid(neigh1, shape) and valid(neigh2, shape)\n    ]\n    return neighbors\n\ndef border_cell_is_valid(\n    border_cell: ((int, int), int),\n    solution: \"0/1 numpy array with 1s being inside cells\",\n    verbose: bool = False\n) -&gt; bool:\n    # Since as things change places that used to be border cells\n    # are no longer border cells, we should return False if\n    # it is no longer inside the loop\n    cell = border_cell[0]\n    shape = solution.shape\n    if solution[cell] == 0:\n        if verbose: print(\"empty\")\n        return False\n    \n    # Can't fold inwards if one of the new corners\n    # borders an empty cell\n    # I.e. can't be diagonally adjacent from an empty cell unless\n    # there is an empty cell already between them.\n    for dia, (neigh1, neigh2) in \\\n        get_neighbors_with_diags(cell, shape):\n        \n        if solution[dia] == 0:\n            if solution[neigh1] == 1 and solution[neigh2] == 1:\n                if verbose: print(\"dia\")\n                return False\n            \n    # Can't fold inwards if like:\n    #\n    #  0 1 0\n    #  0 * 0\n    #  0 1 0\n    # \n    # Note: if I remove the last two conditions from the following if statements,\n    # then it is the start of a nurikabe generator!\n    if (\n        # Above is a 0\n        (cell[0]-1 &lt; 0 or solution[cell[0]-1, cell[1]] == 0)\n        # Below is a 0\n        and (cell[0]+1 &gt;= shape[0] or solution[cell[0]+1, cell[1]] == 0)\n        # Left is a 1\n        and (cell[1]-1 &gt;= 0 and solution[cell[0], cell[1]-1] == 1)\n        # Right is a 1\n        and (cell[1]+1 &lt; shape[1] and solution[cell[0], cell[1]+1] == 1)\n    ):\n            if verbose: print(\"pinch ab\")\n            return False\n    if (\n        # Left is a 0\n        (cell[1]-1 &lt; 0 or solution[cell[0], cell[1]-1] == 0)\n        # Right is a zero\n        and (cell[1]+1 &gt;= shape[1] or solution[cell[0], cell[1]+1] == 0)\n        # Above is a 1\n        and (cell[0]-1 &gt;= 0 and solution[cell[0]-1, cell[1]] == 1)\n        # Below is a 1\n        and (cell[0]+1 &lt; shape[0] and solution[cell[0]+1, cell[1]] == 1)\n    ):\n            if verbose: print(\"pinch lr\")\n            return False\n    \n    # Can't be valid to remove if it has no inside adjacents\n    # This shouldn't trigger except in trivial cases\n    if border_cell[1] == 0:\n        return False\n    return True\n\n\ndef random_slitherlink(\n    shape: (\"height\", \"width\"),\n    target_ratio: float = 2,\n    area_ratio: float = 0.5\n) -&gt; \"0/1 numpy array with shape `shape`\":\n    \n    # Initialize to full loop\n    solution = np.ones(shape)\n    \n    # Get all cells bordering\n    border_cells: (dict, \"(x, y): # outside cells it borders\")\n    border_cells = set(\n        [(0, j) for j in range(shape[1])]\n        + [(shape[0]-1, j) for j in range(shape[1])]\n        + [(i, 0) for i in range(shape[0])]\n        + [(i, shape[1]-1) for i in range(shape[0])]\n    )\n    border_cells = {\n        cell: 3\n        for cell in border_cells\n    }\n    border_cells[(0, 0)] = 2\n    border_cells[(shape[0]-1, 0)] = 2\n    border_cells[(0, shape[1]-1)] = 2\n    border_cells[(shape[0]-1, shape[1]-1)] = 2\n    total_weight = sum(border_cells.values())\n    \n    # Initialize circ-area ratio\n    circumference = 2 * (shape[0] + shape[1])\n    max_area = shape[0] * shape[1]\n    area = max_area\n    circ_area_ratio = circumference / area\n    \n    while circ_area_ratio &lt; target_ratio and area / max_area &gt; area_ratio:\n        # Pick a random border_cell, by weight\n        border_cells_by_weight = {\n            key: value\n            for key, value in border_cells.items()\n            if border_cell_is_valid((key, value), solution)\n        }\n        total_weight = sum(border_cells_by_weight.values())\n        border_cells_by_weight = {\n            key: value / total_weight\n            for key, value in border_cells_by_weight.items()\n        }\n        if len(border_cells_by_weight) == 0:\n            print(\"Out of valid cells to remove\")\n            break\n        rando = np.random.random()\n        chosen_key: \"The cell to fold in on itself\" = None\n        for key, value in border_cells_by_weight.items():\n            rando -= value\n            if rando &lt;= 0:\n                chosen_key = key\n                break\n                \n        if chosen_key is None:\n            raise Exception(\"No key chosen\")\n        \n        if solution[chosen_key] == 0:\n            raise Exception(\"Cell removed twice\")\n        solution[chosen_key] = 0\n        inner_neighbors = border_cells[chosen_key]\n        \n        # Update neighbors\n        neighbors = get_neighbors(chosen_key, shape)\n        \n        # Every adjacent cell should loose 1 from its value\n        for neighbor in neighbors:\n            if neighbor not in border_cells:\n                neigh_neighbors = get_neighbors(neighbor, shape)\n                border_cells[neighbor] = sum(\n                    solution[neigh_neighbor]\n                    for neigh_neighbor in neigh_neighbors\n                )\n            else:\n                border_cells[neighbor] -= 1\n        \n        # Update values for the next loop\n        circumference += inner_neighbors\n        area -= 1\n        circ_area_ratio = circumference / area\n    \n    return solution\n\ndisplay_slitherlink_solution(\n    slitherlink := random_slitherlink(\n        (7, 7),\n        target_ratio = 3.5\n    )\n)\n\n(&lt;Figure size 600x600 with 1 Axes&gt;, &lt;AxesSubplot: &gt;)\n\n\n\n\n\nAnd it seems to have worked quite well! Now we just need the clues. Calculating the amount of adjacent cells is like a convolution (as in a layer from a CNN). To keep the environment minimal, I won’t import anything beyond NumPy; I’ll just code it myself.\n\ndef get_clues_from_solution(solution):\n    right = np.pad(solution[:, 1:], ((0, 0), (0, 1)), constant_values=0)\n    left = np.pad(solution[:, :-1], ((0, 0), (1, 0)), constant_values=0)\n    down = np.pad(solution[1:, :], ((0, 1), (0, 0)), constant_values=0)\n    up = np.pad(solution[:-1, :], ((1, 0), (0, 0)), constant_values=0)\n    clues = (left + right + up + down)\n    \n    # For outside cells, borders are other way round\n    clues[solution == 1] = 4 - clues[solution == 1]\n    return clues\n\n\ndef display_slitherlink_clues(\n    solution: \"0/1 numpy array\"\n) -&gt; \"(fig, ax) tuple for plotted figure\":\n    fig, ax = plt.subplots(figsize=(6, 6))\n    shape = solution.shape\n    grid = get_clues_from_solution(solution)\n    ax.imshow(solution)\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            text = ax.text(j, i, int(grid[i, j]),\n                ha=\"center\", va=\"center\", color=\"gray\")\n    return (fig, ax)\ndisplay_slitherlink_clues(slitherlink)\npass"
  },
  {
    "objectID": "000_Blog/008_simplex/enter_the_simplex.html",
    "href": "000_Blog/008_simplex/enter_the_simplex.html",
    "title": "Enter the Simplex",
    "section": "",
    "text": "“Compositional data” is data representing the composition of something - i.e. water is 66% H and 33% O (H20) by atom count, so a glass of water could be described as the compositional vector [2/3 1/3] regardless of how big the glass is. It’s scale-independent; it should always sum to one.\nThe space that compositional data lives in is called the Simplex. “Simplex” is a fancy name for an arbitrarily-dimensional triangle. Just like how we can imagine the set of vectors with magnitude one as existing on a hypersphere (arbitrarily-dimensional sphere), the set of vectors with sum one exists on a (hyper)triangle with vertices along each of the axes - hence the name “simplex”. This is in contrast with ordinary data, which lives in Euclidean space.\nEuclidean space is “normal” space; your intuitions about geometric facts are likely to be valid here. However, the Simplex is a different space with different rules. If we do not account for those rules when performing analyses, our results will be erroneous.\nIf you don’t like the line on the left being “straight”, call it a geodesic; it means the same thing but you probably don’t have any preconceived notions as to its meaning. Note that this is the same reason that long haul planes don’t travel on “straight” lines on the map; the plane is travelling on a straight line (geodesic) in the natural geometry of the earth’s surface (the curvature). A geodesic is the shortest path1 between two points. The Simplex is graphed as a triangle because of its, well, triangular nature - note that it actually has three axes, one for each side. It may have three parameters, but the dimensionality of the data is 2. In arbitrary dimensions, you may have \\(n\\) numbers, but the last number can always be worked out by 1 minus the sum of the others."
  },
  {
    "objectID": "000_Blog/008_simplex/enter_the_simplex.html#log-ratio-transforms",
    "href": "000_Blog/008_simplex/enter_the_simplex.html#log-ratio-transforms",
    "title": "Enter the Simplex",
    "section": "Log-Ratio Transforms",
    "text": "Log-Ratio Transforms\nTake the set of variables \\(\\{\\forall i : x_i\\}\\) in your compositional data. The set of log-ratios would be \\(\\{\\forall i, j : \\log \\frac{x_i}{x_j}\\}\\). Since compositional data is all about relative sizes, it does make intuitive sense why ratios would be important. There are three major log-ratio transforms (CLR, ALR, ILR): they all depend on picking out a particularly nice, small, subset of these log-ratios (or log-ratio-ish things). They’re particularly natural when you have reason to believe your data is close to being log-normally distributed.\n\nCentered Log-Ratio (CLR)\nCLR is an isometry - it is a reversible mapping from the Simplex to Euclidean space which perfectly preserves the geometric structure. In other words, it gives us a way of interpreting the Simplex in terms of our pre-existing Euclidean intuitions. While transformations being an isometry does not necessarily make it the right tool for the job, the fact that this is an isometry points to it being a mathematically “natural” thing to do. Mathematics often rewards naturality.\nCLR preserves the amount of parameters of the data. The parameters preserve their meaning: the \\(i\\)th parameter pre- and post-CLR still represent a measure of size for the same thingamajig (somewhat like, but different to, measuring things in miles vs kilometers).\nIts downside is that it preserves the number of parameters. As we saw, data in the Simplex requires more parameters than its dimensionality - this is still true in the output of CLR as it does not remove parameters.\nAs an aside, the inverse function of CLR is Softmax.\n\n\nAdditive Log-Ratio (ALR)\nALR is not an isometry, it is usually close enough to being one (Greenacre et al. 2022) that the difference does not matter too much. Unlike CLR, ALR reduces the amount of parameters of your data to match its dimensionality. It does this by picking one of your original parameters to be the reference; it will be the denominator in all the log-ratios. Hence the log-ratio associated with the reference will always be 0 and can be ignored. This is, of course, a downside if all of the potential references are interesting in their own right, as one will have to be sacrificed.\nThis is my personal favorite of the log-ratio transforms; even though I am a mathematician, I think approximate equality is just as good as exact equality in the real world. I don’t need my transform to be an exact isometry.\n\n\nSidequest: How to choose a good reference for ALR\n\nI couldn’t find anything online 🤷‍♂️ I assume it’s best to just pick the least interesting parameter. For what I do, there are many uninteresting parameters so this should not be a problem. If we chose a constant parameter \\(1\\), then I guess this would be equivalent to just taking the log transform.\n\n\n\nIsometric Log-Ratio (ILR)\nILR is an isometry, and it also reduces the parameterspace to match the dimensionality. It does this at the cost of interpretability; while there is some kind of interpretation based on the concept of “balances”2, the parameters post-ILR are not directly related to those pre-ILR. This is my least favorite of the log-ratio transforms; it is mathematically interesting, but at least in the work that I’ve been doing lately I very much want the parameters pre-ILR and post-ILR to have obvious and immediate relationships."
  },
  {
    "objectID": "000_Blog/008_simplex/enter_the_simplex.html#other-transforms",
    "href": "000_Blog/008_simplex/enter_the_simplex.html#other-transforms",
    "title": "Enter the Simplex",
    "section": "Other Transforms",
    "text": "Other Transforms\nThere are other transforms, such as Box-Cox (Tsagris, Preston, and Wood (2011)), which are interesting in their own right."
  },
  {
    "objectID": "000_Blog/008_simplex/enter_the_simplex.html#footnotes",
    "href": "000_Blog/008_simplex/enter_the_simplex.html#footnotes",
    "title": "Enter the Simplex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, locally shortest.↩︎\nWhich I must confess I do not really understand.↩︎"
  },
  {
    "objectID": "000_Blog/012_go/go_win.html",
    "href": "000_Blog/012_go/go_win.html",
    "title": "Go Victory",
    "section": "",
    "text": "I beat the 7kyu bot “Carnation”!\n\nTo be honest, I think it was a fluke - I managed to with the fight for the upper-left white group’s survival, but that’s because I got a bit lucky with what I had to work with. Nevertheless, I’m proud of it. The bot resigned. I’ll have to play against it a few more times until I can win consistently."
  },
  {
    "objectID": "000_Blog/010_blogging/polyglotting.html",
    "href": "000_Blog/010_blogging/polyglotting.html",
    "title": "Polyglot Programming 2",
    "section": "",
    "text": "Yesterday, I tried to combine Python and R into the same notebook, with mixed results (leaning towards “not good enough”). However, today I realized I could use IPython magic to run R (magic shown in comment since Quarto auto-removes it for final display1):\n# Proof that I'm in a python environment\nimport sys\nsys.__version__\n\nsys.version_info(major=3, minor=9, micro=13, releaselevel='final', serial=0)\n#%%script R --no-save\nprint(c(\"Hello\", \"World\"))\n\n\nR version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; #%%script R --no-save\n&gt; print(c(\"Hello\", \"World\"))\n[1] \"Hello\" \"World\"\n&gt;\nUnfortunately this comes with a lot of fluff, so I’ll need to figure out how to get rid of it! Also, it syntax highlights according to Python, although that isn’t too big a deal.\nI found that, with rpy2 installed, we can get a simple %%R cell magic; however I have trouble installing rpy2 because I’m on an M1 Mac so things are weird (for example, note the platform output in:\nBy specifying --no-echo, however, we get rid of most of the extra fluff!\n#%%script R --no-save --no-echo\nprint(\"Hello\")\nprint(\"World\")\nprint(c(\"Nice\", \"To\", \"Meet\", \"You\"))\n\n[1] \"Hello\"\n[1] \"World\"\n[1] \"Nice\" \"To\"   \"Meet\" \"You\"\nHowever, with this setup variables are not saved:\n#%%script R --no-save --no-echo\nx &lt;- 5\nprint(x)\n\n[1] 5\n#%%script R --no-save --no-echo\ntryCatch(x, error=function(cond){\"X not found!\"})\n\n[1] \"X not found!\"\nThis is not a problem if everything can be done in one cell before continuing with python. I should note also that in an R notebook, we could still run Python:\n#%%python3\nprint(\"Hello\")\nprint(\"World\")\n\nHello\nWorld\nThere are 3 flaws with this approach: * Syntax highlighting not correct - No big deal, highlighting R as if it were Pyhon should be mostly fine * No persistent variables - This could be pretty annoying for some tasks * Need both R and Python in the same conda environment - This is a big deal as they have mutual dependencies that they fight over, preventing conda installation of Bioconductor packages (as far as I can tell; my diagnosis might not be perfect)\nTo fix the last flaw, I could potentially write a bash script that handles environment activation and deactivation, and to fix the middle flaw I could potentially save / load .Rdata files in such a script as well. However, I’d rather not have to write my own bash script - they scare me 😅."
  },
  {
    "objectID": "000_Blog/010_blogging/polyglotting.html#footnotes",
    "href": "000_Blog/010_blogging/polyglotting.html#footnotes",
    "title": "Polyglot Programming 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich I like, but is annoying for this specific task. There’s probably a Quarto setting to turn it off, if I look hard enough for it.↩︎"
  },
  {
    "objectID": "000_Blog/010_blogging/customizing-your-terminal.html",
    "href": "000_Blog/010_blogging/customizing-your-terminal.html",
    "title": "Attempting to Customize The ls Command",
    "section": "",
    "text": "Standard output of ls\n\n\nFor years now my Mac has been patiently pestering me to change my terminal from bash to zsh, so I bit the bullet today and did it. This got me thinking about what a shell actually is, and their ability to be customized. This quest brought to my attention that the ls command, which prints files in a directory, has some optional flags you can pass to it!\n\n\n\nYou can reformat the output to give more detailed information with ls -l.\n\n\n\n\n\nYou can color directories differently with ls -G.\n\n\nYou can even combine the two with ls -lG, but I’ll let you try that for yourself.\nI like this very much, so I thought I would try to control the colors that are outputted. To do this, I went into my ~/.zshrc file (presumably if I kept with bash it would be ~/.bashrc) - this is the file that runs every time you create a new terminal. The current contents of this file were:\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/Users/baileyandrew/mambaforge/bin/conda' 'shell.zsh' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/Users/baileyandrew/mambaforge/etc/profile.d/conda.sh\" ]; then\n        . \"/Users/baileyandrew/mambaforge/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/Users/baileyandrew/mambaforge/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n\nif [ -f \"/Users/baileyandrew/mambaforge/etc/profile.d/mamba.sh\" ]; then\n    . \"/Users/baileyandrew/mambaforge/etc/profile.d/mamba.sh\"\nfi\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nWhich I assume was auto-created when I first installed conda/mamba.\nI then added the following:\n# &gt;&gt;&gt; my custom scripts &gt;&gt;&gt;\n\nexport CLICOLOR=1\nexport LSCOLORS=\"gxfxcxdxbxegedabagacad\"\n\n# &lt;&lt;&lt; my custom scripts &lt;&lt;&lt;\n\n\n\nThe result of ls -lG.\n\n\nSo its clear I can affect the colors, but they’re not the colors I want. And here’s where I get sad - I wanted the to customize specific colors, but terminals are limited to only 256 (whyyyyyyy) and I don’t have fine-grained control over which file type gets which color.\nThe end."
  },
  {
    "objectID": "000_Blog/010_blogging/multikernels.html",
    "href": "000_Blog/010_blogging/multikernels.html",
    "title": "On the Road to Polyglot Programming with Jupyter",
    "section": "",
    "text": "Since I work with both R and Python a lot, it would be useful to be able to use both languages in the same notebook. I don’t really care about being able to pass data between them, just being able to display multiple languages together - I can always save and load data as a csv (or other format). However, passing data between them is a bonus.\nI considered making a Jupyter kernel to do this myself, but it seems like I’d need a decent amount of knowledge to be able to take that step - even though all I want to do is multiplex kernels.\nMy ideal setup is something like: * Add a cell tag to specify the kernel * Syntax highlighting and code running work in JupyterLab - As a bonus, both should work in JupyterLab Desktop * They should be compatible with Quarto * Allows side-by-side code blocks"
  },
  {
    "objectID": "000_Blog/010_blogging/multikernels.html#cheating-a-bit",
    "href": "000_Blog/010_blogging/multikernels.html#cheating-a-bit",
    "title": "On the Road to Polyglot Programming with Jupyter",
    "section": "Cheating a bit",
    "text": "Cheating a bit\nI can get halfway to being a polyglot as follows:\n\n\n\nPython\n\n\nR\n\n\n\nprint([\"Hello\", \"World\"])\n\n\n['Hello', 'World']\n\n\n\n\nprint(c(\"Hello\", \"World\"))\n\n\n[1] \"Hello\" \"World\"\n\n\n\n\n\n\nSo, how did I do it? Well, it looks a bit ulgy in JupyterLab:\n\n\n\nFigure 2\n\n\nBut it was set up as follows:\n&lt;table style=\"width:100%\"&gt;\n    &lt;th&gt;Python&lt;/th&gt;&lt;th&gt;R&lt;/th&gt;\n    &lt;tr&gt;&lt;td&gt;\n        \n```python\nprint([\"Hello\", \"World\"])\n```\n&lt;!-- Python Code Cell Here--&gt;\n&lt;/td&gt;&lt;td&gt;\n        \n```r\nprint(c(\"Hello\", \"World\"))\n```\n&lt;!-- R Code Cell Here--&gt;\n&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;\nTo do this, I had to have my Python kernel be from an environment that had access to an R distribution; unfortunately that’s something I want to move away from2."
  },
  {
    "objectID": "000_Blog/010_blogging/multikernels.html#footnotes",
    "href": "000_Blog/010_blogging/multikernels.html#footnotes",
    "title": "On the Road to Polyglot Programming with Jupyter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternatively, starting JupyterLab in Multi-Kernel should probably work, but you’d likely have to install JupyterLab there first.↩︎\nI want to instead have every environment be precisely one language, since I think having multiple is what is preventing me from installing Bioconductor packages with conda↩︎\n.ipynb file format specification↩︎"
  },
  {
    "objectID": "000_Blog/2023tuejan3.html",
    "href": "000_Blog/2023tuejan3.html",
    "title": "Conferences",
    "section": "",
    "text": "I recently had to do some research for conferences in my field (thesis title “Zero Inflation in scRNA-seq Data”). Here are the results.\nMy first search method was to blindly google for conferences. It gave some results, listed as follows.\n\n\nCould maybe sprint for ICML with its Jan 26 deadline, otherwise the deadlines have either already passed or are extremely imminent.\n\n\n\nShould occur sometime late 2023 (last year was late November & early December)\nDeadlines not released, but last year was in May I think for the main conference\nHas proceedings\n\n\n\n\n\nJan 26 deadline for papers\nWorkshops won’t be decided until Mar 16.\nFirst day of conference is Jul 23 in Honolulu\nHas proceedings (PMLR)\nTopics of conference (bold highlights are directly relevant to my work)\n\nGeneral Machine Learning (active learning, clustering, online learning, ranking, reinforcement learning, supervised, semi- and self-supervised learning, time series analysis, etc.)\nDeep Learning (architectures, generative models, deep reinforcement learning, etc.)\nLearning Theory (bandits, game theory, statistical learning theory, etc.)\nOptimization (convex and non-convex optimization, matrix/tensor methods, stochastic, online, non-smooth, composite, etc.)\nProbabilistic Inference (Bayesian methods, graphical models}, Monte Carlo methods, etc.)\nTrustworthy Machine Learning (accountability, causality, fairness, privacy, robustness, etc.)\nApplications (computational biology, crowdsourcing, healthcare, neuroscience, social good, climate science, etc.)\n\n\n\n\n\n\nDeadline was Aug 8\nWorkshops are released, below are ones relevant to me\n\nW18: Graphs and More Complex Structures for Learning and Reasoning (GCLR)\n\nDeadline was Oct 28\n\nW19: Health Intelligence (W3PHIAI-23)\n\nDeadline was Nov 15\n\n\nFirst day on Feb 7 in Washington DC\nHas proceedings (AAAI Digital Library)\n\n\n\n\n\nDeadline was Sep 28\nHas workshops\n\nAI for Public Health\n\nDeadline Mar 3\n\n\nFirst day on May 1 in Kigali (I really want to go 😢 )\nRelevant topics (does not seem relevant to me at the moment)\n\nunsupervised, semi-supervised, and supervised representation learning\nrepresentation learning for planning and reinforcement learning\nrepresentation learning for computer vision and natural language processing\nmetric learning and kernel learning\nsparse coding and dimensionality expansion\nhierarchical models\noptimization for representation learning\nlearning representations of outputs or states\noptimal transport\ntheoretical issues in deep learning\nsocietal considerations of representation learning including fairness, safety, privacy, and interpretability, and explainability\nvisualization or interpretation of learned representations\nimplementation issues, parallelization, software platforms, hardware\nclimate, sustainability\napplications in audio, speech, robotics, neuroscience, biology, or any other field\n\nHas proceedings (OpenReview)\n\n\n\n\n\nDeadline is Jan 16\nWorkshops not decided till Mar 6\nFirst day on Aug 19 in Cape Town\nRelevant topics\n\nSubmissions to IJCAI 2023 should report on significant, original, and previously unpublished results on any aspect of artificial intelligence.\n\nHas proceedings\n\n\n\n\n\n\n\n\nWebsite not updated for this year, but last year the deadline was June 3 and the conference started on Oct 5.\nApparently doesn’t take place until 2024!\n\n\n\n\n\n\n\nThis is a predatory publisher.\n\n\n\nProceedings exist but are not guaranteed\nJan 23rd in London\nDeadline was Dec 21\nThis conference is really weird. I can read a list of selected papers, and none of them are about bioinformatics. In fact, 3/15 are about “Fighter Aircraft Selection” and a fourth is about “Military Attack Helicopter Selection” and they’re all by the same person????\nLink to page\n\nIn general, the World Academy of Science, Engineering, and Technology lists lots of them. Their deadlines are all mid-month the month before the conference, except for the ones in April whose deadlines passed in December for some reason…\nMany of them happen multiple times per year. Would not be to hard to fit in with basically all the topics.\nThey all have proceedings.\n\n\n\n\n\nDeadline was Aug, conference was Dec\nMight not be active this year? Doesn’t occur every year, doesn’t seem to have a regular pattern, is frequent so there is a good chance though.\nConference has proceedings\n\n\n\n\n\nDeadline was in Jan with conference in June, but it doesn’t seem to be active this year.\n\n\n\n\n\nJan 20 deadline\nStarts on Apr 21\nIn Hangzhou\nThere are proceedings\nTopics of interest:\n\nBioinformatics\nComputational Biology\nBig Data Analytics in Bioinformatics\nRobustness and Reproducibility in Bioinformatics\nBio-Ontology & Data Mining\nGenes and their Regulation\nBig Data Analytics in Healthcare"
  },
  {
    "objectID": "000_Blog/2023tuejan3.html#general-ai-conferences",
    "href": "000_Blog/2023tuejan3.html#general-ai-conferences",
    "title": "Conferences",
    "section": "",
    "text": "Could maybe sprint for ICML with its Jan 26 deadline, otherwise the deadlines have either already passed or are extremely imminent.\n\n\n\nShould occur sometime late 2023 (last year was late November & early December)\nDeadlines not released, but last year was in May I think for the main conference\nHas proceedings\n\n\n\n\n\nJan 26 deadline for papers\nWorkshops won’t be decided until Mar 16.\nFirst day of conference is Jul 23 in Honolulu\nHas proceedings (PMLR)\nTopics of conference (bold highlights are directly relevant to my work)\n\nGeneral Machine Learning (active learning, clustering, online learning, ranking, reinforcement learning, supervised, semi- and self-supervised learning, time series analysis, etc.)\nDeep Learning (architectures, generative models, deep reinforcement learning, etc.)\nLearning Theory (bandits, game theory, statistical learning theory, etc.)\nOptimization (convex and non-convex optimization, matrix/tensor methods, stochastic, online, non-smooth, composite, etc.)\nProbabilistic Inference (Bayesian methods, graphical models}, Monte Carlo methods, etc.)\nTrustworthy Machine Learning (accountability, causality, fairness, privacy, robustness, etc.)\nApplications (computational biology, crowdsourcing, healthcare, neuroscience, social good, climate science, etc.)\n\n\n\n\n\n\nDeadline was Aug 8\nWorkshops are released, below are ones relevant to me\n\nW18: Graphs and More Complex Structures for Learning and Reasoning (GCLR)\n\nDeadline was Oct 28\n\nW19: Health Intelligence (W3PHIAI-23)\n\nDeadline was Nov 15\n\n\nFirst day on Feb 7 in Washington DC\nHas proceedings (AAAI Digital Library)\n\n\n\n\n\nDeadline was Sep 28\nHas workshops\n\nAI for Public Health\n\nDeadline Mar 3\n\n\nFirst day on May 1 in Kigali (I really want to go 😢 )\nRelevant topics (does not seem relevant to me at the moment)\n\nunsupervised, semi-supervised, and supervised representation learning\nrepresentation learning for planning and reinforcement learning\nrepresentation learning for computer vision and natural language processing\nmetric learning and kernel learning\nsparse coding and dimensionality expansion\nhierarchical models\noptimization for representation learning\nlearning representations of outputs or states\noptimal transport\ntheoretical issues in deep learning\nsocietal considerations of representation learning including fairness, safety, privacy, and interpretability, and explainability\nvisualization or interpretation of learned representations\nimplementation issues, parallelization, software platforms, hardware\nclimate, sustainability\napplications in audio, speech, robotics, neuroscience, biology, or any other field\n\nHas proceedings (OpenReview)\n\n\n\n\n\nDeadline is Jan 16\nWorkshops not decided till Mar 6\nFirst day on Aug 19 in Cape Town\nRelevant topics\n\nSubmissions to IJCAI 2023 should report on significant, original, and previously unpublished results on any aspect of artificial intelligence.\n\nHas proceedings"
  },
  {
    "objectID": "000_Blog/2023tuejan3.html#specialized-ai-conferences",
    "href": "000_Blog/2023tuejan3.html#specialized-ai-conferences",
    "title": "Conferences",
    "section": "",
    "text": "Website not updated for this year, but last year the deadline was June 3 and the conference started on Oct 5.\nApparently doesn’t take place until 2024!"
  },
  {
    "objectID": "000_Blog/2023tuejan3.html#bioinformatics-conferences",
    "href": "000_Blog/2023tuejan3.html#bioinformatics-conferences",
    "title": "Conferences",
    "section": "",
    "text": "This is a predatory publisher.\n\n\n\nProceedings exist but are not guaranteed\nJan 23rd in London\nDeadline was Dec 21\nThis conference is really weird. I can read a list of selected papers, and none of them are about bioinformatics. In fact, 3/15 are about “Fighter Aircraft Selection” and a fourth is about “Military Attack Helicopter Selection” and they’re all by the same person????\nLink to page\n\nIn general, the World Academy of Science, Engineering, and Technology lists lots of them. Their deadlines are all mid-month the month before the conference, except for the ones in April whose deadlines passed in December for some reason…\nMany of them happen multiple times per year. Would not be to hard to fit in with basically all the topics.\nThey all have proceedings.\n\n\n\n\n\nDeadline was Aug, conference was Dec\nMight not be active this year? Doesn’t occur every year, doesn’t seem to have a regular pattern, is frequent so there is a good chance though.\nConference has proceedings\n\n\n\n\n\nDeadline was in Jan with conference in June, but it doesn’t seem to be active this year.\n\n\n\n\n\nJan 20 deadline\nStarts on Apr 21\nIn Hangzhou\nThere are proceedings\nTopics of interest:\n\nBioinformatics\nComputational Biology\nBig Data Analytics in Bioinformatics\nRobustness and Reproducibility in Bioinformatics\nBio-Ontology & Data Mining\nGenes and their Regulation\nBig Data Analytics in Healthcare"
  },
  {
    "objectID": "000_Blog/004_scRNA/004_exploration.html",
    "href": "000_Blog/004_scRNA/004_exploration.html",
    "title": "Analyzing scRNA-seq Data – Exploration",
    "section": "",
    "text": "Yesterday, we looked at a scRNA-seq dataset of Danio rerio cells (Hernández et al. 2018). We spent a lot of time understanding how the dataset was created, and ended with a bit of a mystery: why did the cell counts not match up between our datasets? I don’t have an answer to that mystery, unfortunately - but in my experience this mismatch happens in a lot of papers. We might as well continue with the analysis, rather than getting hung up on a minor anomaly.\nI realized since yesterday that there was another gene counts file, (the filtered tpms file). I downloaded this and the “experiment metadata” files; I suspect these may be useful as I know that the TPM values were pre-quality-control:\n\nFor the Smart-seq2 protocol transcript per million (TPM) values reported by Salmon were used for the quality control (QC). Wells with fewer than 900 expressed genes (TPM &gt; 1) or having more than either 60% of ERCC or 45% of mitochondrial content were annotated as poor quality cells. As a result, 322 cells failed QC and 542 single cells were selected for the further study.\n– Quality Control of Single-Cell Data; Materials and Methods Section; Hernández et al. (2018) \n\n\n\nCan I mix R and Python in the same notebook?\n\nYes! I sometimes use SOS Kernel which allows me to swap between kernels at ease. You can change kernels (R vs Python) using %use magic commands; I’ve done this before but the syntax highlighting gets messed up often, so I’ve chosen not to. do that here.\n\n\nraw.counts &lt;- Matrix::readMM('./localdata/E-MTAB-7117.expression_tpm.mtx')\ndim(raw.counts)\n\n\n21797966\n\n\n\n\nDoes the metadata file help us resolve the cell count mystery?\n\nWhat does this extra metadata file tell us? Let’s see:\n\nmeta.data &lt;- read.table(\"./localdata/E-MTAB-7117.sdrf.txt\", sep='\\t', header = TRUE)\nhead(meta.data)\ndim(meta.data)\nlength(which(meta.data['Characteristics.single.cell.quality.'] != \"not OK\"))\n\n\nA data.frame: 6 × 55\n\n\n\nSource.Name\nComment.ENA_SAMPLE.\nComment.BioSD_SAMPLE.\nCharacteristics.organism.\nCharacteristics.strain.\nCharacteristics.age.\nUnit.time.unit.\nCharacteristics.developmental.stage.\nCharacteristics.sex.\nCharacteristics.genotype.\n⋯\nComment.ENA_EXPERIMENT.\nScan.Name\nComment.SUBMITTED_FILE_NAME.\nComment.ENA_RUN.\nComment.FASTQ_URI.\nComment.SPOT_LENGTH.\nComment.READ_INDEX_1_BASE_COORD.\nFactor.Value.genotype.\nFactor.Value.organism.part.\nFactor.Value.single.cell.identifier.\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nCD4_gill_A1\nERS2634491\nSAMEA4814592\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737040\nSLX-10875.N701_S513.C9FTNANXX.s_5.r_1.fq.gz\nSLX-10875.N701_S513.C9FTNANXX.s_5.r_1.fq.gz\nERR2723271\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2723271/ERR2723271_1.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A1\n\n\n2\nCD4_gill_A1\nERS2634491\nSAMEA4814592\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737040\nSLX-10875.N701_S513.C9FTNANXX.s_5.r_2.fq.gz\nSLX-10875.N701_S513.C9FTNANXX.s_5.r_2.fq.gz\nERR2723271\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2723271/ERR2723271_2.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A1\n\n\n3\nCD4_gill_A10\nERS2634691\nSAMEA4814793\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737240\nSLX-10875.N712_S513.C9FTNANXX.s_5.r_1.fq.gz\nSLX-10875.N712_S513.C9FTNANXX.s_5.r_1.fq.gz\nERR2723471\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2723471/ERR2723471_1.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A10\n\n\n4\nCD4_gill_A10\nERS2634691\nSAMEA4814793\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737240\nSLX-10875.N712_S513.C9FTNANXX.s_5.r_2.fq.gz\nSLX-10875.N712_S513.C9FTNANXX.s_5.r_2.fq.gz\nERR2723471\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2723471/ERR2723471_2.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A10\n\n\n5\nCD4_gill_A11\nERS2635221\nSAMEA4815323\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737770\nSLX-10875.N714_S513.C9FTNANXX.s_5.r_1.fq.gz\nSLX-10875.N714_S513.C9FTNANXX.s_5.r_1.fq.gz\nERR2724001\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2724001/ERR2724001_1.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A11\n\n\n6\nCD4_gill_A11\nERS2635221\nSAMEA4815323\nDanio rerio\nAB\n6\nmonth\nadult\nmale\nTg(cd4-1:mCherry)\n⋯\nERX2737770\nSLX-10875.N714_S513.C9FTNANXX.s_5.r_2.fq.gz\nSLX-10875.N714_S513.C9FTNANXX.s_5.r_2.fq.gz\nERR2724001\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR272/001/ERR2724001/ERR2724001_2.fastq.gz\n250\n126\nTg(cd4-1:mCherry)\ngill\nCD4_gill_A11\n\n\n\n\n\n\n211255\n\n\n1952\n\n\nAt first glance, no - it does not. The metadata has 2112 lines, and removing poor quality cells leaves us with 1952.\nBut, it does seem like some cells have multiple names, so let’s filter by unique values.\n\ndim(unique(meta.data['Source.Name']))\nlength(\n    unique(\n        meta.data[\n            which(meta.data['Characteristics.single.cell.quality.'] != \"not OK\"),\n            \"Source.Name\"\n        ]\n    )\n)\n\n\n10561\n\n\n976\n\n\nGee whiz, that 976 number returns… We can re-use the list of 10 extra cells calculated yesterday to investigate them:\n\ndiscrepancies &lt;- c(\n    \"ERR2723217\", \"ERR2723218\", \"ERR2723236\", \"ERR2723237\", \"ERR2723551\",\n    \"ERR2723578\", \"ERR2723789\", \"ERR2723794\", \"ERR2723974\", \"ERR2723990\"\n)\nextra.cells &lt;- meta.data[\n    which(sapply(meta.data[\"Comment.ENA_RUN.\"], `%in%`, discrepancies)),\n]\n\n\n# Output hidden to reduce spam\nextra.cells[1:10]\nextra.cells[11:20]\nextra.cells[21:30]\nextra.cells[31:40]\nextra.cells[41:55]\n\nBut as before there doesn’t really seem to be any outstanding features. So the mystery continues!\n\n\nQuality Control\nThe raw.counts matrix is a Sparse Matrix with 21,797 genes and 966 cells - the values are expressed in TPM. Our goal is to filter out the low quality cells.\n\nFor the Smart-seq2 protocol transcript per million (TPM) values reported by Salmon were used for the quality control (QC). Wells with fewer than 900 expressed genes (TPM &gt; 1) or having more than either 60% of ERCC or 45% of mitochondrial content were annotated as poor quality cells. As a result, 322 cells failed QC and 542 single cells were selected for the further study.\n– Quality Control of Single-Cell Data; Materials and Methods Section; Hernández et al. (2018) \n\nAt the time, I didn’t think we could find the mitochondrial genes with the data we had (and same with ERCC). I’ve gone back and edited this though, in the dropdowns.\n\n\nGetting mitochondrial genes\n\nI used Python since chronologically I did this ater I gave up on R. The work to create the mito-genes csv was done in the blog post Exploring Ensembl.\n\nmito_genes = pd.read_csv(\"./localdata/mito-genes.csv\")\nmito_genes[\"gene_id\"].head()\n\n0    ENSDARG00000083480\n1    ENSDARG00000082753\n2    ENSDARG00000081443\n3    ENSDARG00000080337\n4    ENSDARG00000083046\nName: gene_id, dtype: object\n\n\nNow we want to calculate the percent of genomic material per cell that is contained in just the genes in mito_genes, which should not be too hard! Left as excersize to the reader.\n\n\n\nGetting ERCC genes\n\nI could not figure out how to do this; I think they have already been removed from the dataset that we have.\n\nBut yesterday we noticed that we could get the 542 futher-study cells by looking at the metadata and only grabbing the cells which were eventually assigned a cluster by the researchers.\n\n# Calculate the cells we want to study\nsample.data &lt;- read.table(\"./localdata/ExpDesign-E-MTAB-7117.tsv\", sep='\\t', header=TRUE)\nfiltered &lt;- sample.data[which(\n    sample.data['Sample.Characteristic.cluster.'] != \"unknown\"\n),]\ncells.to.study &lt;- filtered$Assay\nlength(cells.to.study)\n\n542\n\n\n\n# Get which rows correspond to which assay (cell)\nassay.to.row.map &lt;- read.table(\n    './localdata/E-MTAB-7117.expression_tpm.mtx_cols'\n)$V1\nlength(assay.to.row.map)\n\n966\n\n\n\n# Subset the raw.counts matrix\nraw.counts.filtered &lt;- raw.counts[,\n    which(sapply(assay.to.row.map, `%in%`, cells.to.study)),\n]\ndim(raw.counts.filtered)\n# Save the matrix, for posterity\nwriteMM(raw.counts.filtered, \"./localdata/542_cells_21797_genes.tpm.mtx\")\n\n\n21797542\n\n\nNULL\n\n\nNow we need to perform quality control on the genes;\n\nFor each of the 542 single cells, counts reported by Salmon were transformed into normalised counts per million (CPM) and used for the further analysis. This was performed by dividing the number of counts for each gene with the total number of counts for each cell and by multiplying the resulting number by a factor of 1,000,000. Genes that were expressed in less than 1% of cells (e.g. 5 single cells with CPM &gt; 1) were filtered out. In the final step we ended up using 16,059 genes across the 542 single cells. The scran R package (version 1.6.7) Lun, Bach, and Marioni (2016) was then used to normalise the data and remove differences due to the library size or capture efficiency and sequencing depth.\n– Downstream Analysis of Smart-seq2 Data; Materials and Methods Section; Hernández et al. (2018) \n\nIt was harder than expected, as I am less-than-fluent with R and their sparse matrix operations weren’t as analogous to dense matrices as I would have liked. First let’s take a look at our the sparse datatype we have. I found the blogpost by Slowikowski (2020) useful, but ultimately got fed up with R and decided to transition to python.\n\n\nFalse Start: Trying to use R\n\n\nstr(raw.counts.filtered)\n\nFormal class 'dgTMatrix' [package \"Matrix\"] with 6 slots\n  ..@ i       : int [1:1156509] 4 10 19 36 56 57 58 59 62 69 ...\n  ..@ j       : int [1:1156509] 0 0 0 0 0 0 0 0 0 0 ...\n  ..@ Dim     : int [1:2] 21797 542\n  ..@ Dimnames:List of 2\n  .. ..$ : NULL\n  .. ..$ : NULL\n  ..@ x       : num [1:1156509] 0.593 145.541 361.733 0.178 0.107 ...\n  ..@ factors : list()\n\n\n(i, j) gives us the rows and columns, with the value being the corresponding entry in x. The T in dgTMatrix stands for triplet, because it’s essentially just a list of triplets (i, j, x). Another common format is CsparseMatrix, although the explanation is more complicated. It’s explained well by Slowikowski (2020), I’m just in a rush to get to actual data analysis!\n\nraw.counts.csparse &lt;- str(as(raw.counts.filtered, \"CsparseMatrix\"))\n\nFormal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n  ..@ i       : int [1:1156509] 4 10 19 36 56 57 58 59 62 69 ...\n  ..@ p       : int [1:543] 0 1610 3532 5491 7585 9273 11182 13157 14623 15552 ...\n  ..@ Dim     : int [1:2] 21797 542\n  ..@ Dimnames:List of 2\n  .. ..$ : NULL\n  .. ..$ : NULL\n  ..@ x       : num [1:1156509] 0.593 145.541 361.733 0.178 0.107 ...\n  ..@ factors : list()\n\n\nNow, we want to transform the data into normalized counts per million.\n\n# This will throw an error\ncolSums(raw.counts.csparse)\n\nERROR: Error in colSums(raw.counts.csparse): 'x' must be an array of at least two dimensions\n\n\n\n\nBut we know it should work for sparse matrixes from the docs\n\n\n\n\nWhat happens when we run ?colSums to get the docs\n\n\nFun fact: for some reason running the R help command will mess with Quarto’s html, breaking your webpage!\n\n\n# We can try to force it to use the Matrix overload, to no avail.\nMatrix::colSums(raw.counts.csparse)\n\nERROR: Error in base::rowSums(x, na.rm = na.rm, dims = dims, ...): 'x' must be an array of at least two dimensions\n\n\nAnd it’s about here that I get fed up with R.\n\nLet’s load our raw.counts.filtered matrix in python:\n\nfrom scipy.io import mmread\nfrom scipy import sparse\nimport numpy as np\nimport pandas as pd\n\n\nraw_counts_filtered = sparse.csc_array(mmread(\n    './localdata/542_cells_21797_genes.tpm.mtx'\n))\nraw_counts_filtered\n\n&lt;21797x542 sparse array of type '&lt;class 'numpy.float64'&gt;'\n    with 1156509 stored elements in Compressed Sparse Column format&gt;\n\n\nAnd now we’ll copy the process they used to rescale their data in terms of genes per cell.\n\ngenes_per_cell = raw_counts_filtered.sum(axis=0)\nraw_counts_nonzero = raw_counts_filtered[:, genes_per_cell &gt; 0]\ngenes_per_cell = genes_per_cell[genes_per_cell &gt; 0]\nraw_counts_nonzero = raw_counts_nonzero / genes_per_cell * 1e6\nraw_counts_normed = sparse.csr_array(raw_counts_nonzero)\nraw_counts_normed\n\n&lt;21797x542 sparse array of type '&lt;class 'numpy.float64'&gt;'\n    with 1156509 stored elements in Compressed Sparse Row format&gt;\n\n\nNow we need to filter out genes who were expressed in less than 1% of cells. They’re a bit more strict than that - it has to have a CPM of at least 1 to count as being “expressed”.\n\nraw_counts_expressed = raw_counts_normed.copy()\nraw_counts_expressed[raw_counts_expressed &lt;= 1] = 0\nraw_counts_expressed[raw_counts_expressed &gt; 1] = 1\nnumber_of_cells_gene_appears_in = raw_counts_expressed.sum(axis=1)\nraw_counts_genes_filtered = raw_counts_normed[\n    number_of_cells_gene_appears_in &gt; 5,\n    :\n].copy()\n\n\nraw_counts_genes_filtered\n\n&lt;17832x542 sparse array of type '&lt;class 'numpy.float64'&gt;'\n    with 1141288 stored elements in Compressed Sparse Row format&gt;\n\n\nWe have too many genes and I’m not sure why - however, I can’t find anywhere in their paper a count of the pre-filtered genes. Just like how their dataset contains more cells than reported, it may contain more genes than reported!\n\n\nAnalysis\n\nIn order to identify the highly variable genes (HVGs) we utilised the Brennecke Method (Brennecke et al. 2013). We inferred the noise model from the ERCCs and selected genes that vary higher than 20% percentage of variation. This was performed by using the “BrenneckeGetVariableGenes” command of M3Drop v1.4.0 R package setting fdr equal to 0.01 and minimum percentage of variance due to biological factors (minBiolDisp) equal to 0.2. In total, 3,374 were annotated as HVGs.\n– Downstream Analysis of Smart-seq2 Data; Materials and Methods Section; Hernández et al. (2018) \n\nThe BrenneckeGetVariableGenes method is described in the M3Drop documentation as follows:\n\n\n\nBrenneckeGetVariableGenes\n\n\nThus we can see that we need the ERCCs to deduce the HGVs. However, I can’t find any data on the ERCC spike-ins, so at this point we stop here. Sorry to disappoint.\n\nReferences\n\n\nBrennecke, P., S. Anders, J. K. Kim, A. A. Kołodziejczyk, X. Zhang, V. Proserpio, B. Baying, et al. 2013. “Accounting for technical noise in single-cell RNA-seq experiments.” Nat Methods 10 (11): 1093–95.\n\n\nHernández, P. P., P. M. Strzelecka, E. I. Athanasiadis, D. Hall, A. F. Robalo, C. M. Collins, P. Boudinot, J. P. Levraud, and A. Cvejic. 2018. “Single-cell transcriptional analysis reveals ILC-like cells in zebrafish.” Science Immunology 3 (29).\n\n\nLun, A. T., K. Bach, and J. C. Marioni. 2016. “Pooling across cells to normalize single-cell RNA sequencing data with many zero counts.” Genome Biol 17 (April): 75.\n\n\nSlowikowski, Kamil. 2020. “Working with a Sparse Matrix in r.” https://slowkow.com/notes/sparse-matrix/."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer1.html",
    "href": "000_Blog/004_scRNA/cancer1.html",
    "title": "A Living Biobank of Ovarian Cancer – Data Acqusition",
    "section": "",
    "text": "I was recently reading the paper “A living biobank of ovarian cancer ex vivo models reveals profound mitotic heterogeneity” (Nelson et al. 2020), which I found when looking at this dataset from the Single Cell Expression Atlas. Let’s talk about this dataset."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer1.html#the-original-sample",
    "href": "000_Blog/004_scRNA/cancer1.html#the-original-sample",
    "title": "A Living Biobank of Ovarian Cancer – Data Acqusition",
    "section": "The Original Sample",
    "text": "The Original Sample\nWe’ve previously talked extensively about protocols, but it’s also important to understand the original sample rather than just how it was processed!\nSpecifically, we want to know:\n\nWho (was sampled)\nWhere (were they sampled)\nWhen (were they sampled)\nWhy (were they sampled)1\nWhat (part of them was sampled)\nHow many (were sampled)2\n\n\nTo build a living biobank, we established a biopsy pipeline, collecting samples from patients diagnosed with epithelial ovarian cancer treated at the Christie Hospital.\n– “Establishing a living biobank of ovarian cancer ex vivo models” subsection, Results section, (Nelson et al. 2020)\n\nThis satisfies the where of our original question: Christie Hospital:\n\nThe Christie NHS Foundation Trust\nWilmslow Road\nManchester\nM20 4BX\nUnited Kingdom\n\nIt seems reasonable to assume that the grand majority of patients thus reside in the UK, but we’ll have to go on a Sidequest to say much more. A biopsy is when bits of you get removed to be examined clinically.\n\n\nSidequest: The Christie NHS Foundation Trust\n\nIt would be nice to say that the patients reside in the Manchester, but this is not necessarily true. The feeding area for the hopsital may be larger than that, and also if epithelial ovarian cancer is rare enough it might attract patients from further afield. What we’re interested in is catchment; the locations from which people come into the hospital. Conveniently, The Christie has catchment info on their website. It reveals that for gynaecological cancers such as epithelial ovarian cancer, 73% of the patients are from Manchester, and 97% are from the North West.\nSurprisingly, the North West has an official definition:\n\nNorth West England is one of nine official regions of England and consists of the administrative counties of Cheshire, Cumbria, Greater Manchester, Lancashire and Merseyside\n– Wikipedia, the most trustworthy of sources\n\n\nIt also mostly answers the who question; patients with epithelial ovarian cancer. In the pursuit of more details, we can refer to:\n\nBetween May 2016 and June 2019, we collected 312 samples from patients with chemo-naïve and relapsed disease, either as solid biopsies or as ascites (Fig. 1a)\n\n\nTen patients had HGSOC while two had mucinous ovarian carcinoma. Longitudinal biopsies were collected from three patients.\n\n\n\nAn ascites is an abnormal abdominal buildup of fluid.\nWhich conveniently answers when and how many. Fig 1 in the paper provides further specificity, revealing that there are 12 patients.\nWe should probably find out what and why but the what is presumably just “wherever the tumour is” and for why presumably patients were included if and only if they had epithelial ovarian cancer at The Christie."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer1.html#the-protocol",
    "href": "000_Blog/004_scRNA/cancer1.html#the-protocol",
    "title": "A Living Biobank of Ovarian Cancer – Data Acqusition",
    "section": "The Protocol",
    "text": "The Protocol\n\n\n\nFrom Nelson et al. (2020), not my own work.\n\n\nThey used 10x Genomics Protocol CG000183 Rev A (“Chromium Single Cell 3’ Reagent Kits V3 User Guide” 2018).\n\n\n\nOverview of the protocol. Not my own work\n\n\nIn brief:\n\nThe primer contains:\n\nan Illumina TruSeq Read 1 (read 1 sequencing primer)\n16 nt 10x Barcode\n12 nt unique molecular identifier (UMI)\n30 nt poly(dT) sequence\n\n\n\nBarcoded, full-length cDNA is amplified via PCR to generate sufficient mass for library construction."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer1.html#data-exploration",
    "href": "000_Blog/004_scRNA/cancer1.html#data-exploration",
    "title": "A Living Biobank of Ovarian Cancer – Data Acqusition",
    "section": "Data Exploration",
    "text": "Data Exploration\nNow that we’ve taken a peak at the data collection details, let’s explore the actual data. I’ve downloaded the raw counts data from then Single Cell Expression Atlas.\n\nfile.path = './localdata/E-MTAB-8559-quantification-raw-files/'\n\n\nraw.counts &lt;- Matrix::readMM(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx',\n        sep=''\n    )\n)\n\n\ndim(raw.counts)\n\n\n2328419880\n\n\n\nrow.info &lt;- read.table(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx_rows',\n        sep=''\n    ),\n    header=FALSE,\n    col.names=c(\"Ensembl.ID\", \"Redundant\")\n)\n\nif (all(row.info['Ensembl.ID'] == row.info['Redundant'])) {\n    # I don't understand why these datasets tend to write\n    # the same name twice in each row???\n    print(\"Both columns are indeed the same\")\n}\n\n# Drop duplicate column\nrow.info &lt;- row.info['Ensembl.ID']\n\nhead(row.info)\n\n[1] \"Both columns are indeed the same\"\n\n\n\nA data.frame: 6 × 1\n\n\n\nEnsembl.ID\n\n\n\n&lt;chr&gt;\n\n\n\n\n1\nENSG00000000003\n\n\n2\nENSG00000000419\n\n\n3\nENSG00000000457\n\n\n4\nENSG00000000460\n\n\n5\nENSG00000000938\n\n\n6\nENSG00000000971\n\n\n\n\n\n\ncol.info &lt;- read.table(\n    paste(\n        file.path,\n        'E-MTAB-8559.aggregated_filtered_counts.mtx_cols',\n        sep=''\n    ),\n    header=FALSE,\n    sep='-',\n    col.names=c('Sample.Info', 'Cell.Barcode')\n)\ncol.info['Full.ID'] &lt;- apply(\n    col.info,\n    1, # Rows\n    function(row) {\n    paste(\n        row['Sample.Info'],\n        row['Cell.Barcode'],\n        sep='-'\n    )\n})\n\nhead(col.info)\n\n\nA data.frame: 6 × 3\n\n\n\nSample.Info\nCell.Barcode\nFull.ID\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nSAMEA6492740\nAAACCCACAGTTAGGG\nSAMEA6492740-AAACCCACAGTTAGGG\n\n\n2\nSAMEA6492740\nAAACCCACATGTGTCA\nSAMEA6492740-AAACCCACATGTGTCA\n\n\n3\nSAMEA6492740\nAAACCCAGTCGCATGC\nSAMEA6492740-AAACCCAGTCGCATGC\n\n\n4\nSAMEA6492740\nAAACCCAGTCTTTCAT\nSAMEA6492740-AAACCCAGTCTTTCAT\n\n\n5\nSAMEA6492740\nAAACCCATCCGTGTCT\nSAMEA6492740-AAACCCATCCGTGTCT\n\n\n6\nSAMEA6492740\nAAACCCATCCTCTCTT\nSAMEA6492740-AAACCCATCCTCTCTT\n\n\n\n\n\n\nexp.design.table &lt;- read.table(\n    './localdata/ExpDesign-E-MTAB-8559.tsv',\n    header=TRUE,\n    sep='\\t'\n)\n\nWe’ve loaded the data now; interestingly, the data only seems to be from 4 individuals, not 12 as previously mentioned!\n\nunique(col.info$Sample.Info)\nunique(exp.design.table$Sample.Characteristic.individual.)\n\n\n'SAMEA6492740''SAMEA6492741''SAMEA6492742''SAMEA6492743'\n\n\n\n'38b''59''74-1''79'\n\n\nThis is because the scRNA-seq data is only one of may experiments they’ve run on their dataset:\n\nTo extend this analysis, we analysed OCMs 38b, 59, 74–1 and 79 using a 10x Genomics platform.\n– (Nelson et al. 2020)\n\n\n\nOCM stands for ovarian cancer models; which is the name for the cultures that we are studying.\nWe can also see that the data from our counts matrix is not necessarily integer, so we need to know what the counts are actually representing.\n\nraw.counts[1:10, 1:10]\n\n10 x 10 sparse Matrix of class \"dgTMatrix\"\n                           \n [1,] . . . . 1 . . . .   .\n [2,] . 1 1 . . . . . 2.0 .\n [3,] . . . . . 1 . . .   .\n [4,] . . . . . . . . .   .\n [5,] . . . . . . . . .   .\n [6,] . . . . . . . 1 .   .\n [7,] . . . 1 . . . . .   .\n [8,] . . . . . . . . .   .\n [9,] . . . . . . . . .   .\n[10,] . 2 . . 1 . . . 0.5 .\n\n\n\n\nFalse Start: Log Transformed?\n\nOne possible answer is that these “raw” counts have been log-transformed:\n\nRaw counts of the remaining cells were then normalised using the deconvolution-based method and then log transformed.\n\nBut the smallest base that would possibly be used would be 2, and there’s no way \\(2^{2772}\\) genes were observed:\n\nmax(raw.counts)\n\n2771.9734\n\n\n\nAccording to the internet, sometimes values are stated in terms of “expected counts”, since there is luck in RNA sequencing as to how easy it is to paste the reads back together. Looking into the Single Cell Expression Atlas’s section on Supplementary Information, Nelson et al. (2020) used Alevin v1.5.1 (Srivastava et al. 2019), a tool to produce estimated counts. Thus it seems reasonable to conclude that the counts represent estimated counts.\nThat’s all for now - next time, we’ll try to reproduce Figure 5 from the paper."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer1.html#footnotes",
    "href": "000_Blog/004_scRNA/cancer1.html#footnotes",
    "title": "A Living Biobank of Ovarian Cancer – Data Acqusition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs in what were the inclusion/exclusion criteria↩︎\nThere’s another “How”, as in “how were they sampled?”, but that’s a question for once we start looking at the protocol.↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/003_analysis_walkthrough.html",
    "href": "000_Blog/004_scRNA/003_analysis_walkthrough.html",
    "title": "Analyzing scRNA-seq Data – Dataset Acquisition",
    "section": "",
    "text": "In this walkthrough, we’re going to start playing around with real scRNA-seq data. I had no end goal in mind when starting this post; let’s see what happens!"
  },
  {
    "objectID": "000_Blog/004_scRNA/003_analysis_walkthrough.html#experimental-design",
    "href": "000_Blog/004_scRNA/003_analysis_walkthrough.html#experimental-design",
    "title": "Analyzing scRNA-seq Data – Dataset Acquisition",
    "section": "Experimental Design",
    "text": "Experimental Design\nBefore downloading the data, we want to check if it’s actually useful to us - is it mRNA, what type of cells are they, were specific genes targeted, etc. Since the previous blog posts were talking about the wetlab generation of the data, let’s do a deep dive into what they did. The paper (Hernández et al. 2018) is freely available online, and the pertinent information will be contained in the Materials and Methods section. The paper is the source of two datasets on the Single Cell Expression Atlas so we will have to keep that in mind when reading about the methods.\n\nSample Selection\n\nThe aim of this study was to characterised innate and adaptive lymphocytes in zebrafish in steady state and following the immune challenge, using scRNA-seq. Multiple zebrafish, either in steady state or exposed to immune challenge, were used to collect cells for sequencing.\n– Study Design Subsection; Materials and Methods Section; Hernández et al. (2018) \n\nA lymphocyte is a specific type of cell that is part of your immune system; B, T, and NK cells are all lymphocytes. We can also see that the experiment was done on multiple zebrafish, rather than just one - and that some were “exposed to immune challenge”, which I assume means they were made sick to try to trigger interesting processes in their immune system.\nThe paper goes into more depth into this immune challenge (it involves Vibrio anguillarum, a “fish pathogen” (details: Frans et al. 2011)), however this does not seem to be relevant for our dataset. The paper seems to refer to our dataset as the Smart-seq2 experiment, whereas the other dataset is the 10x experiment. The immune challenge was only applied to the 10x experiment. We can double-check this by comparing the sample characteristics of our dataset and the other dataset on the Single Cell Expression Atlas, noting that infect is not one of the experimental variables for our dataset.\nA triple-check, if the above was not convincing enough2, can be found when we read the whole paper:\n\nTo allow easy retrieval of sequencing data from zebrafish innate and adaptive lymphocytes we generated a cloud repository (https://www.sanger.ac.uk/science/tools/lymphocytes/ lymphocytes/) with transcriptional profiles of over 14,000 single cells collected from healthy and immune challenged zebrafish using 10x genomics and Smart-seq2 methodology (please see Explanatory Note in Supplementary Material).\n– “Single cell atlas of innate and adaptive lymphocytes in zebrafish” section; Hernández et al. (2018) \n\nWe can then track down this explanatory note3 to see what it has to say;\n\nAvailable datasets includes Smart-seq2 data from kidney, thymus, spleen, guts and gills of healthy, unstimulated wild-type zebrafish as well as 10x datasets from gut of unstimulated zebrafish both wild-type and rag1-/- mutant and immune-challenged (V. anguillarum- or A. simplex-injected) rag1-/- mutant.\n– Supplementary Material; Explanatory Note Hernández et al. (2018) \n\nWe know the 10x experiment contains over 10,000 cells, and experienced immune challenges. Ours doesn’t. Our dataset should be from multiple body parts (corroborated by the sample characteristics of our dataset) and the 10x dataset should only be from the gut (corroborated by the sample characteristics of the other dataset).\nThe above quote also points out that all our zebrafish were wild-type. Interestingly, the sample characteristics of our dataset do list multiple genotypes, including rag1-/- mutants. The quote seems to imply that rag1-/- is not “wild-type”, as does the rest of the paper. However, the rest of the paper does explicitly say that the Smart-seq2 data contains rag1-/- mutants, so I assume this was either an oversight by the authors or a minor misinterpretation on my part4. Either way, I’m satisfied with this understanding; we can move on to the tissue preparation.\n\n\nAbout rag1-/- mutants\n\n\nWell, maybe not move on just yet; I’m just a bit curious as to what a rag1-/- mutant is! According to “RAG1 Gene” (2017), it’s related to VDJ-recombination (something that was briefly mentioned, but not expanded on, in a prior blogpost)). It stands for “recombination activating gene 1”, and when it is absent it can really mess up your immune system as VDJ-recombination is essential for B and T cells to adapt to new pathogens. For a paper specifically on this mutation’s effects in zebrafish, see Hohn and Petrie-Hanson (2012) and Petrie-Hanson, Hohn, and Hanson (2009).\n\n\n\n\nTissue Preparation\n\nKidneys from heterozygote transgenic zebrafish either wild-type or rag1-/- mutant, were dissected and processed as previously described (Athanasiadis et al. 2017). The guts, spleens, gills and thymuses were dissected and placed in ice cold PBS/5% foetal bovine serum.\n– FACS Sorting Subsection; Materials and Methods Section; Hernández et al. (2018) \n\nWhich gives us another paper to read! Also, I never thought I’d read the phrase “ice cold fetal bovine serum”…\n\nA single kidney from heterozygote transgenic or wild-type fish was dissected and placed in ice cold PBS/5% foetal bovine serum.\n– Single-Cell Sorting Subsection; Methods Section; Athanasiadis et al. (2017) \n\nDespite being given another paper to read, it seems that the same dissection type was used for all organs. That reduces the amount of work for us!\n\n\nCell Isolation\nThey used FACS (Fluorescence-Activated Cell Sorting). There’s a brief overview of it available from “Fluorescence-Activated Cell Sorting (FACS)” (n.d.).\n\nFor [the] Smart-seq2 experiment individual cells were index sorted into 96 well plates using a BD Influx Index Sorter\n– FACS Sorting Subsection; Materials and Methods Section; Hernández et al. (2018) \n\n\n\n96\n\n\nWhy does the number 96 keep popping up??? I’ll never know.\n\n\n\n\nLibrary Preparation and Sequencing\n\nThe Smart-seq2 protocol was used for whole transcriptome amplification and library preparation as previously described. Generated libraries were sequenced in pair-end mode on [a] Hi-Seq4000 platform.\n– Plate-Based Single-Cell RNA processing Subsection; Materials and Methods Section; Hernández et al. (2018) \n\nThis is short and straight to the point. They say a bit more about sequencing later:\n\nFor the samples that were processed using the Smart-seq2 protocol, the reads were aligned to the zebrafish reference genome (Ensemble BioMart version 89) combined with the sequences for EGFP, mCherry, mhc2dab and ERCC spike-ins. Salmon v0.8.2 (Patro et al. 2017) was used for both alignment and quantification of reads with the default paired-end parameters, while library type was set to inward (I) relative orientation (reads face each other) with unstranded (U) protocol (parameter –l IU).\n– Alignment and Quantification of Single-Cell RNA-Sequencing Data; Materials and Methods Section; Hernández et al. (2018)"
  },
  {
    "objectID": "000_Blog/004_scRNA/003_analysis_walkthrough.html#acquiring-and-exploring-the-dataset",
    "href": "000_Blog/004_scRNA/003_analysis_walkthrough.html#acquiring-and-exploring-the-dataset",
    "title": "Analyzing scRNA-seq Data – Dataset Acquisition",
    "section": "Acquiring and Exploring the Dataset",
    "text": "Acquiring and Exploring the Dataset\nThe page for this dataset on the Single Cell Expression Atlas contains a download tab. We want the raw counts matrix (we don’t want to use the normalized counts, since we intend to recreate their analysis and that includes normalizing the counts ourselves!).\nThe file we’re interested in, after downloading and unzipping, is E-MTAB-7117.aggregated_filtered_counts.mtx. (The other files are important too; they’re the row and column names!) We can read the file in using R.\n\nraw.counts &lt;- Matrix::readMM('./localdata/E-MTAB-7117.aggregated_filtered_counts.mtx')\n\nLet’s take a peak at this matrix to see what it’s like.\n\nprint(dim(raw.counts))\nraw.counts[1:10, 1:10]\nmax(raw.counts)\n\n[1] 21797   966\n\n\n10 x 10 sparse Matrix of class \"dgTMatrix\"\n                                          \n [1,] .   .         .      . .   . . . . .\n [2,] .   .         .      . .   . . . . .\n [3,] .   .         .      . .   . . . . .\n [4,] .   .        48.000 77 .   . . . . .\n [5,] 1 680.8777 1151.611  1 .   . . . . .\n [6,] .   .         .      . .   . . . . .\n [7,] .   .         .      . .   . . . . .\n [8,] .   .         .      . . 111 . . . .\n [9,] .   .         .      . .   . . . . .\n[10,] .   .         .      . .   . . . . .\n\n\n292841\n\n\nWe can see that the matrix is 21,797 by 966 - thus, there are 21,797 genes in the dataset and each column corresponds to a specific cell.\nStrangely, some values seem to be noninteger? Let’s consult the paper!\n\nFor each of the 542 single cells, counts reported by Salmon were transformed into normalised counts per million (CPM) and used for the further analysis. This was performed by dividing the number of counts for each gene with the total number of counts for each cell and by multiplying the resulting number by a factor of 1,000,000. Genes that were expressed in less than 1% of cells (e.g. 5 single cells with CPM &gt; 1) were filtered out. In the final step we ended up using 16,059 genes across the 542 single cells. The scran R package (version 1.6.7) Lun, Bach, and Marioni (2016) was then used to normalise the data and remove differences due to the library size or capture efficiency and sequencing depth.\n– Downstream Analysis of Smart-seq2 Data; Materials and Methods Section; Hernández et al. (2018) \n\nThis explains the non-integer values; there’s division involved in producing them.\nHowever, we’ve found a new discrepancy! Why are there 966 cells in the dataset, when we should only have 542? Let’s read a bit more of the paper:\n\nFor the Smart-seq2 protocol transcript per million (TPM) values reported by Salmon were used for the quality control (QC). Wells with fewer than 900 expressed genes (TPM &gt; 1) or having more than either 60% of ERCC or 45% of mitochondrial content were annotated as poor quality cells. As a result, 322 cells failed QC and 542 single cells were selected for the further study.\n– Quality Control of Single-Cell Data; Materials and Methods Section; Hernández et al. (2018) \n\nSo 322 cells failed quality control - but that still only brings us up to 864 cells, 102 too few. Let’s download the experiment design table from the Single Cell Expression Atlas, as we’ll need it for later anyways (it contains non-gene info about the samples) and because it contains info on sample quality via the fields “well information” and “single cell quality”. The name of the file once downloaded is ExpDesign-E-MTAB-7117.tsv.\n\nsample.data &lt;- read.table(\"./localdata/ExpDesign-E-MTAB-7117.tsv\", sep='\\t', header=TRUE)\nhead(sample.data)\n\n\nA data.frame: 6 × 33\n\n\n\nAssay\nSample.Characteristic.organism.\nSample.Characteristic.Ontology.Term.organism.\nSample.Characteristic.strain.\nSample.Characteristic.Ontology.Term.strain.\nSample.Characteristic.age.\nSample.Characteristic.Ontology.Term.age.\nSample.Characteristic.developmental.stage.\nSample.Characteristic.Ontology.Term.developmental.stage.\nSample.Characteristic.sex.\n⋯\nSample.Characteristic.single.cell.quality.\nSample.Characteristic.Ontology.Term.single.cell.quality.\nSample.Characteristic.cluster.\nSample.Characteristic.Ontology.Term.cluster.\nFactor.Value.genotype.\nFactor.Value.Ontology.Term.genotype.\nFactor.Value.organism.part.\nFactor.Value.Ontology.Term.organism.part.\nFactor.Value.single.cell.identifier.\nFactor.Value.Ontology.Term.single.cell.identifier.\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n⋯\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\nERR2722968\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_A12\nNA\n\n\n2\nERR2722969\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_A4\nNA\n\n\n3\nERR2722970\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_B1\nNA\n\n\n4\nERR2722971\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_B2\nNA\n\n\n5\nERR2722972\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_C10\nNA\n\n\n6\nERR2722973\nDanio rerio\nhttp://purl.obolibrary.org/obo/NCBITaxon_7955\nAB\nNA\n6 month\nNA\nadult\nhttp://www.ebi.ac.uk/efo/EFO_0001272\nmale\n⋯\nOK\nNA\n1\nNA\nTg(lck:EGFP)\nNA\nkidney\n\nLCK_kidney_C11\nNA\n\n\n\n\n\n\ndim(sample.data)\n\n\n105633\n\n\n\ndim(sample.data[which(\n    sample.data['Sample.Characteristic.single.cell.quality.'] == \"OK\"\n    & sample.data['Sample.Characteristic.well.information.'] == \"single cell\"\n),])\n\n\n97633\n\n\n\nfiltered &lt;- sample.data[which(\n    sample.data['Sample.Characteristic.cluster.'] != \"unknown\"\n),]\n\n\ndim(filtered)\n\n\n54233\n\n\nThe mystery keeps compounding! For some reason, we start with 90 extra rows (cells) in this dataset! We can filter out the obviously bad entries, but we still have 10 extra rows. There’s a field “cluster” which we can use to get the post-filtered value of 542, although I’m fairly certain that these clusters were created after the analysis rather than as part of the data creation, hence why there’s only 542.\nSince we know the names of all ‘names’ of all the samples in our experiment design table, we can compare this with the list of row names for our gene count matrix to hopefully shed some light on what’s going wrong:\n\n# Grab the lists of genes in the experiment\n# and in the gene counts, for comparison\nexperiment.assays &lt;- sample.data$Assay\ngene.assays &lt;- read.table(\n    './localdata/E-MTAB-7117.aggregated_filtered_counts.mtx_cols'\n)$V1\n\n\n# Get the list of genes that appear in the experiment data\n# but not the gene counts\ndiscrepancies &lt;- setdiff(experiment.assays, gene.assays)\nlength(discrepancies)\n\n90\n\n\n\n# Now get the experiment info of the 10 of these cells that\n# aren't filtered out by our previous filtering method\nextra.cells &lt;- sample.data[\n    which(sapply(experiment.assays, `%in%`, discrepancies)),\n]\nextra.cells &lt;- extra.cells[\n    which(\n        extra.cells['Sample.Characteristic.single.cell.quality.'] == \"OK\"\n    ),\n]\n\n\n# Show all columns.  There's probably a better way,\n# but oh well...\n# (I've hidden the outputs of this code block on the blog,\n# because it's just a bunch of ugly, unenlightening tables.)\nextra.cells[,1:10]\nextra.cells[,11:20]\nextra.cells[,21:30]\nextra.cells[,31:33]\n\nThe above contains the 10 outstanding cells. Frustratingly, there doesn’t seem to be any reason I can find as to why these cells were excluded at this stage!\nTo summarize:\n\nThe website reports 966 cells\nThe gene count table also reports 966 cells\nThe experiment table reports 1056 cells\nFlitering the experiment table yields 976 cells\nThe paper reports 864 cells pre-quality control\n\nThis isn’t an uncommon occurance; in fact, every time I’ve read a paper like this (which, admittedly, is only 3ish times) I’ve been unable to get a consistent answer on how many cells were actually used! It’s a major nuisance.\nWe’ll pick up where we left off in the next blog post. While annoying, this discrepancy isn’t fatal - we can perform quality control on the 976 cells we get from filtering.\n\nReferences\n\n\nAthanasiadis, E. I., J. G. Botthof, H. Andres, L. Ferreira, P. Lio, and A. Cvejic. 2017. “Single-cell RNA-sequencing uncovers transcriptional states and fate decisions in haematopoiesis” 8 (1): 2045.\n\n\n“Fluorescence-Activated Cell Sorting (FACS).” n.d. SinoBiological. https://www.sinobiological.com/category/fcm-facs-facs.\n\n\nFrans, I., C. W. Michiels, P. Bossier, K. A. Willems, B. Lievens, and H. Rediers. 2011. “Vibrio anguillarum as a fish pathogen: virulence factors, diagnosis and prevention.” Journal of Fish Diseases 34 (9): 643–61.\n\n\nHernández, P. P., P. M. Strzelecka, E. I. Athanasiadis, D. Hall, A. F. Robalo, C. M. Collins, P. Boudinot, J. P. Levraud, and A. Cvejic. 2018. “Single-cell transcriptional analysis reveals ILC-like cells in zebrafish.” Science Immunology 3 (29).\n\n\nHohn, C., and L. Petrie-Hanson. 2012. “Rag1-/- mutant zebrafish demonstrate specific protection following bacterial re-exposure.” PLoS One 7 (9): e44451.\n\n\nLucero, Sophia. 2008. “HTML5: &lt;B&gt; and &lt;i&gt; Tags Are Going to Be Useful (Read: Semantic) Again!” https://stellify.net/html5-b-and-i-tags-are-going-to-be-useful-read-semantic-again/.\n\n\nLun, A. T., K. Bach, and J. C. Marioni. 2016. “Pooling across cells to normalize single-cell RNA sequencing data with many zero counts.” Genome Biol 17 (April): 75.\n\n\nPatro, R., G. Duggal, M. I. Love, R. A. Irizarry, and C. Kingsford. 2017. “Salmon provides fast and bias-aware quantification of transcript expression.” Nat Methods 14 (4): 417–19.\n\n\nPetrie-Hanson, L., C. Hohn, and L. Hanson. 2009. “Characterization of rag1 mutant zebrafish leukocytes.” BMC Immunology 10 (February): 8.\n\n\n“RAG1 Gene.” 2017, February. https://medlineplus.gov/genetics/gene/rag1/."
  },
  {
    "objectID": "000_Blog/004_scRNA/003_analysis_walkthrough.html#footnotes",
    "href": "000_Blog/004_scRNA/003_analysis_walkthrough.html#footnotes",
    "title": "Analyzing scRNA-seq Data – Dataset Acquisition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApparently it does seem to be one of the ‘useful’ animals in biology - you learn new things every day! Shoutout to Sam’s blog for introducing me to being able to use footnotes.↩︎\nIt wasn’t really, at least for me - I like to be sure!↩︎\nI found it on NIH; warning - you’ll have to download a 16MB file to read it!↩︎\nI’m leaning towards misinterpretation, as I don’t see why wild populations would be incapable of having a rag1-/- mutation - unless maybe the rag1-/- mutants used in the study were specifically bred in the lab?↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/005_ensemble.html",
    "href": "000_Blog/004_scRNA/005_ensemble.html",
    "title": "Exploring Ensembl",
    "section": "",
    "text": "So, I’ve been working on blog posts that do a walkthrough on working with genetic data. I keep running into issues and having to go on sidequests (😅) but I think this one deserves its own blog post. Shoutout to Morgan for the help.\n\n\n\nThe Ensemble homepage\n\n\nA major resource in bioinformatics is Ensembl. In this blog post, we’ll spend some time exploring it, capping off with using it to accomplish the useful task of how to grab all known mitochondrial genes in Zebrafish (Danio rerio).\nSuppose in our dataset we had a gene called ENSDARG00000000001. What can we actually say about it? Well, first of all, this is an Ensembl ID, so it follows the pattern ENS[species prefix][feature type prefix][a unique eleven digit number]. In this case, DAR is the species (Danio rerio) and G indicates that it is referring to a gene.\nWe can look this gene up on Ensembl to learn more about it:\n\nThis gene is also known as slc35a5, but these names can be harder to work with as it is subject to change if our knowledge about its role in biological processes changes; the Ensembl ID is more permanent. On the other hand, this name is arguably more informative, as it is an abbreviation for its role as “solute carrier family 35 member A5”.\nIf we want to investigate this gene more, we can click on the ZFIN link provided in the summary section.\n\n\n\nZFIN page for ENSDARG00000000001\n\n\nHere we can see yet another gene ID (ZDB-GENE-030616-55), as well as a link to the naming history of this gene which may be fun to explore. There are some other goodies on this page, but we’ll return to Enbembl as that is the topic of this post.\nOur goal is to get a list of all mitochondrial genes in Danio rerio. ENSDARG00000000001 is not a mitochondrial gene, because it is located on chromosome 9. One way to find a list of mitochondrial genes is to search for genes with names beginning with mt-, because that is how mitochondrial genes are named.\n\nHowever, this isn’t convenient for manual use!\n\n\nFailed attempt at using scanpy\n\nOne way to try to get this data is to use scanpy:\n\n# I had to also install pybiomart, which was only on pip\nfrom scanpy import queries\n\n\nqueries.mitochondrial_genes(\n    \"drerio\",\n    attrname = \"ensembl_gene_id\"\n)\n\nHTTPError: 500 Server Error: Internal Server Error for url: http://www.ensembl.org:80/biomart/martservice?type=registry\n\n\nUnfortunately, Biomart was down at the time I tried to do this! (Biomart seems to be the api for this type of stuff).\n\nUse this data-mining tool to export custom datasets from Ensembl.\n– Ensemble docs about what Biomart is\n\nI found that the “asia” mirror of Biomart gives a different error:\n\nmitgenes = queries.mitochondrial_genes(\n    \"drerio\",\n    attrname = \"ensembl_gene_id\",\n    host = \"asia.ensembl.org\"\n)\nmitgenes\n\nHTTPError: 504 Server Error: Gateway Time-out for url: http://asia.ensembl.org:80/biomart/martservice?type=registry\n\n\n\nAnother way to get the data is to search by location. We can play around with their region-searcher to see that the mitochondrial dna of a zebrafish is just over 16 kilobases long (16,596 bases to be exact).\n\nIf we do that, we can find this cute overview of the mitochondrial dna:\n\nNot relevant to us, though - we want to click on the “export” button, which will give us this popup:\n\nYou can then download all the mitochondrial data! It’s a rather small file that looks like this:\nseqname,source,feature,start,end,score,strand,frame,hid,hstart,hend,genscan,gene_id,transcript_id,exon_id,gene_type,variation_name,probe_name\nMT,EVA,variation,113,113,.,+,.,,,,,,,,,rs508804888,\n[...]\nMT,Ensembl,gene,951,1019,.,+,0,,,,,ENSDARG00000083480.3,ENSDART00000116162.3,ENSDARE00000880048,Mt_tRNA,,\n[...]\n\nimport pandas as pd\n\n\nmito_data = pd.read_csv(\"./localdata/mt-danio-rerio.txt\", sep=\",\")\nmito_data\n\n\n\n\n\n\n\n\nseqname\nsource\nfeature\nstart\nend\nscore\nstrand\nframe\nhid\nhstart\nhend\ngenscan\ngene_id\ntranscript_id\nexon_id\ngene_type\nvariation_name\nprobe_name\n\n\n\n\n0\nMT\nEVA\nvariation\n113\n113\n.\n+\n.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nrs508804888\nNaN\n\n\n1\nMT\nEVA\nvariation\n239\n239\n.\n+\n.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nrs513784503\nNaN\n\n\n2\nMT\nEVA\nvariation\n314\n314\n.\n+\n.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nrs504526537\nNaN\n\n\n3\nMT\nEVA\nvariation\n339\n339\n.\n+\n.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nrs511578098\nNaN\n\n\n4\nMT\nEVA\nvariation\n438\n438\n.\n+\n.\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nrs514602887\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n174\nMT\nEnsembl\ngene\n14714\n15232\n.\n-\n0\nNaN\nNaN\nNaN\nNaN\nENSDARG00000063922.3\nENSDART00000093623.3\nENSDARE00000685766\nprotein_coding\nNaN\nNaN\n\n\n175\nMT\nEnsembl\ngene\n15233\n15301\n.\n-\n0\nNaN\nNaN\nNaN\nNaN\nENSDARG00000083312.3\nENSDART00000116823.3\nENSDARE00000882905\nMt_tRNA\nNaN\nNaN\n\n\n176\nMT\nEnsembl\ngene\n15308\n16448\n.\n+\n0\nNaN\nNaN\nNaN\nNaN\nENSDARG00000063924.3\nENSDART00000093625.3\nENSDARE00000685768\nprotein_coding\nNaN\nNaN\n\n\n177\nMT\nEnsembl\ngene\n16449\n16520\n.\n+\n0\nNaN\nNaN\nNaN\nNaN\nENSDARG00000083462.3\nENSDART00000116552.3\nENSDARE00000881627\nMt_tRNA\nNaN\nNaN\n\n\n178\nMT\nEnsembl\ngene\n16527\n16596\n.\n-\n1\nNaN\nNaN\nNaN\nNaN\nENSDARG00000081475.3\nENSDART00000115546.3\nENSDARE00000882281\nMt_tRNA\nNaN\nNaN\n\n\n\n\n179 rows × 18 columns\n\n\n\n\n# Grab all the genes, get their ids, and chop off the '.3' \n# ending which indicates the version\nmito_genes = mito_data[mito_data[\"feature\"] == \"gene\"][\"gene_id\"].apply(\n    lambda x: x[:-2]\n)\nmito_genes\n\n142    ENSDARG00000083480\n143    ENSDARG00000082753\n144    ENSDARG00000081443\n145    ENSDARG00000080337\n146    ENSDARG00000083046\n147    ENSDARG00000063895\n148    ENSDARG00000083118\n149    ENSDARG00000080630\n150    ENSDARG00000082084\n151    ENSDARG00000063899\n152    ENSDARG00000080718\n153    ENSDARG00000080401\n154    ENSDARG00000081938\n155    ENSDARG00000082789\n156    ENSDARG00000080128\n157    ENSDARG00000063905\n158    ENSDARG00000081369\n159    ENSDARG00000083519\n160    ENSDARG00000063908\n161    ENSDARG00000080151\n162    ENSDARG00000063910\n163    ENSDARG00000063911\n164    ENSDARG00000063912\n165    ENSDARG00000081758\n166    ENSDARG00000063914\n167    ENSDARG00000080329\n168    ENSDARG00000063916\n169    ENSDARG00000063917\n170    ENSDARG00000082716\n171    ENSDARG00000082123\n172    ENSDARG00000081280\n173    ENSDARG00000063921\n174    ENSDARG00000063922\n175    ENSDARG00000083312\n176    ENSDARG00000063924\n177    ENSDARG00000083462\n178    ENSDARG00000081475\nName: gene_id, dtype: object\n\n\n\nmito_genes.to_csv(\"./localdata/mito-genes.csv\", index=False)\n\nSo, uh, yeah - success!"
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna2.html",
    "href": "000_Blog/004_scRNA/scrna2.html",
    "title": "Overview of scRNA-seq Data Creation – Sequencing",
    "section": "",
    "text": "I’m heavily indebted to Morgan for help in understanding this topic. I’m still learning about it, so don’t take my word as gospel.\n\n\nLibrary preparation is the collection of the bits of data that you’re actually interested in (these bits would be your ‘library’). The methods I’ve seen do this by attaching something to the mRNA molecules that we can take advantage of later. For scRNA-seq we are interested in the transcriptome (this is the set of RNA thingamajigs in the cell). Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) lists a few library preparation techniques for the transcriptome:\n\n\nFull-length RNA-seq\nmRNA end-tag amplification\nTargeted panels\n\nTargeted towards measuring specific things\n\nIR-seq\n\nSpecifically used for B and T cells.\n\n\n\nThese are important for me to understand so I’ll definitely be delving into this topic deeper in the future.\n\n\n\nWe’ve now isolated our cells and grabbed/tagged the bits that we’re interested in. Now, we want to actually be able to learn about the mRNA that we’ve measured. To do this, we need to sequence it.\n\n\nOne important concept for sequencing is the Flow Cell. Intuitively, the flow cell is a sticky plate that will grab the mRNA. An Introduction to Next-Generation Sequencing Technology (2017) defines them as the following:\n\nFlow Cell: A glass slide with one, two, or eight physically separated lanes, depending on the instrument platform. Each lane is coated with a lawn of surface bound, adapter-complimentary oligos. A single library or a pool of up to 96 multiplexed libraries can be run per lane, depending on application parameters.\n– Illumina handbook on next generation sequencing (An Introduction to Next-Generation Sequencing Technology 2017)\n\nAn oligo is a short strand of synthetic DNA (“What Is an Oligo?” 2019). These are the “sticky bits”. Multiplexing is the process of sequencing multiple samples at a time; this can be useful as sequencing produces a lot of data, more than may be necessary for a single project on its own (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.). This can be done by attaching molecular barcodes to the prepared cDNA to indicate the original sample from which the cDNA hails.\n\n\nThe flow cell definition given by Illumina is specifically tuned to their flow cells - this may have been obvious due to the use of specific numbers like “96”.\n\n\nTangential Observation\n\nSurprisingly, though, the competitor company Oxford Nanopore also uses this number. I assume this is due to some chemical feature of the bardcodes, but it probably isn’t as simple as that since I know you can have more than 96 UMIs which I thought would be a similar process…\n\n Illumina uses a Sequencing by Synthesis approach; we’ll look at another approach, Nanopore Sequencing, later. During Illumina’s library preparation phase they convert the RNA to cDNA.\n\n\n\nExample of a patterned flow cell\n\n\nThe above diagram is that of a 2D slice of a Patterned Flow Cell. These differ from Nonpatterned Flow Cells in the use of nanowells (little valleys in the cell) to ideally keep fragments of DNA from binding to nearby oligos. Patterned flow cells are a more recent innovation, and for a brief overview of the differences you can read “Calculating Percent Passing Filter for Patterned and Nonpatterned Flow Cells” (2017) and Wingett (2017).\nThe cDNA created during library preparation is added onto the flow cell. The strands of cDNA (reads) will bind to the oligos, and then start multiplying (“bridge amplification”) so that duplicates of the read will be bound to nearby oligos. For patterned flow cells, this should mean that the nanowells are full of copies of the same read - for unpatterned flow cells the clusters are more chaotic.\nAfter you’ve prepared your flow cell, you’ll plug it into a sequencer to read the base pairs. These use chemical tricks to cause each base pair in a read to let out a flash of a specific color. The sequence and color of these flashes will inform the sequencer of what the base pair sequence is. After doing this, you’ll have raw data on the contents of each read, likely in the bcl file format (bcl is the output for Illumina sequencers).\n\n\n\nWhile most of this blog has focused on Illumina, Oxford Nanopore is definitely worth talking about. They use Nanopore Sequencing instead of Sequencing by Synthesis. A brief comparison of the two companines is given by “NGS Platforms: Illumina Vs Oxford Nanopore” (2022).\n\n\n¿cDNA?\n\nIt is possible to directly use RNA, instead of cDNA, depending on what is done at the library preparation stage. I’ll assume we use cDNA here. It would make an interesting blog post, I think, to delve into the intricacies as to what is going on and why we should care about cDNA vs RNA.\n\n\nNanopore sequencing is a newer method; instead of sticking to an oligo, the cDNA passes through a nanopore. To do this, the cDNA has a motor protein attached to it during the library preparation stage. The act of passing through the nanopore creates a detectable electrical signal that is dependent on the bases in the cDNA. By measuring this signal, we can sequence the molecule.\n\n\n\nNanopore sequencing. The cDNA will pass through the nanopore as it gets sequenced. The tether will grab on to the motor protein to accomplish this. (“Product Brochure” 2022)\n\n\nOne interesting advantage of nanopore sequencing is the ability to detect modified bases (which I didn’t know were a thing!) (“Delivering the Future of Genomic Pathogen Surveillance” 2022). Basically, the structure that makes up your G, T, A, C, U nucleic acid bases can actually get modified in certain ways, turning them into different sub-molecules that may affect gene expression.\nOxford Nanopore outputs FAST5 files, a type of HDF5 file. These contain the raw electrical signals the nanopores measured. One can then perform basecalling to determine the sequence of bases corresponding to the signals. Due to the aforementioned detectability of modified bases, these raw electrical signals can have value rather than just looking at an end product of everything converted to G/T/A/C/U (Su 2019).\n\n\n\nFlow cells have two key measures (besides data quality); the amount of reads and the read length. Read counts can be in the hundreds of millions. Read lengths may be much smaller; the flow cells paired with the Illumina NextSeq 550 can only have reads as long as 150 base pairs on average. Oxford Nanopore (and in general any nanopore method) can have unbounded read lengths. Longer reads are desirable from the perspective of reconstructing larger sequences, as it is easier to determine if two segments have a significant overlap.\n\n\n\nMy theory as to why it’s called “depth”\n\n\nAn important concept is coverage, or the amount of times a gene has a read associated with it. Higher coverage is better as it allows us to piece together the gene more accurately and weed out incorrect bases. This is also called sequence depth. Deep sequencing is important for detecting rare genes. This can be affected by the steps done during library preparation; immune cells, which undergo VDJ recombination - in short, each cell has its own unique markers that need to be accounted for (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.)\nFor single cell applications depth refers to the amount of reads per cell instead of the reads per base pair measure used for bulk sequencing. When cell populations are more homogenous, the depth should be larger as false overlaps will be more likely.\nFinally, there’s the decision of whether to use paired-end or single-end sequencing. DNA and RNA are linear structures with two different ends - the 5’ and 3’ ends (“5-prime” and “3-prime”). By reading from both the ends, we can better find errors as well as increase the read length (intuitively: instead of reading n base pairs in one direction, you can read 2n total pairs with n in each direction). These advantages decrease when you have UMIs (unique molecular identifiers) or other feature you can take advantage of (such as VDJs), but that is out of scope for this blog post. (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.) The advantages are not completely nullified, as single-end sequencing cannot detect certain types of errors (such as “indel” errors). (Single-Cell Sequencing Workflow: Critical Steps and Considerations 2019)\n\n\n\n\n\n\nAn Introduction to Next-Generation Sequencing Technology. 2017. Illumina. https://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf.\n\n\n“Calculating Percent Passing Filter for Patterned and Nonpatterned Flow Cells.” 2017. Illumina. https://emea.illumina.com/content/dam/illumina-marketing/documents/products/technotes/hiseq-x-percent-pf-technical-note-770-2014-043.pdf.\n\n\n“Delivering the Future of Genomic Pathogen Surveillance.” 2022. Oxford Nanopore. https://nanoporetech.com/resource-centre/white-paper/future-of-genomic-pathogen-surveillance.\n\n\n“NGS Considerations: Coverage, Read Length, Multiplexing.” n.d. iRepertoire. https://irepertoire.com/ngs-considerations-coverage-read-length-multiplexing/.\n\n\n“NGS Platforms: Illumina Vs Oxford Nanopore.” 2022. Fios Genomics. https://www.fiosgenomics.com/ngs-platforms-illumina-vs-oxford-nanopore/.\n\n\n“Product Brochure.” 2022. Oxford Nanopore. https://nanoporetech.com/sites/default/files/s3/literature/product-brochure.pdf.\n\n\nSingle-Cell Sequencing Workflow: Critical Steps and Considerations. 2019. Illumina. https://emea.illumina.com/destination/multiomics-transcriptomics.html.\n\n\nSu, Shian. 2019. “A Look at the Nanopore Fast5 Format,” May. https://medium.com/@shiansu/a-look-at-the-nanopore-fast5-format-f711999e2ff6.\n\n\n“What Is an Oligo?” 2019. Thermo Fisher Scientific; Behind the Bench. https://www.thermofisher.com/blog/behindthebench/what-is-an-oligo/.\n\n\nWingett, Steven. 2017. “Illumina Patterned Flow Cells Generate Duplicated Sequences.” https://sequencing.qcfail.com/articles/illumina-patterned-flow-cells-generate-duplicated-sequences/."
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna2.html#library-preparation",
    "href": "000_Blog/004_scRNA/scrna2.html#library-preparation",
    "title": "Overview of scRNA-seq Data Creation – Sequencing",
    "section": "",
    "text": "Library preparation is the collection of the bits of data that you’re actually interested in (these bits would be your ‘library’). The methods I’ve seen do this by attaching something to the mRNA molecules that we can take advantage of later. For scRNA-seq we are interested in the transcriptome (this is the set of RNA thingamajigs in the cell). Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) lists a few library preparation techniques for the transcriptome:\n\n\nFull-length RNA-seq\nmRNA end-tag amplification\nTargeted panels\n\nTargeted towards measuring specific things\n\nIR-seq\n\nSpecifically used for B and T cells.\n\n\n\nThese are important for me to understand so I’ll definitely be delving into this topic deeper in the future."
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna2.html#sequencing",
    "href": "000_Blog/004_scRNA/scrna2.html#sequencing",
    "title": "Overview of scRNA-seq Data Creation – Sequencing",
    "section": "",
    "text": "We’ve now isolated our cells and grabbed/tagged the bits that we’re interested in. Now, we want to actually be able to learn about the mRNA that we’ve measured. To do this, we need to sequence it.\n\n\nOne important concept for sequencing is the Flow Cell. Intuitively, the flow cell is a sticky plate that will grab the mRNA. An Introduction to Next-Generation Sequencing Technology (2017) defines them as the following:\n\nFlow Cell: A glass slide with one, two, or eight physically separated lanes, depending on the instrument platform. Each lane is coated with a lawn of surface bound, adapter-complimentary oligos. A single library or a pool of up to 96 multiplexed libraries can be run per lane, depending on application parameters.\n– Illumina handbook on next generation sequencing (An Introduction to Next-Generation Sequencing Technology 2017)\n\nAn oligo is a short strand of synthetic DNA (“What Is an Oligo?” 2019). These are the “sticky bits”. Multiplexing is the process of sequencing multiple samples at a time; this can be useful as sequencing produces a lot of data, more than may be necessary for a single project on its own (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.). This can be done by attaching molecular barcodes to the prepared cDNA to indicate the original sample from which the cDNA hails.\n\n\nThe flow cell definition given by Illumina is specifically tuned to their flow cells - this may have been obvious due to the use of specific numbers like “96”.\n\n\nTangential Observation\n\nSurprisingly, though, the competitor company Oxford Nanopore also uses this number. I assume this is due to some chemical feature of the bardcodes, but it probably isn’t as simple as that since I know you can have more than 96 UMIs which I thought would be a similar process…\n\n Illumina uses a Sequencing by Synthesis approach; we’ll look at another approach, Nanopore Sequencing, later. During Illumina’s library preparation phase they convert the RNA to cDNA.\n\n\n\nExample of a patterned flow cell\n\n\nThe above diagram is that of a 2D slice of a Patterned Flow Cell. These differ from Nonpatterned Flow Cells in the use of nanowells (little valleys in the cell) to ideally keep fragments of DNA from binding to nearby oligos. Patterned flow cells are a more recent innovation, and for a brief overview of the differences you can read “Calculating Percent Passing Filter for Patterned and Nonpatterned Flow Cells” (2017) and Wingett (2017).\nThe cDNA created during library preparation is added onto the flow cell. The strands of cDNA (reads) will bind to the oligos, and then start multiplying (“bridge amplification”) so that duplicates of the read will be bound to nearby oligos. For patterned flow cells, this should mean that the nanowells are full of copies of the same read - for unpatterned flow cells the clusters are more chaotic.\nAfter you’ve prepared your flow cell, you’ll plug it into a sequencer to read the base pairs. These use chemical tricks to cause each base pair in a read to let out a flash of a specific color. The sequence and color of these flashes will inform the sequencer of what the base pair sequence is. After doing this, you’ll have raw data on the contents of each read, likely in the bcl file format (bcl is the output for Illumina sequencers).\n\n\n\nWhile most of this blog has focused on Illumina, Oxford Nanopore is definitely worth talking about. They use Nanopore Sequencing instead of Sequencing by Synthesis. A brief comparison of the two companines is given by “NGS Platforms: Illumina Vs Oxford Nanopore” (2022).\n\n\n¿cDNA?\n\nIt is possible to directly use RNA, instead of cDNA, depending on what is done at the library preparation stage. I’ll assume we use cDNA here. It would make an interesting blog post, I think, to delve into the intricacies as to what is going on and why we should care about cDNA vs RNA.\n\n\nNanopore sequencing is a newer method; instead of sticking to an oligo, the cDNA passes through a nanopore. To do this, the cDNA has a motor protein attached to it during the library preparation stage. The act of passing through the nanopore creates a detectable electrical signal that is dependent on the bases in the cDNA. By measuring this signal, we can sequence the molecule.\n\n\n\nNanopore sequencing. The cDNA will pass through the nanopore as it gets sequenced. The tether will grab on to the motor protein to accomplish this. (“Product Brochure” 2022)\n\n\nOne interesting advantage of nanopore sequencing is the ability to detect modified bases (which I didn’t know were a thing!) (“Delivering the Future of Genomic Pathogen Surveillance” 2022). Basically, the structure that makes up your G, T, A, C, U nucleic acid bases can actually get modified in certain ways, turning them into different sub-molecules that may affect gene expression.\nOxford Nanopore outputs FAST5 files, a type of HDF5 file. These contain the raw electrical signals the nanopores measured. One can then perform basecalling to determine the sequence of bases corresponding to the signals. Due to the aforementioned detectability of modified bases, these raw electrical signals can have value rather than just looking at an end product of everything converted to G/T/A/C/U (Su 2019).\n\n\n\nFlow cells have two key measures (besides data quality); the amount of reads and the read length. Read counts can be in the hundreds of millions. Read lengths may be much smaller; the flow cells paired with the Illumina NextSeq 550 can only have reads as long as 150 base pairs on average. Oxford Nanopore (and in general any nanopore method) can have unbounded read lengths. Longer reads are desirable from the perspective of reconstructing larger sequences, as it is easier to determine if two segments have a significant overlap.\n\n\n\nMy theory as to why it’s called “depth”\n\n\nAn important concept is coverage, or the amount of times a gene has a read associated with it. Higher coverage is better as it allows us to piece together the gene more accurately and weed out incorrect bases. This is also called sequence depth. Deep sequencing is important for detecting rare genes. This can be affected by the steps done during library preparation; immune cells, which undergo VDJ recombination - in short, each cell has its own unique markers that need to be accounted for (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.)\nFor single cell applications depth refers to the amount of reads per cell instead of the reads per base pair measure used for bulk sequencing. When cell populations are more homogenous, the depth should be larger as false overlaps will be more likely.\nFinally, there’s the decision of whether to use paired-end or single-end sequencing. DNA and RNA are linear structures with two different ends - the 5’ and 3’ ends (“5-prime” and “3-prime”). By reading from both the ends, we can better find errors as well as increase the read length (intuitively: instead of reading n base pairs in one direction, you can read 2n total pairs with n in each direction). These advantages decrease when you have UMIs (unique molecular identifiers) or other feature you can take advantage of (such as VDJs), but that is out of scope for this blog post. (“NGS Considerations: Coverage, Read Length, Multiplexing,” n.d.) The advantages are not completely nullified, as single-end sequencing cannot detect certain types of errors (such as “indel” errors). (Single-Cell Sequencing Workflow: Critical Steps and Considerations 2019)\n\n\n\n\n\n\nAn Introduction to Next-Generation Sequencing Technology. 2017. Illumina. https://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf.\n\n\n“Calculating Percent Passing Filter for Patterned and Nonpatterned Flow Cells.” 2017. Illumina. https://emea.illumina.com/content/dam/illumina-marketing/documents/products/technotes/hiseq-x-percent-pf-technical-note-770-2014-043.pdf.\n\n\n“Delivering the Future of Genomic Pathogen Surveillance.” 2022. Oxford Nanopore. https://nanoporetech.com/resource-centre/white-paper/future-of-genomic-pathogen-surveillance.\n\n\n“NGS Considerations: Coverage, Read Length, Multiplexing.” n.d. iRepertoire. https://irepertoire.com/ngs-considerations-coverage-read-length-multiplexing/.\n\n\n“NGS Platforms: Illumina Vs Oxford Nanopore.” 2022. Fios Genomics. https://www.fiosgenomics.com/ngs-platforms-illumina-vs-oxford-nanopore/.\n\n\n“Product Brochure.” 2022. Oxford Nanopore. https://nanoporetech.com/sites/default/files/s3/literature/product-brochure.pdf.\n\n\nSingle-Cell Sequencing Workflow: Critical Steps and Considerations. 2019. Illumina. https://emea.illumina.com/destination/multiomics-transcriptomics.html.\n\n\nSu, Shian. 2019. “A Look at the Nanopore Fast5 Format,” May. https://medium.com/@shiansu/a-look-at-the-nanopore-fast5-format-f711999e2ff6.\n\n\n“What Is an Oligo?” 2019. Thermo Fisher Scientific; Behind the Bench. https://www.thermofisher.com/blog/behindthebench/what-is-an-oligo/.\n\n\nWingett, Steven. 2017. “Illumina Patterned Flow Cells Generate Duplicated Sequences.” https://sequencing.qcfail.com/articles/illumina-patterned-flow-cells-generate-duplicated-sequences/."
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells.html",
    "href": "000_Blog/004_scRNA/chicken-cells.html",
    "title": "A tale of two chickens",
    "section": "",
    "text": "Today we’ll be looking at the paper “A single-cell transcriptomic atlas of the developing chicken limb”, by Feregrino et al. (2019) - specifically, we’ll use this dataset from the Single Cell Expression Atlas. However, we’re not actually gonna read their paper, we’re just gonna do random stuff!\nSpoilers: We don’t get much work done, due to a mystery involving chicken breeds in Ensembl.\nI wanted to perform QC using mitochondrial genes. This should have been as easy as reusing my code for humans with a few tweaks to look for chicken genes instead. However, when I try to use biomaRt I get no result.\nmart &lt;- useDataset(\"ggallus_gene_ensembl\", useMart(\"ensembl\"))\n\ngene.to.symbol.map &lt;- getBM(\n    filters=\"ensembl_gene_id\",\n    attributes=c(\n        \"ensembl_gene_id\",\n        \"hgnc_symbol\"\n    ),\n    values=rownames(sce),\n    mart=mart\n)\ngene.to.symbol.map\n\n\nA data.frame: 0 × 2\n\n\nensembl_gene_id\nhgnc_symbol\n\n\n&lt;lgl&gt;\n&lt;lgl&gt;\nSo I went directly to the biomaRt web interface, and searched for a gene I knew was in the dataset: ENSGALG00000000003\nHowever, you can see that in the top left corner it reports no results! If you look closely, you can see that the example search is with ENSGALG00010000002, which does have results - and in fact adding 10,000,000 to gene numbers tends to create a valid findable gene:\nThese genes don’t seem to be related to eachother though.\nIf we filter for mitochondrial genes:\nThen we do get results:\nWhich all have ids greater than 10,000,000. This list is not comprehensive, I can find (off biomaRt) another mitochondrial gene, whose id is less than 10,000,000:\nFrom the above, we can see that there should be 24 mt-rRNA and mt-tRNA in total, which is in fact what we find with biomaRt! (The previous image only shows 10, but that’s because it only shows 10 per page - the total amount found was 24).\nThese two images seem to conclusively show that the gene sets are different; the genes are in a similar place but have different names between the two tracks.\nFrom this, we can hypothesize that the ids less than 10,000,000 are out of date, presenting some frustration for using the data we have. Can we prove it?\nNo: the gene has never changed its ID (which is a good thing for sanity purposes but is not ideal for this specific confusion).\nHowever, it seems like there are multiple chicken genomes in Ensembl:\nThis proves, conclusively, that there are at least two breeds of chicken. And I was looking at the wrong one.\nThe question then becomes - how do we actually find the right one? We can get a list of all the possible biomart-queryable datasets:\nall.datasets &lt;- listDatasets(mart = mart)\nall.datasets[70:80,]\n\n\nA data.frame: 11 × 3\n\n\n\ndataset\ndescription\nversion\n\n\n\n&lt;I&lt;chr&gt;&gt;\n&lt;I&lt;chr&gt;&gt;\n&lt;I&lt;chr&gt;&gt;\n\n\n\n\n70\nfheteroclitus_gene_ensembl\nMummichog genes (Fundulus_heteroclitus-3.0.2)\nFundulus_heteroclitus-3.0.2\n\n\n71\ngaculeatus_gene_ensembl\nStickleback genes (BROAD S1)\nBROAD S1\n\n\n72\ngevgoodei_gene_ensembl\nGoodes thornscrub tortoise genes (rGopEvg1_v1.p)\nrGopEvg1_v1.p\n\n\n73\ngfortis_gene_ensembl\nMedium ground-finch genes (GeoFor_1.0)\nGeoFor_1.0\n\n\n74\nggallus_gene_ensembl\nChicken genes (bGalGal1.mat.broiler.GRCg7b)\nbGalGal1.mat.broiler.GRCg7b\n\n\n75\nggorilla_gene_ensembl\nGorilla genes (gorGor4)\ngorGor4\n\n\n76\ngmorhua_gene_ensembl\nAtlantic cod genes (gadMor3.0)\ngadMor3.0\n\n\n77\nhburtoni_gene_ensembl\nBurton's mouthbrooder genes (AstBur1.0)\nAstBur1.0\n\n\n78\nhcomes_gene_ensembl\nTiger tail seahorse genes (H_comes_QL1_v1)\nH_comes_QL1_v1\n\n\n79\nhgfemale_gene_ensembl\nNaked mole-rat female genes (HetGla_female_1.0)\nHetGla_female_1.0\n\n\n80\nhhucho_gene_ensembl\nHuchen genes (ASM331708v1)\nASM331708v1\nBut this does not contain the option to choose anything other than bGalGal1.mat.broiler.GRCg7b, the paternal White leghorn layer chicken.\nOnline, we can find an archived version of biomaRt that grants access:\nAnd the results line up with prior expectations.\nSo if we use the biomaRt R interface with an archived host, maybe we can get the Red Junglefowl’s genes?\nmart.archive &lt;- useMart(\"ensembl\", host=\"https://apr2022.archive.ensembl.org\")\nall.datasets &lt;- listDatasets(mart = mart.archive)\nall.datasets[70:80,]\n\n\nA data.frame: 11 × 3\n\n\n\ndataset\ndescription\nversion\n\n\n\n&lt;I&lt;chr&gt;&gt;\n&lt;I&lt;chr&gt;&gt;\n&lt;I&lt;chr&gt;&gt;\n\n\n\n\n70\nfheteroclitus_gene_ensembl\nMummichog genes (Fundulus_heteroclitus-3.0.2)\nFundulus_heteroclitus-3.0.2\n\n\n71\ngaculeatus_gene_ensembl\nStickleback genes (BROAD S1)\nBROAD S1\n\n\n72\ngevgoodei_gene_ensembl\nGoodes thornscrub tortoise genes (rGopEvg1_v1.p)\nrGopEvg1_v1.p\n\n\n73\ngfortis_gene_ensembl\nMedium ground-finch genes (GeoFor_1.0)\nGeoFor_1.0\n\n\n74\nggallus_gene_ensembl\nChicken genes (GRCg6a)\nGRCg6a\n\n\n75\nggorilla_gene_ensembl\nGorilla genes (gorGor4)\ngorGor4\n\n\n76\ngmorhua_gene_ensembl\nAtlantic cod genes (gadMor3.0)\ngadMor3.0\n\n\n77\nhburtoni_gene_ensembl\nBurton's mouthbrooder genes (AstBur1.0)\nAstBur1.0\n\n\n78\nhcomes_gene_ensembl\nTiger tail seahorse genes (H_comes_QL1_v1)\nH_comes_QL1_v1\n\n\n79\nhgfemale_gene_ensembl\nNaked mole-rat female genes (HetGla_female_1.0)\nHetGla_female_1.0\n\n\n80\nhhucho_gene_ensembl\nHuchen genes (ASM331708v1)\nASM331708v1\nVictory! GRCg6a is the Red Junglefowl.\nmart.archive &lt;- useDataset(\n    \"ggallus_gene_ensembl\",\n    useMart(\n        \"ensembl\",\n        host=\"https://apr2022.archive.ensembl.org\"\n    )\n)\ngene.to.symbol.map &lt;- getBM(\n    filters=\"ensembl_gene_id\",\n    attributes=c(\n        \"ensembl_gene_id\",\n        \"hgnc_symbol\"\n    ),\n    values=rownames(sce),\n    mart=mart.archive\n)\nhead(gene.to.symbol.map)\n\n\nA data.frame: 6 × 2\n\n\n\nensembl_gene_id\nhgnc_symbol\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nENSGALG00000000003\nPANX2\n\n\n2\nENSGALG00000000011\nC10orf88\n\n\n3\nENSGALG00000000038\n\n\n\n4\nENSGALG00000000044\nWFIKKN1\n\n\n5\nENSGALG00000000055\n\n\n\n6\nENSGALG00000000059\nDouble victory!\ngene.to.symbol.map[\n    apply(\n        gene.to.symbol.map[\"hgnc_symbol\"],\n        1,\n        function(x) grepl(\"^MT-\", x)\n    ),\n]\n\n\nA data.frame: 2 × 2\n\n\n\nensembl_gene_id\nhgnc_symbol\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n9151\nENSGALG00000032142\nMT-CO1\n\n\n11684\nENSGALG00000043768\nMT-ND2\nAll that for just two genes. Oh well. We’ll analyze this dataset in a future blog post.\nAs a learning experience, I would recommend always specifying a specific Ensembl version when querying biomaRt, since evidently code that used to work will break if they change the default breed.\nAlso, it’s a bit frustrating that I can’t specify the breed in biomaRt, because Ensembl still has the data for the other breed - it’s just for some reason not accessible via biomaRt which only presents a single breed to query."
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells.html#but-why",
    "href": "000_Blog/004_scRNA/chicken-cells.html#but-why",
    "title": "A tale of two chickens",
    "section": "But why?",
    "text": "But why?\nI’m left with one question: why are there two chickens? Why is the junglefowl no longer on modern BioMart? What is their relation? (Ahhhhhhhhhhhh)\nI did manage to find a summary of some chicken breeds, but it did not resolve my question.\nTo investigate further, I went through the history of the Ensembl versions:\n\nVersion 108 (current, October 2022): Default chicken is bGalGal1.mat.broiler.GRCg7b\n\nBut when we look at the chicken breeds page they list it as bGalGal1.pat.whiteleghornlayer.GRCg7w\n\nVersion 107 (July 2022): Default chicken is bGalGal1.mat.broiler.GRCg7b\n\nAlthough in this version they call it the “maternal broiler”\nAnd when we look at the chicken breeds page they list it as bGalGal1.pat.whiteleghornlayer.GRCg7w\n\nVersion 106 (April 2022): Default chicken is GRCg6a\n\nAnd there is no chicken breeds page\n\n\nIt seems like the chicken breeds pages don’t list the default breed on it, and that all of the genes for GRCg7w are listed with IDs beginning with 00015 (such as here). This is in contrast with the 00010 beginning for GRCg7b and GRCg6a.\nThus, we can say this:\n\nThere are three chickens in play1, GRCg6a (“red junglefowl”), GRCg7w (“white leghorn”), and GRCg7b (“maternal broiler”).\nGRCg7b (“maternal broiler”) is the current default\nGRCg6a (“red junglefowl”) used to be the default back in version 106\n\nLooking up “maternal broiler”, I am able to find this page talking about the bGalGal1.mat.broiler.GRCg7b genome. It lists its breed as specifically being:\n\nBreed: Cross of Broiler mother + white leghorn layer father\n\nThis only deepens the mystery. Why did we go from Red Junglefowl to Maternal Broiler? How does the White Leghorn come into the picture, and is it related to why GRCg7w is a crossbreed?\n\nThe chicken has been the most studied bird. It serves as a model organism for basic and biomedical science, and is one of the most commonly consumed food sources for humans.\nThis trio of samples was collected to create a high-quality reference genome for two haplotypes.\nThe female offspring (bGalGal1) is a cross between a modern broiler breeder mother (bGalGal2) and white Leghorn father (bGalGal3).\nBreeding and sample collection was conducted by Nick Anthony at the University of Arkansas, in coordination with Wesley Warren, Ron Okimoto, Hans Cheng, and Rachel Hawken.\nBlood from these samples were used to create high-quality reference genome assemblies of each haplotype, coordinated by Erich Jarvis, Wesley Warren, and Olivier Fedrigo, at the Rockefeller University Vertebrate Genome Lab, as part of the Vertebrate Genomes Project (VGP) and the Genome Reference Consortium (GRC).\n– Webpage for GRCg7b\n\nThe webpage identifies GRCg7b as coming from bGalGal3 and GRCg7w as coming from bGalGal2.\nIt seems bGalGal1 does not have its own assembly, as the list of chicken assemblies does not include it. The most information I can get as to bGalGal1 is on its sample page, which is significantly less detailed than its parent’s. As an aside, it seems like a lot of new chicken assemblies were made in 2022, which may be interesting for the future.\nAccording to the page for bGalGal3, the GRCg7w assembly has been updated to GRCg7w_WZ. I believe this did not require getting more real-world data, but rather was a re-sequencing (re-assembling) of the collected data."
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells.html#tracking-the-history-of-the-events",
    "href": "000_Blog/004_scRNA/chicken-cells.html#tracking-the-history-of-the-events",
    "title": "A tale of two chickens",
    "section": "Tracking the history of the events",
    "text": "Tracking the history of the events\n\n\n\nThis shows the history of the assembly chosen for the chicken, procured from here\n\n\nOn the Ensembl blog, we can get a changelog of what has happened:\n\n\nNew Assemblies and/or Annotation\n\n\nVertebrates\n\n\nReannotation of the reference assembly for pig (Sscrofa11.1)\nReannotation of chicken assembly (GRCg6a)\nAnnotation of 2 new GRC chicken assemblies (GRCg7w and GRCg7b)\nChicken reference will be changed from GRCg6a to GRCg7b\nAssembly and Gene set will be updated for Tropical clawed frog (Xenopus tropicalis): Xenopus tropicalis v9.1 to UCB Xtro 10.0\n\n– Release notes for Ensembl 107\n\nAll in all, this has been a really confusing adventure, but I think I learned something…"
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells.html#footnotes",
    "href": "000_Blog/004_scRNA/chicken-cells.html#footnotes",
    "title": "A tale of two chickens",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDespite my earlier belief in this being a two-chicken conundrum!↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells-3.html",
    "href": "000_Blog/004_scRNA/chicken-cells-3.html",
    "title": "3 Chicken 3 Furious",
    "section": "",
    "text": "Today we’ll once again continue where we left off. Some general goals for this project:\n\nFollow the Bioconductor OSCA Basics Book\nMaybe re-assemble from the FASTQ files using the new chicken assembly in Ensembl\n\nI’d like to go through today and practice Feature Selection and Dimensionality Reduction.\n\n\nSet up utility functions\n\nIn the future I’ll split these off into a utility file.\n\nset.seed(0)\nlibrary(SingleCellExperiment)\nlibrary(scran)\nlibrary(scater)\nlibrary(dplyr)\nlibrary(biomaRt)\nlibrary(sparseMatrixStats)\n\n\nload.scea.data &lt;- function(dir.path, exp.name) {\n    # Load in the matrix of counts\n    counts.mat &lt;- Matrix::readMM(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx'\n        )\n    ) |&gt; as(Class=\"CsparseMatrix\")\n    \n    # Get the gene names for the matrix\n    counts.rows &lt;- read.csv(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx_rows'\n        ),\n        sep='\\t',\n        header=FALSE\n    )$V1\n\n    # Get the cell ids for the matrix\n    counts.cols &lt;- read.csv(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx_cols'\n        ),\n        sep='\\t',\n        header=FALSE\n    )$V1\n    \n    # Get all cellwise metadata\n    meta.data &lt;- read.csv(\n        paste0(\n            dir.path,\n            'ExpDesign-', exp.name, '.tsv'\n        ),\n        sep='\\t',\n        header=TRUE,\n        row.names='Assay'\n    )\n    \n    rownames(counts.mat) &lt;- counts.rows\n    colnames(counts.mat) &lt;- counts.cols\n    \n    # Ensure that the meta.data lines up with the cells\n    if (!identical(rownames(meta.data), colnames(counts.mat))) {\n        warning(\"The metadata does not line up with the cells\")\n    }\n    \n    return(\n        SingleCellExperiment(\n            assays=list(counts=counts.mat),\n            colData=meta.data\n        )\n    )\n}\n\nadd.hgnc.names &lt;- function(sce, mart) {\n    rowData(sce) &lt;- getBM(\n        filters=\"ensembl_gene_id\",\n        attributes=c(\n            \"ensembl_gene_id\",\n            \"hgnc_symbol\"\n        ),\n        values=rownames(sce),\n        mart=mart\n    )\n    return(sce)\n}\n\nadd.mito.info &lt;- function(sce) {\n    rowSubset(sce, field=\"is.mito\") &lt;- grep(\"MT-\", rowData(sce)$hgnc_symbol)\n    return(sce)\n}\n\n\nmart &lt;- useDataset(\n    \"ggallus_gene_ensembl\",\n    useMart(\n        \"ensembl\",\n        host=\"https://apr2022.archive.ensembl.org\"\n    )\n)\n\nsce &lt;- load.scea.data(\"./localdata/Datasets/E-CURD-13/\", \"E-CURD-13\")\n\n\nAfter setting up our utility functions, it’s pretty easy to catch up to where we were yesterday:\n\nmart &lt;- useDataset(\n    \"ggallus_gene_ensembl\",\n    useMart(\n        \"ensembl\",\n        host=\"https://apr2022.archive.ensembl.org\"\n    )\n)\nsce &lt;- load.scea.data(\"./localdata/Datasets/E-CURD-13/\", \"E-CURD-13\") |&gt;\n    add.hgnc.names(mart=mart) |&gt;\n    add.mito.info() |&gt;\n    quickPerCellQC(subsets=list(Mito=is.mito), sub.fields=\"subsets_Mito_percent\") |&gt;\n    logNormCounts()\n\n\nsce\n\nclass: SingleCellExperiment \ndim: 13645 7430 \nmetadata(0):\nassays(2): counts logcounts\nrownames(13645): ENSGALG00000000003 ENSGALG00000000011 ...\n  ENSGALG00000055127 ENSGALG00000055132\nrowData names(3): ensembl_gene_id hgnc_symbol is.mito\ncolnames(7430): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(29): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ... discard sizeFactor\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nWe’re now going to find information on the variance of the genes, for feature selection. The OSCA book makes the following plot:\n\ngene.var.data &lt;- modelGeneVar(sce)\nmd &lt;- metadata(gene.var.data)\nmd.as.df &lt;- data.frame(mean=md$mean, var=md$var)\n\n\nggplot(md.as.df, aes(mean, var)) +\n    geom_point(shape=2) +\n    geom_line(\n        aes(mean, md$trend(mean), lty='Variance Trend'),\n        color=\"blue\",\n        linewidth=2,\n    ) +\n    labs(\n        x=\"Mean of Log-Counts\",\n        y=\"Variance of Log-Counts\"\n    ) +\n    ggtitle(\"Mean-Variance Plot of Log-Counts\") +\n    scale_linetype(\"\") # This changes the legend title for some reason\n\n\n\n\nThis plot is showing the mean-variance trend of the logs. The trend is assumed to be the ‘uninteresting variation’, i.e. the baseline variation of the experiment (assumed to be the technical variation and the ‘boring’ biological variation). Under this assumption, the deviation from this trend represents the interesting biological variation.\nBut speaking of mean-variance trends, this reminds me of the fact that Poisson distributions have equal mean and variance; it’s reasonable to believe that our original (non-log) counts follow a Poisson distribution: is this correct?\n\nmeans &lt;- rowMeans(assay(sce, \"counts\"))\nvars &lt;- rowVars(assay(sce, \"counts\"))\n\nggplot(\n    data.frame(\n        means=means,\n        vars=vars\n    ),\n    aes(\n        x=means,\n        y=vars\n    )\n) +\n    geom_point() +\n    geom_abline(slope=1, color='blue')\n\n\n\n\nInterestingly, this is not accurate at all! The variance is way higher than the means. This could be because the counts have already been scaled by a value. I’ll have a think about this for the future. I did discover the modelGeneVarByPoisson function, so let’s see what that does:\n\ngene.var.data &lt;- modelGeneVarByPoisson(sce)\nmd &lt;- metadata(gene.var.data)\nmd.as.df &lt;- data.frame(mean=md$mean, var=md$var)\n\n\nggplot(md.as.df, aes(mean, var)) +\n    geom_point(shape=2) +\n    geom_line(\n        aes(mean, md$trend(mean), lty='Variance Trend'),\n        color=\"blue\",\n        linewidth=1,\n    ) +\n    labs(\n        x=\"Mean of Log-Counts\",\n        y=\"Variance of Log-Counts\"\n    ) +\n    ggtitle(\"Mean-Variance Plot of Log-Counts Poissonly\") +\n    scale_linetype(\"\") # This changes the legend title for some reason\n\n\n\n\nInterestingly enough, this looks very accurate! So it’s plausible that I messed up somewhere in my prior plot. Of course, I don’t really understand what this plot represents at the moment - but it’s clear the predicted trend deviates somewhat from the true trend at the maximum (it oversetimates it).\nFor completeness, let’s peek at the most varying genes!\n\ngenes.by.variance &lt;- gene.var.data[order(gene.var.data$bio, decreasing=TRUE),]\nhead(genes.by.variance)\n\nDataFrame with 6 rows and 6 columns\n                        mean     total      tech       bio      p.value\n                   &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;\nENSGALG00000037167  1.554552   2.50414  0.848294  1.655847 3.16059e-154\nENSGALG00000035998  3.027209   2.17292  0.525832  1.647090  0.00000e+00\nENSGALG00000046616  0.717956   2.08196  0.619730  1.462226 3.02326e-224\nENSGALG00000010745  1.929346   2.16633  0.816513  1.349819 2.86377e-111\nENSGALG00000052192  2.813587   1.65282  0.577865  1.074959 2.77975e-140\nENSGALG00000015970  0.590967   1.49926  0.542383  0.956873 2.09606e-126\n                            FDR\n                      &lt;numeric&gt;\nENSGALG00000037167 8.62525e-151\nENSGALG00000035998  0.00000e+00\nENSGALG00000046616 1.03131e-220\nENSGALG00000010745 2.60508e-108\nENSGALG00000052192 5.41853e-137\nENSGALG00000015970 3.17786e-123\n\n\n\nrowData(sce[rownames(sce) == \"ENSGALG00000037167\"])$hgnc_symbol\nrowData(sce[rownames(sce) == \"ENSGALG00000035998\"])$hgnc_symbol\nrowData(sce[rownames(sce) == \"ENSGALG00000046616\"])$hgnc_symbol\nrowData(sce[rownames(sce) == \"ENSGALG00000010745\"])$hgnc_symbol\nrowData(sce[rownames(sce) == \"ENSGALG00000052192\"])$hgnc_symbol\nrowData(sce[rownames(sce) == \"ENSGALG00000015970\"])$hgnc_symbol\n\n''\n\n\n''\n\n\n'CHGB'\n\n\n''\n\n\n'TUBA1B'\n\n\n''\n\n\nMost of these aren’t named, annoyingly - which made me realize that the H in “HGNC” stands for “human” 😅\nIf we use other sources, such as bgee, we can get a bit more info.\n\nBgee is a database for retrieval and comparison of gene expression patterns across multiple animal species. It provides an intuitive answer to the question “where is a gene expressed?” and supports research in cancer and agriculture as well as evolutionary biology.\nbgee ‘About’ page\n\n\nENSGALG00000037167\n\nNo hgnc identifier\n“HIST1H103” https://bgee.org/gene/ENSGALG00000037167/\n\nENSGALG00000035998\n\nNo hgnc identifier\n“HINTW” https://bgee.org/gene/ENSGALG00000035998/\n\nENSGALG00000046616\n\n“CHGB” hgnc\n“CHGB” https://bgee.org/gene/ENSGALG00000046616/\n\nENSGALG00000010745\n\nNo hgnc identifier\n“HMGB2” https://bgee.org/gene/ENSGALG00000010745/\n\nENSGALG00000052192\n\n“TUBA1B” hgnc\n“TUBA1B” https://bgee.org/gene/ENSGALG00000052192/\n\nENSGALG00000015970\n\nNo hgnc identifier\n“COL9A1” https://bgee.org/gene/ENSGALG00000015970/\n\n\nThis whole thing about chickens “not being” humans is annoying, because it throws into question my mitochondrial analysis as well. What if the reason only two mitochondrial genes is because the whole check-hgnc-name-trick failed due to lack of a valid hgnc name?\nIf we get the chromosome_name tag from biomart, this would be a more direct way to get the mitochondrial genes!\n\ngene.chromosome.map &lt;- getBM(\n    filters=\"ensembl_gene_id\",\n    attributes=c(\n        \"ensembl_gene_id\",\n        \"chromosome_name\"\n    ),\n    values=rownames(sce),\n    mart=mart\n)\n\n\ngene.chromosome.map[gene.chromosome.map[\"chromosome_name\"] == \"MT\",]\n\n\nA data.frame: 13 × 2\n\n\n\nensembl_gene_id\nchromosome_name\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n8565\nENSGALG00000029500\nMT\n\n\n8768\nENSGALG00000030436\nMT\n\n\n9146\nENSGALG00000032079\nMT\n\n\n9151\nENSGALG00000032142\nMT\n\n\n9218\nENSGALG00000032456\nMT\n\n\n9220\nENSGALG00000032465\nMT\n\n\n9858\nENSGALG00000035334\nMT\n\n\n10038\nENSGALG00000036229\nMT\n\n\n10372\nENSGALG00000037838\nMT\n\n\n11108\nENSGALG00000041091\nMT\n\n\n11425\nENSGALG00000042478\nMT\n\n\n11478\nENSGALG00000042750\nMT\n\n\n11684\nENSGALG00000043768\nMT\n\n\n\n\n\nAnd we can see that there are 13 mitochondrial genes in our dataset, much more than the two we originally found!\nWe’ll continue with our analysis for now, but next time we return to this dataset we’ll redo the QC steps. It’s plausible that we’ll get wildly different results, which would prove the importance of QC.\n\nsce &lt;- sce[getTopHVGs(gene.var.data, prop=0.1),]\nsce\n\nclass: SingleCellExperiment \ndim: 1160 7430 \nmetadata(0):\nassays(2): counts logcounts\nrownames(1160): ENSGALG00000035998 ENSGALG00000037167 ...\n  ENSGALG00000011330 ENSGALG00000026203\nrowData names(3): ensembl_gene_id hgnc_symbol is.mito\ncolnames(7430): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(29): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ... discard sizeFactor\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\n\nsce &lt;- sce |&gt;\n    runPCA(ncomponents=14) |&gt;\n    runUMAP(ncomponents=2, dimred='PCA') |&gt;\n    runTSNE(ncomponents=2, dimred='PCA') |&gt;\n    runPCA(ncomponents=2, name='PCA.tiny')\nsce\n\nclass: SingleCellExperiment \ndim: 1160 7430 \nmetadata(0):\nassays(2): counts logcounts\nrownames(1160): ENSGALG00000035998 ENSGALG00000037167 ...\n  ENSGALG00000011330 ENSGALG00000026203\nrowData names(3): ensembl_gene_id hgnc_symbol is.mito\ncolnames(7430): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(29): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ... discard sizeFactor\nreducedDimNames(4): PCA UMAP TSNE PCA.tiny\nmainExpName: NULL\naltExpNames(0):\n\n\n\nplotReducedDim(sce, dimred='UMAP') + ggtitle(\"UMAP\")\nplotReducedDim(sce, dimred='TSNE') + ggtitle(\"TSNE\")\nplotReducedDim(sce, dimred='PCA.tiny') + ggtitle(\"PCA\")\n\n\n\n\n\n\n\n\n\n\nNone of these look great, but they do seem comparable to the UMAP reported by the Single Cell Expression Atlas:\n\nWe can cheat by coloring in the cells by the predicted cell types, to get a thumbnail for this post (ideally we’d like to learn how to predict these ourselves).\n\nplotReducedDim(\n    sce,\n    dimred='UMAP',\n    color_by=\"Factor.Value.inferred.cell.type...ontology.labels.\"\n) + ggtitle(\"UMAP\")"
  },
  {
    "objectID": "000_Blog/006_collapsible_headers/collapsable_headers.html",
    "href": "000_Blog/006_collapsible_headers/collapsable_headers.html",
    "title": "Collapsable Multi-Cell Blocks in Quarto Output of .ipynb",
    "section": "",
    "text": "In JupyterLab, we can collapse all cells underneath a header, which can be useful for hiding extraneous info. However, when we render it in Quarto the collapsability is not preserved. It would be nice if we could recreate this feature!\nQuarto does offer some methods for collapsing:\n\n\n The first is &lt;details&gt; html blocks. \n\nThese are my preferred method because it is pure html and thus highly customizable.\n\n\n\n\n\n\n\nThe second is a Quarto-specific collapsable callout block\n\n\n\n\n\nWhich I don’t use because it’s not pure html, and thus doesn’t render in the notebook - it only appears after rendering with Quarto.\n\n\n\nThe quarto method has the benefit that it can be split over multiple cells, allowing the collapsing of code cells:\n\n\n\n\n\n\nYou can see that it still works!\n\n\n\n\n\n\nprint(\"It worked!\")\n\n[1] \"It worked!\"\n\n\n\n\n\n\n\nMessing around with Quarto\n\nThere’s a major problem, though; the Quarto method looks a little too professional… I want the callout blocks to have the same appearance as a &lt;details&gt; tag!\nThe first step we can do is add a class to our callout block:\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nTest\n\n\nThe markdown to generate is:\n::: {.callout-tip collapse=\"true\" #custom-callout}\n  &lt;h2&gt;Test&lt;/h2&gt;\n  &lt;! -- ETC... --&gt;\n:::\n\n\n\nTo make that work, I used a fairly advanced bit of CSS:\nFirst, create a file blocks.css, into which you’ll put the style, and then add this to the Quarto header:\nformat:\n  html:\n    css: blocks.css\nFill the css file with the following:\n.callout-tip #custom-callout :is(h1, h2, h3, h4, h5, h6) {\n  color:#C0CF96\n}\nThis says “for all objects with the callout-tip class and the custom-callout id, check if any of their children are h1 or h2 or (…) or h6. If so, give them this specific color.”\nIt’s not perfect, though - as the header (“Test”) no longer shows up on the title but rather only appears when you open the drop-down.\n\n\n\n\n\n\nTest\n\n\n\n\n\nThe markdown to generate is:\n::: {.callout-tip collapse=\"true\" #custom-callout2}\n  ## Test\n  &lt;! -- ETC... --&gt;\n:::\n\n\n\nWe can start poking around with inspect element to get the classes we need to mess with in the CSS:\n#custom-callout2 div.callout-caption-container.flex-fill {\n  color:#C0CF96;\n  font-weight:bold;\n}\n#custom-callout2 div.callout-icon-container {\n  opacity:0%;\n}\nAll that’s left is to remove the pesky background:\n\n\n\n\n\n\nTest\n\n\n\n\n\nThe markdown to generate is:\n::: {.callout-tip collapse=\"true\" #clean-collapse}\n  ## ▶ Test\n  &lt;! -- ETC... --&gt;\n:::\n\nprint(\"Running code just to demonstrate it still works\")\n\n[1] \"Running code just to demonstrate it still works\"\n\n\n\n\n\nI added the following CSS:\n#clean-collapse div.callout-caption-container.flex-fill {\n  color:#C0CF96;\n  font-weight:bold;\n}\n\n#clean-collapse div.callout-icon-container {\n  opacity:0%;\n}\nAnd to get rid of the background I had to remove the callout-style-default class using JavaScript:\n&lt;script&gt;\n  element = document.getElementById(\"clean-collapse\");\n  element.classList.remove(\"callout-style-default\");\n&lt;/script&gt;\nWhich can just be put anywhere on the page - probably it will make sense to make a .js file that will contain all of these scripts, such as this and the comments scripts, and paste them on each page.\nHowever, the fatal flaw with this method - that can only ever edit one dropdown at once, and it requires you to manually add arrows. To fix this, we can do more sophistic JavaScript:\n&lt;script&gt;\n    // Remove background\n    elements = document.querySelectorAll('[id=clean-collapse]').forEach(\n        function(element) {\n            element.classList.remove(\"callout-style-default\");\n        }\n    )\n    \n    // Add arrow to front to signify dropdownyness\n    arrow = \"▶\";\n    elements = document.querySelectorAll('.callout-caption-container').forEach(\n        function(element) {\n            element.innerHTML = arrow + element.innerHTML;\n        }\n    )\n&lt;/script&gt;\n\n\n\n\n\n\nConclusion\n\n\n\n\n\nI took this opportunity to refactor the website, so that I don’t need to put comments, css, and javascript at the end of every webpage, rather I can do it by including premade files, like so:\n---\ntitle: \"Collapsable Multi-Cell Blocks in Quarto Output of .ipynb\"\ndescription: \"Surprisingly easy!\"\nauthor: \"Bailey Andrew\"\ndate: \"Jan 9 2023\"\ndraft: false\ncategories: [Work, Useful]\nformat:\n  html:\n    css:\n      - ../../html_scripts/collapse.css\n      - ../../html_scripts/pretty_shortcuts.css\n    include-after-body: \n      - ../../html_scripts/collapse.html\n      - ../../html_scripts/comments.html\n---\n\n\n\n\nBut in fact you don’t need to mess around with the Quarto way, because the &lt;details&gt; tag works multi-line as well - that’s how I hid all the “playing around with quarto” stuff."
  },
  {
    "objectID": "000_Blog/009_optimization/convex_optimization.html",
    "href": "000_Blog/009_optimization/convex_optimization.html",
    "title": "Convex Optimization Taster",
    "section": "",
    "text": "If you like optimization, chances are you’ve heard of convex optimization, the study of optimization of convex functions. In this post, we’ll look at what it means to be convex, and why we should care.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig, ax = plt.subplots(figsize=(6, 6))\nax.plot(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100)**2)\nax.set_title(\"$x^2$ - a convex function\")\npass\nA convex function is a function (of potentially multiple variables) with the following property: any line drawn between two points on the function will never pass below the function. Note that this forces the function to have a single local mimimum1 - think about it!2. If you consider the shape formed by all the points above the function, you will have a convex shape - hence the name."
  },
  {
    "objectID": "000_Blog/009_optimization/convex_optimization.html#footnotes",
    "href": "000_Blog/009_optimization/convex_optimization.html#footnotes",
    "title": "Convex Optimization Taster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, a flat line is convex but it has infinitely many minima - so we should in theory be more precise about our wording. Note, however that all minima are just as good as any other.↩︎\nAnother aside; the convex function \\(e^x\\) has no minima, unless we count the minimum at \\(-\\infty\\). The real benefit is that we no longer have to worry about getting stuck in suboptimal local mimima.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hi, I’m Bailey. This is my website. It’s not really meant to be read - the point, rather, is to have a website that could theoretically be read. This should (hopefully) guide me towards being a better communicator, and make me more motivated to communicate in general.\nThis is especially true for the blog posts, whose quality, quite frankly, is questionable. However there are other aspects of this site that I am proud of, such as the puzzles - give them a try! They’re handmade and should always be able to be solved logically :)\n\n\n\n\n\n\n\n\n\n\nBlog\n\n\n\nBailey Andrew\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoulders in Valleys\n\n\n\nBailey Andrew\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther People’s Blogs\n\n\n\nBailey Andrew\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlideshows\n\n\n\nBailey Andrew\n\n\nJan 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatic Posts\n\n\n\nBailey Andrew\n\n\nJan 21, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "000_Blog/005_BnV/bouldersinvalleystips.html",
    "href": "000_Blog/005_BnV/bouldersinvalleystips.html",
    "title": "Boulders in Valleys Tips and Tricks",
    "section": "",
    "text": "Boulders in Valleys is a puzzle I enjoy creating. It is also a puzzle I enjoy solving. This post is all about solving techniques.\n\n\n\n\n\nSimple rules to get a beginner started.\n\n\nThese are four basic techniques to get started. Don’t be intimidated by the variable names, if you see them in practice it’ll make sense.\nThe first pattern should be straightforward; a zero can’t roll anywhere, so it has to sit perched exactly where it is. The xs are used to support the boulder to prevent it rolling off a corner.\nThe second pattern requires a bit more thinking, but if \\(m\\neq n+1\\) then, if it could fall straight down, it would be forced to follow the exact trajectory of \\(n\\), just from 1 higher - i.e. its deepest depth would be \\(n+1\\), a contradiction.\nFor the third pattern, we know that \\(q\\) needs to roll but it can’t roll left or straight down; hence it needs to roll right. We can mark the place it has rolled to with \\(r-1\\), which can prove useful when the puzzle is difficult.\nThe fourth pattern is similar to the second and third in reasoning; if \\(n\\) rolled left, it would be forced to follow \\(p\\)’s deepest trajectory. As \\(p &gt; n\\), this is a contradiction.\n\n\n\nTo take your solving skills to the next step, you should learn some more advanced patterns. In this section, a thick dashed line represents that we are “conditioning” on an edge. That is, we ask ourselves “what happens if this edge is filled in, and what happens if it is crossed out?”. The goal is to find a contradiction down one of the paths, proving that it must be one way or the other.\n\n\n\nThe Elbow Pattern\n\n\nWe can see that, if there is an edge to the left of the 1, the 1 would fall too far. Hence we can arrive at the “elbow” shape shown on the right. This is quite a nice pattern in that it immediately gives you a lot of information about the board; 5 xs and three edges is nothing to scoff at!\n\n\n\nThe Drop Pattern\n\n\nIf a boulder is sandwiched between two same-direction-but-deeper boulders, then we know that the boulder has to fall straight down to prevent merging its trajectory with one of the adjacent boulders.\n\n\n\nThe Corner Pattern\n\n\nThe corner pattern is harder to derive, but also very useful. We can follow a chain of logic to show that there are only two possible local solutions to the pattern. This gives us two xs and two edges, plus we know that the square marked by the diagonal dashed line either bends outwards or inwards. This is also a good pattern to have in your toolkit.\n\n\n\nThe Resting Place Pattern\n\n\nI won’t go through how to derive this pattern, as it’s a straightforward application of the resting place rule descriped in the Advanced Techniques section.\n\n\n\nBoundary Conditions\n\n\nThe next trick is not a pattern, but an observation - since the grid has to be partitioned in two, if you already touch the boundary in two places you know you cannot touch it in any other place. This can give you quite a few xs that are useful for near-boundary logics.\n\n\n\nPhantom Boulders\n\n\nThe next trick is something that was touched upon in the beginner’s section; if you know a boulder has to fall one way, it can be useful to mark that cell with the new boulder’s information. The red cell is an implied boulder of depth 1, due to the fall path of the 3. As an example of how this might be useful, the grey edges show deductions that can be made using the phantom boulder and the corner pattern.\n\n\n\nThe Staircase of Death\n\n\nIn certain situations you can create a staircase of death. If the 5↓ rolls to the right, then it will trigger an cascade that can only be stopped by intersection with the grid boundary. If it would not touch the grid boundary at the right distance, then it produces an invalid configuration and can be ruled out - this forces the 5↓ to roll left and the 5→ to fall right.\n\n\n\nAdvanced techniques can be very powerful, but require a deeper understanding of how Boulders in Valleys works. I’m sure there are some other fundamental concepts that would qualify for being listed here, but that I don’t currently know as I’m still learning myself.\n\n\n\nThe No-Traversal Rule\n\n\nThe “no-traversals” rule comes from a simple observation - no edge can cross the path of a falling boulder. If it does, then the boulder would either stop or roll to the side, thus changing its path. In the left image, we can see a possible path for the boulder to take. However, that would separate the two edges. Since the solution has to be connected, we know it cannot take this path! Thus, we can update our possible paths for the boulder (blue background) by taking this into account.\n\n\n\nAnother use of the no-traversals rule\n\n\nIn this case, we can work out that the boulder falls straight down - if there were anything to its left deflecting it to the right, it would be stuck on the wrong side of the trajectory.\n\n\n\nThe Resting Place Rule\n\n\nEvery boulder n↓ needs to have a resting place exactly \\(n\\) cells below it. Suppose the resting place were over the 0s, then we’d have a clear contradiction as it’d be resting on a corner. Hence it has to rest on the dashed line. This is the resting place rule. Note that a resting place is not just a horizontal edge, but a horizontal edge with two xs beneath it.\n\n\n\nDiagonal Propagations. Starting with a diagonal deduced (dashed line) and a well-placed 1↓, we can propagate the diagonal. The final deduction is in the bottom right.\n\n\nAs we saw from the corner rule, we can sometimes get a “diagonal” deduced, saying that “either all edges on one side of the diagonal are filled in, or all edges on the other side are filled in”. Some things can extend this chain, such as a 1↓ in the right place. Note that this extension has a minor flaw in that the diagonal is not “perfect” - one of the conditions allows a third edge on the diagonal to form a ‘u’ shape to catch the 1↓. However, if we kept on adding 1↓s to the up-right of the previous 1↓, we could continue to propagate this logic."
  },
  {
    "objectID": "000_Blog/005_BnV/bouldersinvalleystips.html#beginner-tips",
    "href": "000_Blog/005_BnV/bouldersinvalleystips.html#beginner-tips",
    "title": "Boulders in Valleys Tips and Tricks",
    "section": "",
    "text": "Simple rules to get a beginner started.\n\n\nThese are four basic techniques to get started. Don’t be intimidated by the variable names, if you see them in practice it’ll make sense.\nThe first pattern should be straightforward; a zero can’t roll anywhere, so it has to sit perched exactly where it is. The xs are used to support the boulder to prevent it rolling off a corner.\nThe second pattern requires a bit more thinking, but if \\(m\\neq n+1\\) then, if it could fall straight down, it would be forced to follow the exact trajectory of \\(n\\), just from 1 higher - i.e. its deepest depth would be \\(n+1\\), a contradiction.\nFor the third pattern, we know that \\(q\\) needs to roll but it can’t roll left or straight down; hence it needs to roll right. We can mark the place it has rolled to with \\(r-1\\), which can prove useful when the puzzle is difficult.\nThe fourth pattern is similar to the second and third in reasoning; if \\(n\\) rolled left, it would be forced to follow \\(p\\)’s deepest trajectory. As \\(p &gt; n\\), this is a contradiction."
  },
  {
    "objectID": "000_Blog/005_BnV/bouldersinvalleystips.html#intermediate-tricks",
    "href": "000_Blog/005_BnV/bouldersinvalleystips.html#intermediate-tricks",
    "title": "Boulders in Valleys Tips and Tricks",
    "section": "",
    "text": "To take your solving skills to the next step, you should learn some more advanced patterns. In this section, a thick dashed line represents that we are “conditioning” on an edge. That is, we ask ourselves “what happens if this edge is filled in, and what happens if it is crossed out?”. The goal is to find a contradiction down one of the paths, proving that it must be one way or the other.\n\n\n\nThe Elbow Pattern\n\n\nWe can see that, if there is an edge to the left of the 1, the 1 would fall too far. Hence we can arrive at the “elbow” shape shown on the right. This is quite a nice pattern in that it immediately gives you a lot of information about the board; 5 xs and three edges is nothing to scoff at!\n\n\n\nThe Drop Pattern\n\n\nIf a boulder is sandwiched between two same-direction-but-deeper boulders, then we know that the boulder has to fall straight down to prevent merging its trajectory with one of the adjacent boulders.\n\n\n\nThe Corner Pattern\n\n\nThe corner pattern is harder to derive, but also very useful. We can follow a chain of logic to show that there are only two possible local solutions to the pattern. This gives us two xs and two edges, plus we know that the square marked by the diagonal dashed line either bends outwards or inwards. This is also a good pattern to have in your toolkit.\n\n\n\nThe Resting Place Pattern\n\n\nI won’t go through how to derive this pattern, as it’s a straightforward application of the resting place rule descriped in the Advanced Techniques section.\n\n\n\nBoundary Conditions\n\n\nThe next trick is not a pattern, but an observation - since the grid has to be partitioned in two, if you already touch the boundary in two places you know you cannot touch it in any other place. This can give you quite a few xs that are useful for near-boundary logics.\n\n\n\nPhantom Boulders\n\n\nThe next trick is something that was touched upon in the beginner’s section; if you know a boulder has to fall one way, it can be useful to mark that cell with the new boulder’s information. The red cell is an implied boulder of depth 1, due to the fall path of the 3. As an example of how this might be useful, the grey edges show deductions that can be made using the phantom boulder and the corner pattern.\n\n\n\nThe Staircase of Death\n\n\nIn certain situations you can create a staircase of death. If the 5↓ rolls to the right, then it will trigger an cascade that can only be stopped by intersection with the grid boundary. If it would not touch the grid boundary at the right distance, then it produces an invalid configuration and can be ruled out - this forces the 5↓ to roll left and the 5→ to fall right."
  },
  {
    "objectID": "000_Blog/005_BnV/bouldersinvalleystips.html#advanced-techniques",
    "href": "000_Blog/005_BnV/bouldersinvalleystips.html#advanced-techniques",
    "title": "Boulders in Valleys Tips and Tricks",
    "section": "",
    "text": "Advanced techniques can be very powerful, but require a deeper understanding of how Boulders in Valleys works. I’m sure there are some other fundamental concepts that would qualify for being listed here, but that I don’t currently know as I’m still learning myself.\n\n\n\nThe No-Traversal Rule\n\n\nThe “no-traversals” rule comes from a simple observation - no edge can cross the path of a falling boulder. If it does, then the boulder would either stop or roll to the side, thus changing its path. In the left image, we can see a possible path for the boulder to take. However, that would separate the two edges. Since the solution has to be connected, we know it cannot take this path! Thus, we can update our possible paths for the boulder (blue background) by taking this into account.\n\n\n\nAnother use of the no-traversals rule\n\n\nIn this case, we can work out that the boulder falls straight down - if there were anything to its left deflecting it to the right, it would be stuck on the wrong side of the trajectory.\n\n\n\nThe Resting Place Rule\n\n\nEvery boulder n↓ needs to have a resting place exactly \\(n\\) cells below it. Suppose the resting place were over the 0s, then we’d have a clear contradiction as it’d be resting on a corner. Hence it has to rest on the dashed line. This is the resting place rule. Note that a resting place is not just a horizontal edge, but a horizontal edge with two xs beneath it.\n\n\n\nDiagonal Propagations. Starting with a diagonal deduced (dashed line) and a well-placed 1↓, we can propagate the diagonal. The final deduction is in the bottom right.\n\n\nAs we saw from the corner rule, we can sometimes get a “diagonal” deduced, saying that “either all edges on one side of the diagonal are filled in, or all edges on the other side are filled in”. Some things can extend this chain, such as a 1↓ in the right place. Note that this extension has a minor flaw in that the diagonal is not “perfect” - one of the conditions allows a third edge on the diagonal to form a ‘u’ shape to catch the 1↓. However, if we kept on adding 1↓s to the up-right of the previous 1↓, we could continue to propagate this logic."
  },
  {
    "objectID": "000_Blog/2022thudec15.html",
    "href": "000_Blog/2022thudec15.html",
    "title": "How to add comments to your blog",
    "section": "",
    "text": "How to add comments to your blog\nUse giscus. When you try to add it to your blog following the instructions in the link, you should immediately get the following error:\n\n\n\nThe Error\n\n\nThe first issue is that you probably don’t have giscus installed. Install it.\n\n\n\nInstall it.\n\n\nThe second issue is that your repo probably doesn’t have discussions enabled. Go to your repo’s settings, then scroll down a bit until you see a checkmark for discussions. Click it.\n\n\n\nThe checkmark\n\n\nAfter that, it should work:\n\n\n\nYay.\n\n\nThen go through the rest of the page and select the options you want. It should auto-generate some html for you; mine looks like this:\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"baileyandrew/blog\"\n        data-repo-id=\"R_kgDOInJwKg\"\n        data-category=\"Announcements\"\n        data-category-id=\"DIC_kwDOInJwKs4CTGOQ\"\n        data-mapping=\"title\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"dark_protanopia\"\n        data-lang=\"en\"\n        data-loading=\"lazy\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\nYou can just put it in a markdown code cell in your blog at the end, and it will appear magically when pushed!"
  },
  {
    "objectID": "000_Blog/004_scRNA/16srna.html",
    "href": "000_Blog/004_scRNA/16srna.html",
    "title": "What is 16s rRNA?",
    "section": "",
    "text": "16s rRNA sequencing is a type of sequencing that specifically targets the 16s rRNA gene. It is frequently used for metagenomics - the sequencing of genetic material directly from an environment (such as sea water or the gut microbiome via stool samples) rather than the sequencing of genes from a specific organism. rRNA stands for “ribosomal RNA”.\nRibosomes are the bits inside the cell that convert mRNA into proteins. It is made up of rRNA and (chicken-and-egg-ily) proteins. Names have been given to the different bits of rRNA in a ribosome. 16s rRNA is the name for one of the parts, specifically in prokaryotic species.\n16s rRNA sequencing targets the gene that codes for 16s rRNA, because it tends to evolve slowly - thus it can act as a fingerprint for different species, allowing us to get an overview of the prokaryotic species in a sample.\n\nHow is 16s rRNA sequencing done?\n\nWe use the high-fidelity Phusion polymerase for amplification of 16s and ITS2 marker genes. If extractions contain some carry-over inhibition or a high concentration of DNA, typically we test 1:1 and 1:10 dilutions and run the PCR products on gels for verification. PCR is done with dual-barcoded primers (Kozich et al. 2014) targeting either the prokaryotic (16S V4, V1-V3) or fungal (ITS2) regions. Our barcoding strategy enables multiplexing up to 384 samples per run. PCR products are verified visually by running a representative subset of samples on a gel. Samples with failed PCRs (or spurious bands) are re-amplified by optimizing PCR conditions. The PCR reactions are cleaned up and normalized using the high-throughput SequalPrep 96-well Plate Kit. Samples are then pooled to make one library that is quantified accurately with the KAPA qPCR Library Quant kit.\n– 16S rRNA Sequencing Service page; Library Preparation Section, Microbiome Insights\n\nA polymerase is a molecule that can create new strands of DNA/RNA (so its no wonder that it is used for amplification). Notably, polymerase is the “P” in PCR (Polymerase Chain Reaction). Primers are used to encourage amplification of only the 16s rRNA gene (we can ignore the bit about the ITS2 gene as they are talking about fungi there).\n\nThe 16S rRNA gene is approximately 1600 base pairs long and includes nine hypervariable regions of varying conservation (V1-V9). More conservative regions are useful for determining the higher-ranking taxa, whereas more quickly evolving ones can help identify genus or species.\nBukin et al. (2019)\n\n\n\n\n\n\nReferences\n\nBukin, Yu. S., Yu. P. Galachyants, I. V. Morozov, S. V. Bukin, A. S. Zakharenko, and T. I. Zemskaya. 2019. “The Effect of 16S rRNA Region Choice on Bacterial Community Metabarcoding Results.” https://doi.org/https://doi.org/10.1038/sdata.2019.7."
  },
  {
    "objectID": "000_Blog/004_scRNA/ziln2.html",
    "href": "000_Blog/004_scRNA/ziln2.html",
    "title": "Paper Replication on Sparse Microbial Networks – Measuring the Network",
    "section": "",
    "text": "This post follows on from the previous one. In the last post, we recreated a graph (a network of microbial associations) in the paper with moderate success. In the paper (Vincent Prost (2021)), they then add a numerical measure to this network to understand the extent to which it is feasible network. Specifically, they measure the assortativity of the graph. Assortativity is a measure of how much vertices of the same group tend to be connected to eachother - in this case, the paper groups vertices by taxonomy1.\nThey computed assortativity at three different taxonomic levels, with the values being given in Table S1:\nThe network is stored as an igraph object adjacency.graph, calculated in the code hidden by the dropdown.\nadjacency.graph\n\nIGRAPH 36c88f2 U--- 565 1200 -- \n+ edges from 36c88f2:\n [1]  1--  2  1-- 76  2-- 77  2--195  3--324  4--528  5--221  5--479  5--480\n[10]  5--526  5--534  5--542  5--553  5--563  7--244  7--261  7--299  7--479\n[19]  7--485  7--526  8--  9  8-- 66  8-- 69  8-- 85  8--153  8--234  8--246\n[28]  9-- 66  9-- 67  9-- 68  9-- 69  9--128  9--130  9--211 10-- 67 10--128\n[37] 11-- 82 11--120 11--243 11--290 11--336 11--363 11--435 11--437 11--456\n[46] 11--457 12--542 13-- 53 13-- 76 13--192 13--272 13--326 14--106 14--244\n[55] 14--254 14--342 14--554 15-- 34 15-- 46 15--243 15--337 16-- 17 16--316\n[64] 16--451 16--458 16--510 16--519 17-- 64 17--316 17--343 17--510 17--525\n[73] 19-- 20 19-- 21 19-- 25 19-- 30 19-- 32 19-- 34 19-- 35 19-- 54 19-- 81\n+ ... omitted several edges\nEvaluating the assortativity of a graph is straightforward with the igraph package:\nget.assortativity.at.level &lt;- function(adjacency.graph, taxmat, taxa.level) {\n    groups &lt;- as.integer(as.factor(taxmat[, taxa.level]))\n    return(assortativity(adjacency.graph, groups))\n}\n\nget.assortativity.at.levels &lt;- function(adjacency.graph, taxmat) {\n    curried.assortativity &lt;- function(taxa.level) get.assortativity.at.level(\n        adjacency.graph,\n        taxmat,\n        taxa.level\n    )\n    return(\n        lapply(\n            colnames(taxmat),\n            curried.assortativity\n        )[2:length(colnames(taxmat))]\n    )        \n}\n\nprint.assortativity.performances &lt;- function(adjacency.graph, taxmat) {\n    for (taxa.level in colnames(taxmat)) {\n        if (taxa.level == \"Domain\") {\n            # Skip b/c everything in dataset has domain being bacteria\n            # which will cause this to evaluate as NaN.\n            next()\n        }\n        print(\n            paste(\n                taxa.level,\n                \": \",\n                get.assortativity.at.level(adjacency.graph, taxmat_el, taxa.level)\n            )\n        )\n    }\n}\nprint.assortativity.performances(adjacency.graph, taxmat_el)\n\n[1] \"Phylum :  0.119028213625117\"\n[1] \"Class :  0.114678278712612\"\n[1] \"Order :  0.00101704475751504\"\n[1] \"Family :  0.0227129118376968\"\n[1] \"Genus :  0.0295176588068461\"\nIt seems that the assortativity for order is much lower than the rest, although the lower-taxa levels are all much lower than the high-taxa levels. This would be expected as it is easier to accurately group things in a few larger groups than many smaller groups.\nUnfortunately for us, this is very far away from the expected results. We know from the last post that we had a lot more isolated vertices than in the paper, so maybe we should look at assortativity as a function of the regularization parameter.\nlambdas &lt;- 10^seq(0, -2, by=-0.1)\nprecision.matrix.path &lt;- huge(zs, lambda = lambdas)$path\n\nConducting Meinshausen & Buhlmann graph estimation (mb)....done\nlibrary(\"ggplot2\")\nlibrary(\"scales\")\nsparsities &lt;- sapply(precision.matrix.path, sum) / 2\nggplot(data.frame(sparsities), aes(lambdas, sparsities)) +\n    geom_line() +\n    scale_x_continuous(\n        trans = \"log10\",\n        breaks = round(\n            lambdas[seq(1, length(lambdas), 4)],\n            digits = 3\n        )\n    ) +\n    ggtitle(\"Number of edges in graph as sparsity increases\") +\n    labs(x = \"Regularization parameter lambda\", y = \"Number of edges\")\nplot.all.assortativities &lt;- function(path, taxmat, lambdas) {\n    graphs &lt;- lapply(\n        lapply(path, graph.adjacency),\n        as.undirected\n    )\n    assortativities &lt;- lapply(\n        graphs,\n        function(graph) get.assortativity.at.levels(graph, taxmat)\n    )\n\n    # Remove first element as full of NaNs\n    assortativities &lt;- assortativities[2:length(assortativities)]\n    lambdas.short &lt;- lambdas[2:length(lambdas)]\n    \n    assortativities.1 &lt;- as.numeric(lapply(assortativities, function(l) l[[1]]))\n    assortativities.2 &lt;- as.numeric(lapply(assortativities, function(l) l[[2]]))\n    assortativities.3 &lt;- as.numeric(lapply(assortativities, function(l) l[[3]]))\n    assortativities.4 &lt;- as.numeric(lapply(assortativities, function(l) l[[4]]))\n    assortativities.5 &lt;- as.numeric(lapply(assortativities, function(l) l[[5]]))\n    ggplot(\n        data.frame(assortativities.1),\n        aes(x=lambdas.short)\n    ) +\n        geom_line(aes(y = assortativities.1, color = \"Phylum\")) +\n        geom_line(aes(y = assortativities.2, color = \"Class\")) +\n        geom_line(aes(y = assortativities.3, color = \"Order\")) +\n        geom_line(aes(y = assortativities.4, color = \"Family\")) +\n        geom_line(aes(y = assortativities.5, color = \"Genus\")) +\n        scale_colour_manual(\"\", \n            breaks = c(\"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"),\n            values = c(\"black\", \"blue\", \"maroon\", \"red\", \"orange\")\n        ) +\n        theme(legend.position = \"top\") +\n        labs(x = \"Regularization parameter lambda\", y = \"Assortativities\") +\n        ggtitle(\"Assortativities at different taxonomic levels\")\n}\nplot.all.assortativities(precision.matrix.path, taxmat_el, lambdas)\nInterestingly, at no point do we reproduce the assortativities seen in the paper. This is easily seen because order never reaches \\(0.10\\), the value that is reported by Vincent Prost (2021). Now, this may be because we’re calculating the precision matrix using the Meinshausen and Buhlmann graph estimation method, the default in igraph. Perhaps we would have better luck using the Graphical LASSO method? We know that Vincent Prost (2021) use Meinshausen and Buhlmann for their Figure 5 data, but they do not report their method for Figure 4 - the figure we’re interested in - so it is reasonable to assume that they may have used Graphical LASSO here (a method also used elsewhere in their paper).\nlambdas.glasso &lt;- 10^seq(0, -2, by=-0.1)\nglasso.path &lt;- huge(zs, lambda = lambdas.glasso, method=\"glasso\")$path\n\nConducting the graphical lasso (glasso) wtih lossless screening....in progress: 95%\nConducting the graphical lasso (glasso)....done.\nsparsities &lt;- sapply(glasso.path, sum) / 2\nggplot(data.frame(sparsities), aes(lambdas.glasso, sparsities)) +\n    geom_line() +\n    scale_x_continuous(\n        trans = \"log10\",\n        breaks = round(\n            lambdas.glasso[seq(1, length(lambdas.glasso), 4)],\n            digits = 3\n        )\n    ) +\n    ggtitle(\"Number of edges in graph as sparsity increases\") +\n    labs(x = \"Regularization parameter lambda\", y = \"Number of edges\")\nplot.all.assortativities(glasso.path, taxmat_el, lambdas.glasso)\nNo - glasso performs much notably worse! Maybe we just didn’t have the resolution with mb to find the point where the assortativity for order reached \\(0.10\\)?\nlambdas.mb.2 &lt;- 10^seq(0, -2, by=-0.05)\nmb.2 &lt;- huge(zs, lambda = lambdas.mb.2)$path\n\nConducting Meinshausen & Buhlmann graph estimation (mb)....done\nplot.all.assortativities(mb.2[2:40], taxmat_el, lambdas.mb.2[2:40])\nUnfortunately, I can’t really replicate their assortativity results, and their github doesn’t contain code to replicate it either!\nWhich is quite frustrating. Anyways, I have my own GLasso method, antGLasso, so I want to try it. It’s experimental so it doesn’t have an R interface, making it difficult to use in this R notebook, so I’ll just export the zs, run it on them, and bring it back here.\nwrite.csv(zs, \"./localdata/computed_zs.csv\")\nantGLasso.mat &lt;- as.matrix(\n    read.csv(\n        \"./localdata/antGLasso-output.csv\",\n        header=FALSE,\n        col.names=paste0(\"C\", 1:565)\n    )\n)\nantGLasso regularizes by computing the full solution and then thresholding. For the previous methods I capped out at keeping the top ~100000 edges, so we’ll do the same here.\nupper.bound &lt;- 20\nlower.bound &lt;- 0.01\nsum(abs(antGLasso.mat) &gt; upper.bound) / 2\nsum(abs(antGLasso.mat) &gt; lower.bound) / 2\n\n14.5\n\n\n131390\nlambdas.antGLasso &lt;- exp(0:19 * (log(upper.bound) - log(lower.bound)) / 19 + log(lower.bound))\nthreshold.matrix &lt;- function(mat., threshold) {\n    mat &lt;- matrix(0, dim(mat.)[[1]], dim(mat.)[[2]])\n    mat[abs(mat.) &lt; threshold] = 0\n    mat[abs(mat.) &gt; threshold] = 1\n    diag(mat) &lt;- 0\n    return(mat)\n}\nantGLasso.path &lt;- lapply(\n    lambdas.antGLasso,\n    function(thresh) threshold.matrix(antGLasso.mat, thresh)\n)\nplot.all.assortativities(antGLasso.path, taxmat_el, lambdas.antGLasso)\nLet’s zoom our assortativity plot into what’s happening before it gets very sparse.\nidcs &lt;- lambdas.antGLasso &lt; 5\nplot.all.assortativities(antGLasso.path[idcs], taxmat_el, lambdas.antGLasso[idcs])\nIt seems that once it gets very sparse, our method does well on the low-level taxonomies - these will be the edges that antGLasso most confidently believes are correct, so its nice to know that it is indeed correct. However, due to the extreme sparsity in those situations its not very useful. When looking at more reasonably sparsified situations, antGLasso does terrible. Not only does it not have substantial assortativity for the higher taxonomic levels, but the low-level taxonomies have negative assortativity.\nOnce again, a paper replication blog post ends on a bit of a downer, this time twofold:\nSuch is life."
  },
  {
    "objectID": "000_Blog/004_scRNA/ziln2.html#footnotes",
    "href": "000_Blog/004_scRNA/ziln2.html#footnotes",
    "title": "Paper Replication on Sparse Microbial Networks – Measuring the Network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSuch as phylum, or order.↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html",
    "href": "000_Blog/004_scRNA/cancer2.html",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "",
    "text": "Last time on the blog, we were looking at the paper “A living biobank of ovarian cancer ex vivo models reveals profound mitotic heterogeneity” (Nelson et al. 2020). We’ll pick up where we left off, this time trying to recreate Figure 5."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#library-size-filtering",
    "href": "000_Blog/004_scRNA/cancer2.html#library-size-filtering",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Library Size Filtering",
    "text": "Library Size Filtering\n\nlibrary.sizes &lt;- Matrix::colSums(raw.counts)\n\n\nlibrary.median.size &lt;- median(library.sizes)\nmedian.absolute.deviation &lt;- median(abs(library.sizes - library.median.size))\nlibrary.lower.bound &lt;- library.median.size - 3*median.absolute.deviation\nany(library.sizes &lt; library.lower.bound)\n\nFALSE\n\n\nIt seems that none of the cells have a sufficiently low library size to be filtered out."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#gene-count-filtering",
    "href": "000_Blog/004_scRNA/cancer2.html#gene-count-filtering",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Gene Count Filtering",
    "text": "Gene Count Filtering\n\ngene.counts &lt;- Matrix::colSums(raw.counts &gt; 0)\n\n\ngene.median.counts &lt;- median(gene.counts)\nmedian.absolute.deviation &lt;- median(abs(gene.counts - gene.median.counts))\ngene.lower.bound &lt;- gene.median.counts - 3*median.absolute.deviation\ngene.filter.index &lt;- gene.counts &lt; gene.lower.bound\nany(gene.filter.index)\n\nTRUE\n\n\n\nsum(gene.filter.index)\n\n557\n\n\nWe’ll filter out these 557 cells.\n\n# gf for gene filtering\nfiltered.counts.gf &lt;- raw.counts[, !gene.filter.index]\nfiltered.row.info.gf &lt;- row.info\nfiltered.col.info.gf &lt;- col.info[!gene.filter.index, ]"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#mitochondrial-filtering",
    "href": "000_Blog/004_scRNA/cancer2.html#mitochondrial-filtering",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Mitochondrial Filtering",
    "text": "Mitochondrial Filtering\nFor this we’ll use the biomart package.\n\n\nInstall: biomaRt\n\nconda search bioconda::r-biomart\nconda install bioconda::bioconductor-biomart=2.54.0 -y\nThis is the first time I’ve had to use the bioconda channel, but their website clearly shows they’re built on top of conda-forge so I know I’m not going to get burned by channel conflicts as has happened before. Unfortunately it doesn’t work, all versions of biomart have dependency conflicts somewhere (so maybe I have been burned by channel conflicts…)\nSo now I will install biocmanager and install it the in-R way.\n# Bash\nconda search conda-forge::r-biocmanager\nconda install conda-forge::r-biocmanager=1.30.19 -y\n# R\nBiocManager::install(\"biomaRt\")\n\n\nlibrary(\"biomaRt\")\n\n\n# Configure biomart to use human info (as opposed to other species)\nmart &lt;- useDataset(\"hsapiens_gene_ensembl\", useMart(\"ensembl\"))\n\ngene.to.symbol.map &lt;- getBM(\n    filters=\"ensembl_gene_id\",\n    attributes=c(\n        \"ensembl_gene_id\",\n        \"hgnc_symbol\"\n    ),\n    values=filtered.row.info.gf$Ensembl.ID,\n    mart=mart\n)\n\nAnnoyingly, getBM silently drops IDs when it can’t find a matching attribute, so we loose twenty genes:\n\nlength(gene.to.symbol.map$ensembl_gene_id)\nlength(filtered.row.info.gf$Ensembl.ID)\n\n23264\n\n\n23284\n\n\n\nthe.lost.genes &lt;- setdiff(\n    filtered.row.info.gf$Ensembl.ID,\n    gene.to.symbol.map$ensembl_gene_id\n)\n\nWe’ll filter these genes out:\n\nnon.missing.gene.index &lt;- !(filtered.row.info.gf$Ensembl.ID %in% the.lost.genes)\nfiltered.counts.mito &lt;- filtered.counts.gf[non.missing.gene.index,]\nfiltered.row.info.mito &lt;- filtered.row.info.gf[non.missing.gene.index,]\nfiltered.col.info.mito &lt;- filtered.col.info.gf\n\nAnd then continue with our attempt to filter out highly-mitochondrial cells.\n\n# For some reason, r is behaving really strangely with `strsplit`\n# which is why this function is so strangely complicated\nmito.genes.index &lt;- sapply(\n    gene.to.symbol.map$hgnc_symbol,\n    function(x) {\n        if (x == \"\") {\n            return(FALSE)\n        }\n        split &lt;- strsplit(x, '-')\n        if (length(split) &gt;= 1) {\n            split. &lt;- split[[1]]\n            if (length(split.) &gt; 1) {\n                split. &lt;- split.[[1]]\n            }\n            out &lt;- tolower(split.) == 'mt'\n            return(out)\n        }\n        else {\n            return(FALSE)\n        }\n    }\n)\n\n\ngene.to.symbol.map[mito.genes.index, ]\n\n\nA data.frame: 13 × 2\n\n\n\nensembl_gene_id\nhgnc_symbol\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n15116\nENSG00000198695\nMT-ND6\n\n\n15119\nENSG00000198712\nMT-CO2\n\n\n15127\nENSG00000198727\nMT-CYB\n\n\n15146\nENSG00000198763\nMT-ND2\n\n\n15153\nENSG00000198786\nMT-ND5\n\n\n15162\nENSG00000198804\nMT-CO1\n\n\n15183\nENSG00000198840\nMT-ND3\n\n\n15203\nENSG00000198886\nMT-ND4\n\n\n15205\nENSG00000198888\nMT-ND1\n\n\n15211\nENSG00000198899\nMT-ATP6\n\n\n15234\nENSG00000198938\nMT-CO3\n\n\n15790\nENSG00000212907\nMT-ND4L\n\n\n17277\nENSG00000228253\nMT-ATP8\n\n\n\n\n\nSo now we have a list of all mitochondrial genes.\n\nmito.counts &lt;- Matrix::colSums(filtered.counts.mito * mito.genes.index)\nnot.mito.counts &lt;- Matrix::colSums(filtered.counts.mito * !mito.genes.index)\n\n\nmito.percent &lt;- mito.counts / not.mito.counts\n\n\nmito.median.percent &lt;- median(mito.percent)\nmedian.absolute.deviation &lt;- median(abs(mito.percent - mito.median.percent))\nmito.lower.bound &lt;- mito.median.percent - 3*median.absolute.deviation\nmito.index &lt;- mito.percent &lt; mito.lower.bound\nany(mito.index)\n\nTRUE\n\n\n\nsum(mito.index)\n\n99\n\n\n\nfiltered.counts.mito.2 &lt;- filtered.counts.mito[, !mito.index]\nfiltered.row.info.mito.2 &lt;- filtered.row.info.mito\nfiltered.col.info.mito.2 &lt;- filtered.col.info.mito[!mito.index, ]"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#multiplets",
    "href": "000_Blog/004_scRNA/cancer2.html#multiplets",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Multiplets",
    "text": "Multiplets\n\nAfter this filtering two cells still showed an outlier distribution on the library size. These two “cells” were assumed to be muliplets and were excluded from further analyses.\n\nTo see the multiplets, we can make a box and whisker plot.\n\nlibrary(\"ggplot2\")\n\n\n# We need to recreate the sizes as things have been filtered since then\nlibrary.sizes &lt;- Matrix::colSums(filtered.counts.mito.2)\nsizes.to.plot &lt;- data.frame(library.sizes)\n\n\nggplot(sizes.to.plot, aes(y=library.sizes)) +\ngeom_boxplot(outlier.colour=\"black\", outlier.shape=16,\n             outlier.size=3, notch=FALSE)\n\n\n\n\nThis box-and-whiskers plot shows more than two outliers, although there are two cells that are clearly more outlier-y than the rest. We should just filter out the two obvious ones, to adhere to the paper, but we’ll filter out all of them for the sake of reducing our dataset further.\n\nAn outlier is an observation that is numerically distant from the rest of the data. When reviewing a boxplot, an outlier is defined as a data point that is located outside the fences (“whiskers”) of the boxplot (e.g: outside 1.5 times the interquartile range above the upper quartile and bellow the lower quartile). – R Statistics Blog, available here.\n\n\noffset &lt;- 1.5 * IQR(library.sizes)\nquants &lt;- quantile(library.sizes, probs=c(0.25, 0.75))\nquants[\"25%\"] &lt;- quants[\"25%\"] - offset\nquants[\"75%\"] &lt;- quants[\"75%\"] + offset\noutliers.index &lt;- library.sizes &lt; quants[\"25%\"] | library.sizes &gt; quants[\"75%\"]\nsum(outliers.index) / length(outliers.index)\n\n0.0242925509779442\n\n\nAbout 2.5% of the data is an outlier! Which to me feels as if they’re not-so-outlier-y after all! (It goes up to 8% if you use the unfiltered data).\n\n\nSidequest: What is the largest percent of a dataset that we can make be outliers?\n\nWell, assuming you have a dataset with 4 elements, the IQR is determined by the middle two elements; you can move the outer two as far away from it as possible. Thus you can force them to be outliers, giving you an outlier percentage of 50%.\nThis should be optimal, because at least 50% of the data has to lie within the IQR by the definition of the IQR.\n\n\n\n50%\nNormally I’d be reluctant to remove these, but we have so much data that it’d be nice to shave some of it off1.\n\nfiltered.counts.iqr &lt;- filtered.counts.mito.2[, !outliers.index]\nfiltered.row.info.iqr &lt;- filtered.row.info.mito.2\nfiltered.col.info.iqr &lt;- filtered.col.info.mito.2[!outliers.index, ]\n\n\ndim(filtered.counts.iqr)\n\n\n2326418757"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#normalization-and-miscellaneous-filtering",
    "href": "000_Blog/004_scRNA/cancer2.html#normalization-and-miscellaneous-filtering",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Normalization and miscellaneous filtering",
    "text": "Normalization and miscellaneous filtering\n\nRaw counts of the remaining cells were then normalised using the deconvolution-based method and then log-transformed. We also filtered out the genes with average counts below 0.01 assuming these low-abundance genes to be unreliable for statistical inference.\n\nThe question is - what is this deconvolution-based method? I found this helpful resource which describes it well. Essentially, we want to be able to normalize each cell by their library size, but due to the sparsity of scRNA-seq we need to pool cells together to achieve sufficient mass. We then “deconvolve” the pooled values to find them on a per-cell basis.\n\n\nInstall: scran\n\nBiocManager::install(\"scran\")\n\n\nlibrary(scran)\n\n\nquick.clusters &lt;- quickCluster(filtered.counts.iqr)\n\nWarning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n“collapsing to unique 'x' values”\n\n\n\ntable(quick.clusters)\n\nquick.clusters\n   1    2    3    4    5    6    7    8    9   10   11   12 \n1807 1803 3116 1371 2731 1939 1382 1620  150 2608  126  104 \n\n\n\ndeconvoluted.counts &lt;- calculateSumFactors(\n    filtered.counts.iqr,\n    cluster=quick.clusters\n)\nsummary(deconvoluted.counts)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1732  0.6554  0.9378  1.0000  1.2619  3.5655 \n\n\nSo, in short, the output is a length of factors to divide the cells by.\n\ndim(filtered.counts.iqr)\nlength(deconvoluted.counts)\n\n\n2326418757\n\n\n18757\n\n\n\nfiltered.counts.deconv &lt;- filtered.counts.iqr / deconvoluted.counts\nfiltered.row.info.deconv &lt;- filtered.row.info.iqr\nfiltered.col.info.deconv &lt;- filtered.col.info.iqr\n\nNow we’ll filter out the genes with small mean counts.\n\nsmall.mean.genes.index &lt;- rowMeans(filtered.counts.deconv) &lt; 0.01\n\n\nfiltered.counts.gene.mean &lt;- filtered.counts.deconv[!small.mean.genes.index, ]\nfiltered.row.info.gene.mean &lt;- filtered.row.info.deconv[!small.mean.genes.index]\nfiltered.col.info.gene.mean &lt;- filtered.col.info.deconv\n\n\ndim(filtered.counts.gene.mean)\n\n\n1487218757\n\n\nFinally, we’ll log-transform the data. We’ll leave the 0 values as 0 for the sake of our RAM, which means we need to peak into the internal workings of the sparse matrix class.\n\nclass(filtered.counts.gene.mean)\n\n'dgCMatrix'\n\n\n\nfiltered.log.counts &lt;- filtered.counts.gene.mean\nfiltered.log.row.info &lt;- filtered.row.info.gene.mean\nfiltered.log.col.info &lt;- filtered.col.info.gene.mean\nfiltered.log.counts@x &lt;- log(filtered.log.counts@x)\n\n\ndim(filtered.log.counts)\n\n\n1487218757"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer2.html#footnotes",
    "href": "000_Blog/004_scRNA/cancer2.html#footnotes",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5a",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI know that’s not the normal attitude, I just don’t want to have to deal with any code that takes longer than a second to run 😅. Although I chose R over numpy so clearly I don’t care that much about speed… Although I guess I’m sure that R can be fast too if you understand how you’re supposed to use it.↩︎\nIn the statistical sense.↩︎\nIf only sos-notebook didn’t ruin syntax highlighting… This might require a Sidequest sometime soon.↩︎\nTwo days, to be precise. It’s 20:54 Jan 21 and I started in the morning of Jan 20 (with many distractions, to be fair)↩︎\nShoutout to my hour-long adventure trying to install scater.↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer3.html",
    "href": "000_Blog/004_scRNA/cancer3.html",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5b-c",
    "section": "",
    "text": "We’ll continue where we left off in the last post, trying to recreate Figure 5 from a paper by Nelson et al. (2020).\nThis time, we’ll focus on Figure 5b, which entails deciding whether a cell is stromal or tumour."
  },
  {
    "objectID": "000_Blog/004_scRNA/cancer3.html#footnotes",
    "href": "000_Blog/004_scRNA/cancer3.html#footnotes",
    "title": "A Living Biobank of Ovarian Cancer – Figure 5b-c",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the sense of “functional programming”↩︎\nAlthough it does allow for non-functional styles too↩︎\nalready loaded as a dependency of either scran or scatter, I have no idea which.↩︎\nI should have thought of that before making the graph…↩︎\nIn their defence, BRCA1 is actually in the dataset, it just got filtered out before we got here.↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/ziln.html",
    "href": "000_Blog/004_scRNA/ziln.html",
    "title": "Paper Replication on Sparse Microbial Networks – Creating the Network",
    "section": "",
    "text": "Here, we will be looking at a paper (Vincent Prost 2021) that uses metagenomics data - specifically that of the LifeLines-DEEP gut microbiome data (Zhernakova et al. 2016).\nSpecifically, we want to recreate the following image:\n# This gives us a 1135x3957 matrix (people x species) of 16s rRNA counts `counts`\n# And a dataframe `taxmat` of 3957 rows giving taxonomic information\nload(\"localdata/ll_deep.rda\")\nInterestingly, there seem to be too many species! There are 3957 species here but there’re clearly less than that in the graph1. The paper does have this to say about it:\nFrustratingly, the image we want to recreate is actually Figure 4 in the paper. Since both figures are interrelated and both are part of the short section “Inference of real-world microbial association networks”, I think it’s reasonable that this filtering method was used for both. It is pretty annoying that it’s not more clear, though.\n# This is their own filtering code, from:\n# https://github.com/vincentprost/Zi-LN/blob/master/venn.R\n# Although I've broken it up a bit to make it more amenable to commenting\n\n# Get a boolean 1135x3957 matrix of whether the species\n# was found in the person or not\nnonzeros &lt;- counts &gt; 0\n\n# Get the number of distinct people that possessed each species\nnum.nonzeros &lt;- apply(nonzeros, 2, sum)\n\n# Get the total amount of people\ntotal.cells &lt;- dim(counts)[1]\n\n# Only keep the species who appear in more than 20% of the people\nkeep.indices &lt;- (num.nonzeros / total.cells) &gt; 0.2\ncounts_el &lt;- as.matrix(counts[, keep.indices])\ntaxmat_el &lt;- taxmat[keep.indices,]\n\n# We should now only be looking at 565 species\ndim(counts_el)\n\n\n1135565\nNow that we have the data filtered in what we hope is the same manner as the paper, we can try to run their analysis. At a high level, the key idea of this paper is to model the data as being some transform of a gaussian latent variable \\(z\\). They wrote a function infer_Z to estimate the latent \\(z\\), after which they can use a technique to estimate the precision matrix, which should give us the association network for the microbes[^I haven’t done a blog post on Gaussian Graphical Models before, but I should… This section will make more sense if you have a background in them.].\n# Load the code from the paper\nsource(\"./localdata/Zi-LN-master/inference.R\")\nsource(\"./localdata/Zi-LN-master/utils/utils.R\")\n# Get the zs\noptions(warn = -1) # turn warnings off because otherwise it's gonna scream...\nzs &lt;- infer_Z(counts_el)\ndim(zs)\n\n\n1135565\nNow we need to infer the precision matrix. We’re stuck again referring to their methodology for Figure 5. Even more annoyingly, they do not use the regularization parameter that they used, but rather just say that they kept the top 1200 edges. So I had to run manual experiments to find out! Spoiler alert, the parameter is \\(\\lambda = 0.251\\)\n# Desired sparsity level (multiply by 2 b/c symmetric)\n(2 * 1200) / (565*565)\n\n0.00751820816038844\nlibrary(\"huge\")\nlibrary(\"igraph\")\nprecision.matrix &lt;- huge(zs, lambda = 0.251)\n\nConducting Meinshausen & Buhlmann graph estimation (mb)....done\nprecision.matrix\n\nModel: Meinshausen & Buhlmann graph estimation (mb)\nInput: The Data Matrix\nPath length: 1 \nGraph dimension: 565 \nSparsity level: 0.007531538 -----&gt; 0.007531538\nassociations &lt;- precision.matrix$path[[1]]\n\n# Number of edges, to verify we've done it right\nsum(associations) / 2\n\n1200\nadjacency.graph = graph.adjacency(associations, mode = \"undirected\")\nplot(adjacency.graph)\nThere we go, we’ve plotted it! But wait… It looks rather crappy. I guess we’ll have to work a bit to make it nice.\n# Add a \"color\" column to prepare for coloring graph nodes\ntaxmat.colors &lt;- cbind(taxmat_el, \"#000000\")\ncolnames(taxmat.colors)[length(colnames(taxmat.colors))] &lt;- \"color\"\ntaxa.level &lt;- \"V5\"\nunique.taxa &lt;- unique(taxmat.colors[,taxa.level])\nnum.taxa &lt;- length(unique.taxa)\nincrement &lt;- 89 / (num.taxa-1)\nincrements &lt;- 10 + round(increment * 0:(num.taxa-1))\nhex.map &lt;- paste(\"#19\", increments, \"60\", sep=\"\")\nfor (i in 1:num.taxa) {\n    taxmat.colors[\n        unique.taxa[i]==taxmat.colors[,taxa.level],\n        \"color\"\n    ] &lt;- hex.map[i]\n}\nplot(\n    adjacency.graph,\n    edge.color = \"green\",\n    vertex.size = 5,\n    vertex.label = \"\",\n    vertex.color = taxmat.colors[,\"color\"],\n    margin=c(0, 0, 0, 0)\n)\nplot(\n    adjacency.graph,\n    vertex.label.color = \"#00000000\",\n    vertex.color = \"#00000000\",\n    edge.color = \"#00000000\",\n    vertex.frame.color = \"#00000000\"\n)\nlegend('top',\n       legend = unique.taxa,\n       bg = \"#757575\",\n       fill = hex.map,\n       ncol = 2\n)\nThis seems close enough. We do end up with much more disconnected species, and I don’t really understand why because in their code they literally run:\nWhere l is whatever lambda value causes 1200 edges. That code, however, is for Figure 5 whereas we want to replicate Figure 4; perhaps they chose a different l for Figure 4? But we’ll stop here for now."
  },
  {
    "objectID": "000_Blog/004_scRNA/ziln.html#footnotes",
    "href": "000_Blog/004_scRNA/ziln.html#footnotes",
    "title": "Paper Replication on Sparse Microbial Networks – Creating the Network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, I hand-counted them and got 500ish↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna.html",
    "href": "000_Blog/004_scRNA/scrna.html",
    "title": "Overview of scRNA-seq Data Creation – Cell Isolation",
    "section": "",
    "text": "I work with single-cell RNA sequencing data (scRNA-seq), but I don’t really understand it… ELI5 is that scRNA-seq data is a list of cells, each of which has a list of genes in that cell. The goal of this blog post is to arrive at an understanding of these.\nA useful resource is the eBook from Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) outlining the scRNA-seq workflow. Illumina is one of the companies that creates products that make scRNA-seq possible.\n\n\n\nA graphical summary of the processes leading up to sequencing.\n\n\n\n\nThe first step in scRNA-seq analysis is to procure the actual samples for the analysis. It is perhaps the most important step, as the tissue preparation method can bias your results (some cells may be better suited to surviving certain preparation methods intact, for example). It also tends to be easier to collect “less clumpy” cells, like blood cells.\nThis makes intuitive sense, as we would need some way of reaching the cells inside a clump to prevent biasing our results towards cells that tend to inhabit the surface of a clump. Reaching these cells is called dissociation. For an example of how the dissociation protocol can effect the cells you get, see Denisenko et al. (2020).\nThe goal of dissociation is to break bonds between cells (to destroy the aforementioned clumps). These bonds can be broken mechanically or enzymatically, or through a combination of both. Bonds can also be broken via the manipulation of temperature. For an example of a dissociation protocol, see Quintanal-Villalonga et al. (2022).\nThe next (optional) step is enrichment. We do this to remove unwanted cells or boost the presence of wanted ones.\nAfter preparing the sample, we want to ensure the sample is high quality. I’ll summarize what Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) has to say on the matter:\n\nWe want to ensure that our dissociation and enrichment methods have not killed too many cells. We can detect residue from dead cells to acquire an estimate, and it seems in practice the standard is to have &gt;85% viable cells.\nIf our cells got broken to pieces, or if we failed to split cells up well, we would expect this info to be observable in a cell size distribution histogram.\nWe want our cells to be concentrated at the right amount.\n\nOne method of getting these methods is flow cytometry.\n\n\n\nOnce we’ve prepared our tissue, we need to isolate the cells from eachother. (Dissociation breaks up clumps, but that does not mean the de-clumped cells are isolated - they’re still together in the same solution).\nA key distinguisher between isolation methods is throughput. Older (mechanical) methods have low throughput, and can only feasibly isolate cells from a sample of size up to the thousands. More modern “high-throughput” methods can dramatically increase this. The methods mentioned in Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) are briefly listed here:\n\n\nLow throughput\n\nSerial dilution\nMouth pipetting\nRobotic micromanipulation\nLaser capture microdissection\nFACS\n\nHigh throughput\n\nMicrofluidics circuits\nDroplet fluidics platforms\nMicrowells\nCombinatorial indexing\n\n\n\nUnderstanding the ins-and-outs of these methods, and the extent to which this list is exhaustive, is a story for another time.\n\n\n\n\nDenisenko, E., B. B. Guo, M. Jones, R. Hou, L. de Kock, T. Lassmann, D. Poppe, et al. 2020. “Systematic assessment of tissue dissociation and storage biases in single-cell and single-nucleus RNA-seq workflows.” Genome Biology 21 (1): 130.\n\n\nQuintanal-Villalonga, Á., J. M. Chan, I. Masilionis, V. R. Gao, Y. Xie, V. Allaj, A. Chow, et al. 2022. “Protocol to dissociate, process, and analyze the human lung tissue using single-cell RNA-seq.” STAR Protoc 3 (4): 101776.\n\n\nSingle-Cell Sequencing Workflow: Critical Steps and Considerations. 2019. Illumina. https://emea.illumina.com/destination/multiomics-transcriptomics.html."
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna.html#tissue-preparation",
    "href": "000_Blog/004_scRNA/scrna.html#tissue-preparation",
    "title": "Overview of scRNA-seq Data Creation – Cell Isolation",
    "section": "",
    "text": "The first step in scRNA-seq analysis is to procure the actual samples for the analysis. It is perhaps the most important step, as the tissue preparation method can bias your results (some cells may be better suited to surviving certain preparation methods intact, for example). It also tends to be easier to collect “less clumpy” cells, like blood cells.\nThis makes intuitive sense, as we would need some way of reaching the cells inside a clump to prevent biasing our results towards cells that tend to inhabit the surface of a clump. Reaching these cells is called dissociation. For an example of how the dissociation protocol can effect the cells you get, see Denisenko et al. (2020).\nThe goal of dissociation is to break bonds between cells (to destroy the aforementioned clumps). These bonds can be broken mechanically or enzymatically, or through a combination of both. Bonds can also be broken via the manipulation of temperature. For an example of a dissociation protocol, see Quintanal-Villalonga et al. (2022).\nThe next (optional) step is enrichment. We do this to remove unwanted cells or boost the presence of wanted ones.\nAfter preparing the sample, we want to ensure the sample is high quality. I’ll summarize what Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) has to say on the matter:\n\nWe want to ensure that our dissociation and enrichment methods have not killed too many cells. We can detect residue from dead cells to acquire an estimate, and it seems in practice the standard is to have &gt;85% viable cells.\nIf our cells got broken to pieces, or if we failed to split cells up well, we would expect this info to be observable in a cell size distribution histogram.\nWe want our cells to be concentrated at the right amount.\n\nOne method of getting these methods is flow cytometry."
  },
  {
    "objectID": "000_Blog/004_scRNA/scrna.html#cell-isolation",
    "href": "000_Blog/004_scRNA/scrna.html#cell-isolation",
    "title": "Overview of scRNA-seq Data Creation – Cell Isolation",
    "section": "",
    "text": "Once we’ve prepared our tissue, we need to isolate the cells from eachother. (Dissociation breaks up clumps, but that does not mean the de-clumped cells are isolated - they’re still together in the same solution).\nA key distinguisher between isolation methods is throughput. Older (mechanical) methods have low throughput, and can only feasibly isolate cells from a sample of size up to the thousands. More modern “high-throughput” methods can dramatically increase this. The methods mentioned in Single-Cell Sequencing Workflow: Critical Steps and Considerations (2019) are briefly listed here:\n\n\nLow throughput\n\nSerial dilution\nMouth pipetting\nRobotic micromanipulation\nLaser capture microdissection\nFACS\n\nHigh throughput\n\nMicrofluidics circuits\nDroplet fluidics platforms\nMicrowells\nCombinatorial indexing\n\n\n\nUnderstanding the ins-and-outs of these methods, and the extent to which this list is exhaustive, is a story for another time.\n\n\n\n\nDenisenko, E., B. B. Guo, M. Jones, R. Hou, L. de Kock, T. Lassmann, D. Poppe, et al. 2020. “Systematic assessment of tissue dissociation and storage biases in single-cell and single-nucleus RNA-seq workflows.” Genome Biology 21 (1): 130.\n\n\nQuintanal-Villalonga, Á., J. M. Chan, I. Masilionis, V. R. Gao, Y. Xie, V. Allaj, A. Chow, et al. 2022. “Protocol to dissociate, process, and analyze the human lung tissue using single-cell RNA-seq.” STAR Protoc 3 (4): 101776.\n\n\nSingle-Cell Sequencing Workflow: Critical Steps and Considerations. 2019. Illumina. https://emea.illumina.com/destination/multiomics-transcriptomics.html."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html",
    "href": "000_Blog/004_scRNA/006_protocols.html",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "",
    "text": "I’ve seen a few “protocols” pop up, such as Smart-seq2 and 10x Chromium. This blog aims to answer the following questions:\nI am indebted to Ziegenhain et al. (2017) for much of my understanding."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#reverse-transcription",
    "href": "000_Blog/004_scRNA/006_protocols.html#reverse-transcription",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "Reverse Transcription",
    "text": "Reverse Transcription\nThe creation of cDNA from RNA (called so because DNA-&gt;RNA is transcription). cDNA is more stable than RNA, so it’s easier to work with."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#nd-strand-synthesis",
    "href": "000_Blog/004_scRNA/006_protocols.html#nd-strand-synthesis",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "2nd Strand Synthesis",
    "text": "2nd Strand Synthesis\nThe synthesis of the second strand of cDNA or RNA (😉). (Since reverse transcription is just creating one strand of cDNA)"
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#amplification",
    "href": "000_Blog/004_scRNA/006_protocols.html#amplification",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "Amplification",
    "text": "Amplification\nBecause of limited read length, fragmentation is required (see the section on it for details). However, it can be hard to reconstruct the original transcript with confidence. If we amplify the RNA/cDNA first then after fragmentation this problem will be mitigated. Amplification is just the process of creating many copies of RNA/cDNA already extant in your sample.\n\n\n\nAmplification is beneficial because it will give us more reads to work with during sequencing, if fragmentation is performed.\n\n\nFun fact: you don’t need amplification when using nanopores.\n\nThe facility of nanopore technology to analyse native DNA, without the requirement for amplification, eliminates PCR bias and allows the identification of base modifications alongside nucleotide sequence — with no requirement for time-consuming, harsh, and, often inefficient, chemical conversion (e.g. bisulfite conversion).\n– Oxford Nanopore, about their products"
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#pcr",
    "href": "000_Blog/004_scRNA/006_protocols.html#pcr",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "PCR",
    "text": "PCR\nPCR stands for Polymerase Chain Reaction.\n\nSometimes called “molecular photocopying,” the polymerase chain reaction (PCR) is a fast and inexpensive technique used to “amplify” - copy - small segments of DNA.\n– “Polymerase Chain Reaction Fact Sheet” (2020)\n\n\nIn the PCR method, a pair of primers hybridizes with the sample DNA and defines the region that will be amplified, resulting in millions and millions of copies in a very short timeframe.\n– Shchelochkov (2023)"
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#ivt",
    "href": "000_Blog/004_scRNA/006_protocols.html#ivt",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "IVT",
    "text": "IVT\nIVT stands for In Vitro Transcription, and is a method of amplification by making the previously produced cDNA molecules be transcribed into RNA. Note that this is why CEL-seq2/C1 and MARS-seq in the diagram above go from RNA to cDNA and then back to RNA (because they both use IVT.\n\nIn vitro transcription is a simple procedure that allows for template-directed synthesis of RNA molecules of any sequence from short oligonucleotides to those of several kilobases in μg to mg quantities. It is based on the engineering of a template that includes a bacteriophage promoter sequence (e.g. from the T7 coliphage) upstream of the sequence of interest followed by transcription using the corresponding RNA polymerase.\n– Beckert and Masquida (2011)"
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#rna-fragmentation",
    "href": "000_Blog/004_scRNA/006_protocols.html#rna-fragmentation",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "RNA fragmentation",
    "text": "RNA fragmentation\n\nAfter poly(A) + selection or rRNA depletion, RNA samples are typically subject to RNA fragmentation to a certain size range before RT. This is necessary because of the size limitation of most current sequencing platforms, e.g., &lt;600 bp on Illumina sequencers.\n–  Fragmentation Subsection; General Aspects OF RNA-Seq Section; Hrdlickova, Toloue, and Tian (2017) \n\nFragmentation is done when there is a limited read length in your flow cell. If we did not fragment the RNA, then we would need large read length to read the base pairs in the middle of a long RNA strand. Fragmentation is not necessary if your read length is longer than the largest RNA molecule (or unlimited). Fragmentation puts extra strain on downstream analysis, because we need to reconstruct the original sequence from the fragments. Furthermore, fragmentation isn’t random; the techniques used to break up the RNA may preferentially cause breaks in some places compared to others. Hrdlickova, Toloue, and Tian (2017) talk about about the biases for different fragmentation methods.\nFragmentation can also be used on the cDNA, but it can be harder to automate. However, tagmentation is done on cDNA."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#tagmentation",
    "href": "000_Blog/004_scRNA/006_protocols.html#tagmentation",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "Tagmentation",
    "text": "Tagmentation\n\nRecent development in using transposon-based, so-called tagmentation method has made it simple to fragment cDNA and add adapter sequences at the same time. In this method, an active variant of the Tn5 transposase mediates the fragmentation of double-stranded DNA and ligates adapter oligonucleotides at both ends in a quick reaction (~5 min). However, it is notable that Tn5 and other enzyme-based cDNA fragmentation methods require a precise enzyme:DNA ratio, making method optimization less straightforward than RNA fragmentation. Consequently, fragmenting RNA is currently still the most frequently used approach in RNA-Seq library preparation.\n–  Fragmentation Subsection; General Aspects OF RNA-Seq Section; Hrdlickova, Toloue, and Tian (2017) \n\nLigation is the joining of two nucleic acid fragments through the action of an enzyme - at least, according to Wikipedia. It seems that this is just the combination of fragmentation and tagging with the adapter oligonucleotides. These processes used to need to be done sequentially.\n\n\nWhy do we want to tag the adapter with oligonucleotides?\n\nThe tags are necessary to attach primers to your cDNA (Salamat 2018). This is described well during the following explanation of Index PCR:\n\nA single Illumina flow cell can sequence multiple samples as long as the expected reads will not saturate (exceed) the capacity of the flow cell. However, since the samples are pooled and loaded as one into the flow cell, there must be a mechanism to which to distinguish sequences from one sample to another. This is accomplished via a process called Index PCR. Here, custom oligonucleotides called index primers are used to amplify and ‘barcode’ the fragments. Each index primer contains the following: * Read (1 or 2) sequencing primer: a segment complementary to the ‘tag’ introduced during tagmentation, * Indexing sequence: a unique DNA sequence for identification / barcoding of samples * Sequencing Anchor (P5/P7): sequences complementary to the oligos in the flow cell. These allow the index PCR products (libraries) to bind to the Illumina flow cell for sequencing.\nSTEP 4: LIBRARY PREPARATION: AMPLIFICATION; Salamat (2018)\n\nFor an explanation on all the details here, see the section on Index PCR; essentially, the tags are necessary to hook an index primer into your cDNA, which is then necessary to hook it up with the oligos in your flow cell."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#primers",
    "href": "000_Blog/004_scRNA/006_protocols.html#primers",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "Primers",
    "text": "Primers\n\nA primer, as related to genomics, is a short single-stranded DNA fragment used in certain laboratory techniques, such as the polymerase chain reaction (PCR). In the PCR method, a pair of primers hybridizes with the sample DNA and defines the region that will be amplified, resulting in millions and millions of copies in a very short timeframe. Primers are also used in DNA sequencing and other experimental processes.\n– Shchelochkov (2023)\n\nPrimers are used to attach to the R/DNA in as a marker for amplification."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#enrichment",
    "href": "000_Blog/004_scRNA/006_protocols.html#enrichment",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "3’ Enrichment",
    "text": "3’ Enrichment\nEnrichment is the process of increasing the proportion of the thingamajig of interest (RNA/cDNA in this case) relative to the rest of stuff in the sample. It differs from amplification in that it’s not increasing the thingamajig of interest but rather getting rid of the thingamajigs of disinterest. (So it’s more like depoorment). 3’ enrichment specifically utilizes the 3’ end of the RNA/cDNA to do the enrichment."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#adapter-ligation",
    "href": "000_Blog/004_scRNA/006_protocols.html#adapter-ligation",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "Adapter Ligation",
    "text": "Adapter Ligation\n\nIn a standard RNA-Seq library protocol, cDNAs of a desired size generated from RT of fragmented RNAs with random hexamer primers or from fragmented full-length cDNAs are ligated to DNA adapters before amplification and sequencing. While simple, this approach loses the information about which DNA strand corresponds to the sense strand of RNA. Lack of strand specificity would make it difficult to identify antisense and novel RNA species and cause inaccurate measurement of sense RNA expression. Several methods have been developed to capture the directionality of RNA in cDNA libraries.\n–  Adapters and Directionality Subsection; General Aspects OF RNA-Seq Section; Hrdlickova, Toloue, and Tian (2017) \n\nRT stands for Reverse Transcription, described earlier. Antisense RNA is RNA that is complementary to mRNA. Adapters are useful for sequencing as they allow the transcripts to bind to the oligos in SBS flow cells."
  },
  {
    "objectID": "000_Blog/004_scRNA/006_protocols.html#umi-vs-full-length",
    "href": "000_Blog/004_scRNA/006_protocols.html#umi-vs-full-length",
    "title": "Overview of scRNA-seq Data Creation – Sequencing Protocol",
    "section": "UMI vs Full Length",
    "text": "UMI vs Full Length\n\nUMI is an acronym for Unique Molecular Identifier. UMIs are complex indices added to sequencing libraries before any PCR amplification steps, enabling the accurate bioinformatic identification of PCR duplicates. UMIs are also known as “Molecular Barcodes” or “Random Barcodes”\n– Tech (n.d.)\n\nUMIs are applied to individual strands of cDNA - i.e they are not applied to identify cells (barcoding) or samples (Unique Dual Indices (UDIs)), but individual transcripts of the original RNA. This can help accurately piece reads together when sequencing. The UMI will only be on one end of the strand and hence many reads will not contain the UMI on them, but when piecing them together post-fragmentation it can provide more information on what can be matched with what.\n\nThe main disadvantage of both Smart-seq protocols is that the generation of full-length cDNA libraries precludes an early barcoding step and the incorporation of UMIs.\n–  Fragmentation Subsection; General Aspects OF RNA-Seq Section; Hrdlickova, Toloue, and Tian (2017) \n\n\nOther protocols have sacrificed full-length coverage in order to sequence part of the primer used for cDNA generation. This enables early barcoding of libraries (i.e., the incorporation of cell-specific barcodes), allowing for multiplexing the cDNA amplification and thereby increasing the throughput of scRNA-seq library generation by one to three orders of magnitude\n–  Introduction Section; Hrdlickova, Toloue, and Tian (2017) \n\nNot all methods give full coverage of the transcripts; when they do, it is called full length."
  },
  {
    "objectID": "000_Blog/004_scRNA/antglasso-fixed.html",
    "href": "000_Blog/004_scRNA/antglasso-fixed.html",
    "title": "Fixed antGLasso",
    "section": "",
    "text": "In previous blog posts we looked at estimating dependencies between microbes in a microbiome (Dataset and Baseline Methodology: Vincent Prost 2021). We saw that my custom method, antGLasso, did not perform as well as the method we compared against - in fact, it seemed as though antGLasso was saying nothing of value!\nAfter much investigation, this was due to an error in my implementation, which I have now fixed. Let’s see how it compares, using their assortativity metric!\n\nlibrary(igraph)\nlibrary(ggplot2)\n\n\n\n\n\nCode here\n\n\n\n\n\nantGLasso.mat &lt;- as.matrix(\n    read.csv(\n        \"./localdata/antGlasso-output-iter-raws--fast-test.csv\",\n        header=FALSE,\n        col.names=paste0(\"C\", 1:565)\n    )\n)\n\n\n# These bounds were chosen by eye.\n#upper.bound &lt;- 0.15\n#lower.bound &lt;- 0.05\nupper.bound &lt;- 0.6\nlower.bound &lt;- 0.07\nsum(abs(antGLasso.mat) &gt; upper.bound) / 2 - 282.5\nsum(abs(antGLasso.mat) &gt; lower.bound) / 2 - 282.5\nantGLasso.lambdas &lt;- exp(0:19 * (log(upper.bound) - log(lower.bound)) / 19 + log(lower.bound))\n\n26\n\n\n14335\n\n\n\n# Construct the regularized antGLasso matrices\nthreshold.matrix &lt;- function(mat., threshold) {\n    mat &lt;- matrix(0, dim(mat.)[[1]], dim(mat.)[[2]])\n    mat[abs(mat.) &lt; threshold] = 0\n    mat[abs(mat.) &gt; threshold] = 1\n    diag(mat) &lt;- 0\n    return(mat)\n}\nnum.elements &lt;- function(mat) {\n    sum(mat != 0)\n}\nantGLasso.path &lt;- lapply(\n    antGLasso.lambdas,\n    function(thresh) threshold.matrix(antGLasso.mat, thresh)\n)\n\n\n\n\n\n# Load the data\nload(\"localdata/ll_deep.rda\")\n\n# Rename the taxmat columns to something more informative\ncolnames(taxmat) &lt;- c(\n    \"Domain\",\n    \"Phylum\",\n    \"Class\",\n    \"Order\",\n    \"Family\",\n    \"Genus\"\n)\n\n# Load the libraries\nsource(\"./localdata/Zi-LN-master/inference.R\")\nsource(\"./localdata/Zi-LN-master/utils/utils.R\")\nlibrary(\"huge\")\nlibrary(\"igraph\")\n\n# Get a boolean 1135x3957 matrix of whether the species\n# was found in the person or not\nnonzeros &lt;- counts &gt; 0\n\n# Get the number of distinct people that possessed each species\nnum.nonzeros &lt;- apply(nonzeros, 2, sum)\n\n# Get the total amount of people\ntotal.cells &lt;- dim(counts)[1]\n\n# Only keep the species who appear in more than 20% of the people\nkeep.indices &lt;- (num.nonzeros / total.cells) &gt; 0.2\ncounts_el &lt;- as.matrix(counts[, keep.indices])\nwrite.csv(counts_el, \"./localdata/filtered-raw-counts-ziln.csv\")\n\ntaxmat_el &lt;- taxmat[keep.indices,]\n\n# Get the zs\noptions(warn = -1) # turn warnings off because otherwise it's gonna scream...\nzs &lt;- infer_Z(counts_el)\n\n\n# Get the matrix for ZiLN methodology\nziln.lambdas &lt;- 10^seq(-0.1, -1.1, by=-0.05)\nziln.path &lt;- huge(zs, lambda=ziln.lambdas)$path\n\nConducting Meinshausen & Buhlmann graph estimation (mb)....done\n\n\n\nnum.elements(ziln.path[[1]])\nnum.elements(ziln.path[[20]])\n\n38\n\n\n13752"
  },
  {
    "objectID": "000_Blog/004_scRNA/antglasso-fixed.html#load-in-the-data",
    "href": "000_Blog/004_scRNA/antglasso-fixed.html#load-in-the-data",
    "title": "Fixed antGLasso",
    "section": "",
    "text": "Code here"
  },
  {
    "objectID": "000_Blog/004_scRNA/antglasso-fixed.html#antglasso",
    "href": "000_Blog/004_scRNA/antglasso-fixed.html#antglasso",
    "title": "Fixed antGLasso",
    "section": "",
    "text": "antGLasso.mat &lt;- as.matrix(\n    read.csv(\n        \"./localdata/antGlasso-output-iter-raws--fast-test.csv\",\n        header=FALSE,\n        col.names=paste0(\"C\", 1:565)\n    )\n)\n\n\n# These bounds were chosen by eye.\n#upper.bound &lt;- 0.15\n#lower.bound &lt;- 0.05\nupper.bound &lt;- 0.6\nlower.bound &lt;- 0.07\nsum(abs(antGLasso.mat) &gt; upper.bound) / 2 - 282.5\nsum(abs(antGLasso.mat) &gt; lower.bound) / 2 - 282.5\nantGLasso.lambdas &lt;- exp(0:19 * (log(upper.bound) - log(lower.bound)) / 19 + log(lower.bound))\n\n26\n\n\n14335\n\n\n\n# Construct the regularized antGLasso matrices\nthreshold.matrix &lt;- function(mat., threshold) {\n    mat &lt;- matrix(0, dim(mat.)[[1]], dim(mat.)[[2]])\n    mat[abs(mat.) &lt; threshold] = 0\n    mat[abs(mat.) &gt; threshold] = 1\n    diag(mat) &lt;- 0\n    return(mat)\n}\nnum.elements &lt;- function(mat) {\n    sum(mat != 0)\n}\nantGLasso.path &lt;- lapply(\n    antGLasso.lambdas,\n    function(thresh) threshold.matrix(antGLasso.mat, thresh)\n)"
  },
  {
    "objectID": "000_Blog/004_scRNA/antglasso-fixed.html#ziln",
    "href": "000_Blog/004_scRNA/antglasso-fixed.html#ziln",
    "title": "Fixed antGLasso",
    "section": "",
    "text": "# Load the data\nload(\"localdata/ll_deep.rda\")\n\n# Rename the taxmat columns to something more informative\ncolnames(taxmat) &lt;- c(\n    \"Domain\",\n    \"Phylum\",\n    \"Class\",\n    \"Order\",\n    \"Family\",\n    \"Genus\"\n)\n\n# Load the libraries\nsource(\"./localdata/Zi-LN-master/inference.R\")\nsource(\"./localdata/Zi-LN-master/utils/utils.R\")\nlibrary(\"huge\")\nlibrary(\"igraph\")\n\n# Get a boolean 1135x3957 matrix of whether the species\n# was found in the person or not\nnonzeros &lt;- counts &gt; 0\n\n# Get the number of distinct people that possessed each species\nnum.nonzeros &lt;- apply(nonzeros, 2, sum)\n\n# Get the total amount of people\ntotal.cells &lt;- dim(counts)[1]\n\n# Only keep the species who appear in more than 20% of the people\nkeep.indices &lt;- (num.nonzeros / total.cells) &gt; 0.2\ncounts_el &lt;- as.matrix(counts[, keep.indices])\nwrite.csv(counts_el, \"./localdata/filtered-raw-counts-ziln.csv\")\n\ntaxmat_el &lt;- taxmat[keep.indices,]\n\n# Get the zs\noptions(warn = -1) # turn warnings off because otherwise it's gonna scream...\nzs &lt;- infer_Z(counts_el)\n\n\n# Get the matrix for ZiLN methodology\nziln.lambdas &lt;- 10^seq(-0.1, -1.1, by=-0.05)\nziln.path &lt;- huge(zs, lambda=ziln.lambdas)$path\n\nConducting Meinshausen & Buhlmann graph estimation (mb)....done\n\n\n\nnum.elements(ziln.path[[1]])\nnum.elements(ziln.path[[20]])\n\n38\n\n\n13752"
  },
  {
    "objectID": "000_Blog/004_scRNA/antglasso-fixed.html#footnotes",
    "href": "000_Blog/004_scRNA/antglasso-fixed.html#footnotes",
    "title": "Fixed antGLasso",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssuming you believe assortativity is something to be maximized, whereas really it shouldn’t be (then the graphs would be boring, it would only be useful for predicting phylogeny of unknown species) - instead assortativity is just a validation measure to show that our graphs are sensible.↩︎"
  },
  {
    "objectID": "000_Blog/004_scRNA/chicken-cells-2.html",
    "href": "000_Blog/004_scRNA/chicken-cells-2.html",
    "title": "Chicken 2: Cock-a-doodle-doo",
    "section": "",
    "text": "Today we’ll continue off from last time, looking at the data from the paper Feregrino et al. (2019). We didn’t get much done yesterday because of a side-quest involving chicken breeds.\n\nset.seed(0)\nlibrary(SingleCellExperiment)\nlibrary(scran)\nlibrary(scater)\nlibrary(dplyr)\nlibrary(biomaRt)\n\nSince every dataset from the Single Cell Expression Atlas has the same form, I figured it might be convenient to write a function to handle it. In the future I’ll refactor this into a separate script on this blog, instead of repeating it every post.\n\nload.scea.data &lt;- function(dir.path, exp.name) {\n    # Load in the matrix of counts\n    counts.mat &lt;- Matrix::readMM(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx'\n        )\n    )\n    \n    # Get the gene names for the matrix\n    counts.rows &lt;- read.csv(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx_rows'\n        ),\n        sep='\\t',\n        header=FALSE\n    )$V1\n\n    # Get the cell ids for the matrix\n    counts.cols &lt;- read.csv(\n        paste0(\n            dir.path,\n            exp.name, '-quantification-raw-files/',\n            exp.name, '.aggregated_filtered_counts.mtx_cols'\n        ),\n        sep='\\t',\n        header=FALSE\n    )$V1\n    \n    # Get all cellwise metadata\n    meta.data &lt;- read.csv(\n        paste0(\n            dir.path,\n            'ExpDesign-', exp.name, '.tsv'\n        ),\n        sep='\\t',\n        header=TRUE,\n        row.names='Assay'\n    )\n    \n    rownames(counts.mat) &lt;- counts.rows\n    colnames(counts.mat) &lt;- counts.cols\n    \n    # Ensure that the meta.data lines up with the cells\n    if (!identical(rownames(meta.data), colnames(counts.mat))) {\n        warning(\"The metadata does not line up with the cells\")\n    }\n    \n    return(\n        SingleCellExperiment(\n            assays=list(counts=counts.mat),\n            colData=meta.data\n        )\n    )\n}\n\n\nsce &lt;- load.scea.data('./localdata/Datasets/E-CURD-13/', 'E-CURD-13')\nsce\n\nclass: SingleCellExperiment \ndim: 13645 7688 \nmetadata(0):\nassays(1): counts\nrownames(13645): ENSGALG00000000003 ENSGALG00000000011 ...\n  ENSGALG00000055127 ENSGALG00000055132\nrowData names(0):\ncolnames(7688): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(18): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ...\n  Factor.Value.inferred.cell.type...ontology.labels.\n  Factor.Value.Ontology.Term.inferred.cell.type...ontology.labels.\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nAnd then, for the purposes of future quality control, we pull the mitochondrial genes using the fruits of our labor from yesterday.\n\nmart.archive &lt;- useDataset(\n    \"ggallus_gene_ensembl\",\n    useMart(\n        \"ensembl\",\n        host=\"https://apr2022.archive.ensembl.org\"\n    )\n)\ngene.to.symbol.map &lt;- getBM(\n    filters=\"ensembl_gene_id\",\n    attributes=c(\n        \"ensembl_gene_id\",\n        \"hgnc_symbol\"\n    ),\n    values=rownames(sce),\n    mart=mart.archive\n)\n\n\ngene.to.symbol.map[\n    apply(\n        gene.to.symbol.map[\"hgnc_symbol\"],\n        1,\n        function(x) grepl(\"^MT-\", x)\n    ),\n]$ensembl_gene_id\n\n\n'ENSGALG00000032142''ENSGALG00000043768'\n\n\n\nrowData(sce) &lt;- gene.to.symbol.map\nis.mito &lt;- grep(\"MT-\", rowData(sce)$hgnc_symbol)\nsce\n\nclass: SingleCellExperiment \ndim: 13645 7688 \nmetadata(0):\nassays(1): counts\nrownames(13645): ENSGALG00000000003 ENSGALG00000000011 ...\n  ENSGALG00000055127 ENSGALG00000055132\nrowData names(2): ensembl_gene_id hgnc_symbol\ncolnames(7688): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(18): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ...\n  Factor.Value.inferred.cell.type...ontology.labels.\n  Factor.Value.Ontology.Term.inferred.cell.type...ontology.labels.\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nAnd now we can run the standard scuttle::perCellQCMetrics to get some basic info on the cell quality.\n\nper.cell.QC &lt;- perCellQCMetrics(sce, subsets=list(Mito=is.mito))\n\n\nper.cell.QC\n\nDataFrame with 7688 rows and 6 columns\n                                sum  detected subsets_Mito_sum\n                          &lt;numeric&gt; &lt;numeric&gt;        &lt;numeric&gt;\nSAMN11526603-AAAAAAATTCAG       972       679                8\nSAMN11526603-AAAAACAAGTAG      1706      1050                9\nSAMN11526603-AAAAACCTGCAT       764       554                4\nSAMN11526603-AAAAACTCGTAA      1102       794               15\nSAMN11526603-AAAAAGGATTCG      1393       844               11\n...                             ...       ...              ...\nSAMN11526603-TTTTTGTTCGGG      2416      1225               14\nSAMN11526603-TTTTTTAGAGGG      2510      1332               16\nSAMN11526603-TTTTTTTGCCCT      1719      1068                5\nSAMN11526603-TTTTTTTGTGAG       898       614                4\nSAMN11526603-TTTTTTTTTTTT       869       750                5\n                          subsets_Mito_detected subsets_Mito_percent     total\n                                      &lt;numeric&gt;            &lt;numeric&gt; &lt;numeric&gt;\nSAMN11526603-AAAAAAATTCAG                     2             0.823045       972\nSAMN11526603-AAAAACAAGTAG                     2             0.527550      1706\nSAMN11526603-AAAAACCTGCAT                     2             0.523560       764\nSAMN11526603-AAAAACTCGTAA                     2             1.361162      1102\nSAMN11526603-AAAAAGGATTCG                     2             0.789663      1393\n...                                         ...                  ...       ...\nSAMN11526603-TTTTTGTTCGGG                     2             0.579470      2416\nSAMN11526603-TTTTTTAGAGGG                     2             0.637450      2510\nSAMN11526603-TTTTTTTGCCCT                     2             0.290867      1719\nSAMN11526603-TTTTTTTGTGAG                     2             0.445434       898\nSAMN11526603-TTTTTTTTTTTT                     2             0.575374       869\n\n\n\nsum(counts(sce[,1]))\nsum(counts(sce[,1])&gt;0)\n\n972.000001\n\n\n679\n\n\nWe can see that the sum and detected are the library size and the number of unique genes. It also gives us info on specific subsets, which is why we passed in the mitochondrial subset info.\n\nsce &lt;- addPerCellQCMetrics(sce, subsets=list(Mito=is.mito))\n\nUsing this, we can work out which cells to discard:\n\nreasons &lt;- perCellQCFilters(\n    sce, \n    sub.fields=c(\"subsets_Mito_percent\")\n)\n\n\nreasons\n\nDataFrame with 7688 rows and 4 columns\n         low_lib_size   low_n_features high_subsets_Mito_percent   discard\n     &lt;outlier.filter&gt; &lt;outlier.filter&gt;          &lt;outlier.filter&gt; &lt;logical&gt;\n1               FALSE            FALSE                     FALSE     FALSE\n2               FALSE            FALSE                     FALSE     FALSE\n3               FALSE            FALSE                     FALSE     FALSE\n4               FALSE            FALSE                      TRUE      TRUE\n5               FALSE            FALSE                     FALSE     FALSE\n...               ...              ...                       ...       ...\n7684            FALSE            FALSE                     FALSE     FALSE\n7685            FALSE            FALSE                     FALSE     FALSE\n7686            FALSE            FALSE                     FALSE     FALSE\n7687            FALSE            FALSE                     FALSE     FALSE\n7688            FALSE            FALSE                     FALSE     FALSE\n\n\nI quite like the scuttle::perCellQCFilters function, since it gives a list of reasons why it would discard each cell. low_lib_size and low_n_features may for example correspond to fake cells (empty droplets with background RNA) or doublets and high_subsets_Mito_percent corresponds to overly mitochondrial data.\nIn fact, there’s even some hidden metadata attached in the form of the chosen thresholds:\n\nattr(reasons$low_lib_size, \"thresholds\")\n\nlower406.582871569237higherInf\n\n\n\nprint(paste0(\"Discarding \", sum(reasons$discard)))\nsce &lt;- sce[, !reasons$discard]\n\n[1] \"Discarding 258\"\n\n\n\nlogNormCounts(sce)\n\nclass: SingleCellExperiment \ndim: 13645 7430 \nmetadata(0):\nassays(2): counts logcounts\nrownames(13645): ENSGALG00000000003 ENSGALG00000000011 ...\n  ENSGALG00000055127 ENSGALG00000055132\nrowData names(2): ensembl_gene_id hgnc_symbol\ncolnames(7430): SAMN11526603-AAAAAAATTCAG SAMN11526603-AAAAACAAGTAG ...\n  SAMN11526603-TTTTTTTGTGAG SAMN11526603-TTTTTTTTTTTT\ncolData names(25): Sample.Characteristic.organism.\n  Sample.Characteristic.Ontology.Term.organism. ... total sizeFactor\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\n\n\n\n\nReferences\n\nFeregrino, C, F Sacher, Parnas O, and P Tschopp. 2019. “A Single-Cell Transcriptomic Atlas of the Developing Chicken Limb.” BMC Genomics."
  },
  {
    "objectID": "000_Blog/010_blogging/cleaning-my-computer.html",
    "href": "000_Blog/010_blogging/cleaning-my-computer.html",
    "title": "Cleaning My Computer",
    "section": "",
    "text": "I recovered over 100 GB!\nI just spent the last hour cleaning my computer - good places to look are the downloads folder, your conda environments, and your Steam games. The remaining files are either an assortment of accumulated documents and images over the years, and a lot of internal stuff that I’m too scared to delete."
  },
  {
    "objectID": "000_Blog/010_blogging/cleaning-my-computer.html#extensions",
    "href": "000_Blog/010_blogging/cleaning-my-computer.html#extensions",
    "title": "Cleaning My Computer",
    "section": "Extensions",
    "text": "Extensions\nI never really messed with JupyterLab extensions before, but I figured I’d give them a try.\nFirst, I had to install nodejs:\nmamba install conda-forge::nodejs=18.12.1\nI installed the following from the extensions tab (looks like a puzzle piece on left hand side of JupyterLab):\n\n@jupyterlab/git\n\nCouldn’t just install from extensions list, also required:\nmamba install conda-forge::jupyterlab-git=0.41.0\n\n@jupyterlab/latex\njupyterlab-system-monitor\n\nCouldn’t just install from extensions list, also required:\nmamba install conda-forge::nbresuse=0.4.0\nAlso, even though I wasn’t told to do this, it seems I needed to run the following:\nmamba install conda-forge::jupyterlab-system-monitor=0.8.0\n\njupyterlab/fasta-extension\njupyterlab-drawio\n\nI don’t know if this mamba install is needed, just did it to cover my bases.\nmamba install conda-forge::jupyterlab-drawio=0.9.0\nFun fact: you don’t need to restart, just rebuild, for this to work!\n\n\nI didn’t test the fasta-extension or latex estensions and based on their track record I am inclined to assume they need external installations.\nAfter install, for each one of these you’ll get the following pop-up:\n\nYou can click rebuild but to be honest it didn’t seem to do anything, I’d keep getting this error:\n\nWhich was all fixed just by restarting JupyterLab Desktop, and now I have this nice convenient Git GUI!\n\nAnd also I can make diagrams real quick!\n\nThey don’t show up in the editor but do in nbdev_preview. Annoyingly drawio only exports to svg (does not show up) or pdf if you go through the hassle of fake-printing it; and even then it doesn’t look great. So the utility of this tool is limited."
  },
  {
    "objectID": "000_Blog/010_blogging/cleaning-my-computer.html#footnotes",
    "href": "000_Blog/010_blogging/cleaning-my-computer.html#footnotes",
    "title": "Cleaning My Computer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd my verdict after playing around with JupyterLab Desktop down below is that it is indeed much faster.↩︎\nThe file path will be found using the same filepaths outputted by conda env list in the previous section; you’ll then have to navigate to /bin/python to be able to select it as an option.↩︎"
  },
  {
    "objectID": "000_Blog/010_blogging/naughty_with_numpy.html",
    "href": "000_Blog/010_blogging/naughty_with_numpy.html",
    "title": "Being Naughty with Numpy",
    "section": "",
    "text": "# Setup\nimport numpy as np\n\nX = np.arange(12).reshape(3, 4)\nA = X[:2]\nB = X[2:]\n\n\ndisplay(A)\ndisplay(B)\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\n\narray([[ 8,  9, 10, 11]])\n\n\n\n# Depends only on A\nnp.lib.stride_tricks.as_strided(\n    A,\n    shape=(12,),\n    strides=(A.itemsize,)\n)[-1] = -1\n\n# A hasn't changed\ndisplay(A)\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\n\n\n# Spooky!\ndisplay(B)\n\narray([[ 8,  9, 10, -1]])\n\n\nCan you figure out why?\nHint: How does the NumPy memory model work?"
  },
  {
    "objectID": "000_Blog/010_blogging/slideshow-on-blog.html#hello-world",
    "href": "000_Blog/010_blogging/slideshow-on-blog.html#hello-world",
    "title": "Slideshow",
    "section": "Hello world!",
    "text": "Hello world!\n\n\n\n\nYou can make a slideshow for your blog!\nTo the left is an example header\n\n\n---\ntitle: \"Slideshow\"\ndescription: \"Aw yeah...\"\nauthor: \"Bailey Andrew\"\ndate: \"Jan 27 2023\"\ndraft: false\ncategories: [Blogging, Useful]\nformat: revealjs\n---\n\n\n\nprint(\"And you can include code!\")\n\nAnd you can include code!"
  },
  {
    "objectID": "000_Blog/010_blogging/slideshow-on-blog.html#but-wait",
    "href": "000_Blog/010_blogging/slideshow-on-blog.html#but-wait",
    "title": "Slideshow",
    "section": "But wait!",
    "text": "But wait!\n\n\nI believe in you :)\nSo here are some tips:\n\nStart code blocks with #| echo: true to display them\nFigure out how to get rid of the pesky empty second slide\n\nI couldn’t!"
  },
  {
    "objectID": "000_Blog/008_simplex/dirichlet_kl.html",
    "href": "000_Blog/008_simplex/dirichlet_kl.html",
    "title": "KL-Divergence of the Dirichlet Distribution",
    "section": "",
    "text": "import ternary\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import dirichlet\nfrom scipy.special import gamma, digamma\n\n\ndef get_comparison_axes():\n    _, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n    return (\n        get_ternary_axes_for_dirichlet(ax=ax1),\n        get_ternary_axes_for_dirichlet(ax=ax2)\n    )\n\ndef get_ternary_axes_for_dirichlet(ax):\n    ## Boundary and Gridlines\n    scale = 30\n    figure, tax = ternary.figure(ax=ax, scale=scale)\n\n    # Draw Boundary and Gridlines\n    tax.boundary(linewidth=1.5)\n    tax.gridlines(color=\"black\", multiple=6)\n    tax.gridlines(color=\"blue\", multiple=2, linewidth=0.5)\n\n    # Set Axis labels and Title\n    fontsize = 12\n    offset = 0.14\n    tax.set_title(\"Dirichlet Distribution\\n\", fontsize=fontsize)\n    tax.left_axis_label(\"$\\\\alpha_1$\", fontsize=fontsize, offset=offset)\n    tax.right_axis_label(\"$\\\\alpha_2$\", fontsize=fontsize, offset=offset)\n    tax.bottom_axis_label(\"$\\\\alpha_3$\", fontsize=fontsize, offset=offset)\n\n    # Background color\n    tax.set_background_color(color=\"whitesmoke\", alpha=0.7) # the detault, essentially\n\n    # Remove default Matplotlib Axes\n    tax.clear_matplotlib_ticks()\n    tax.get_axes().axis('off')\n    \n    return tax\n    \ndef dirch(p, alphas):\n    \"\"\"Computes the Shannon Entropy at a distribution in the simplex.\"\"\"\n    if min(p) == 0:\n        new_ = (np.array(p) + 0.01)\n        return dirch(new_ / np.sum(new_), alphas)\n    return dirichlet.pdf(p, alphas)\n\ndef KL(alphas_1, alphas_2):\n    # Magnitude comparison term\n    sum_1 = gamma(np.sum(alphas_1))\n    sum_2 = gamma(np.sum(alphas_2))\n    term_1 = np.log(sum_1 / sum_2)\n    \n    # Orientation comparison term\n    term_2 = np.sum(\n        np.log(\n            gamma(alphas_1)\n            / gamma(alphas_2)\n        )\n    )\n    \n    # Weighted orientation comparison term\n    term_3 = np.sum(\n        (alphas_1 - alphas_2)\n        * (digamma(alphas_1) - np.sum(digamma(alphas_2)))\n    )\n    \n    return term_1, term_2, term_3\n\n\ntax1, tax2 = get_comparison_axes()\nalphas1 = np.array([0.99, 0.8, 0.99])\nalphas2 = np.array([0.99, 1.1, 0.99])\ntax1.heatmapf(lambda p: dirch(p, alphas1), boundary=True, style='h')\ntax2.heatmapf(lambda p: dirch(p, alphas2), boundary=True, style='h')\nKL(alphas1, alphas2)\n\n(-0.2682430037849523, 0.20193211965967683, -0.1938956945448387)\n\n\n\n\n\n\nalphas1 = np.array([0.99, 0.99, 0.99])\nalphas2 = np.array([0.8, 0.8, 0.8])\ndisplay(out := KL(alphas1, alphas2))\nprint(sum(out))\nprint(sum(out[1:]))\nprint(sum(out[1:])/sum(out))\n\n(0.4487827459423686, -0.43861461490538106, 1.311706455755807)\n\n\n1.3218745867927946\n0.8730918408504258\n0.6604952160921481\n\n\n\nfor i in range(10):\n    alphas1 = np.array([0.99, 0.99, 0.99])\n    alphas2 = np.array([i/10+0.01]*3)\n    out = KL(alphas1, alphas2)\n    print(sum(out[1:])/sum(out))\n\n1.0032516022811897\n1.0049366015508523\n0.9895612860037356\n0.9633624172613837\n0.9272040376306717\n0.8803714691968922\n0.820979045835589\n0.7458099235921262\n0.6497473905563662\n0.5245956186761072\n\n\n\nfor i in range(10):\n    alphas1 = np.array([0.99, 0.99, 0.99])\n    alphas2 = np.array([0.99, i/10+0.01, 0.99])\n    out = KL(alphas1, alphas2)\n    print(sum(out[1:])/sum(out))\n\n0.9929644665475431\n0.9150841636416476\n0.8359960393617754\n0.7611816416699346\n0.6920482952977239\n0.6287613014304215\n0.5710404581911143\n0.5184542687327516\n0.47053741788439\n0.42683850783756333\n\n\n\nfor i in range(10):\n    alphas1 = np.array([0.99, 0.99, 0.99])\n    alphas2 = np.array([i/10+0.01]*3)\n    print(\n        (alphas1 @ alphas2)\n        / (np.linalg.norm(alphas1) * np.linalg.norm(alphas2))\n    )\n\n1.0000000000000002\n1.0\n1.0000000000000002\n1.0000000000000002\n1.0000000000000002\n1.0\n1.0000000000000002\n1.0\n1.0000000000000002\n1.0000000000000002\n\n\n\nfor i in range(10):\n    alphas1 = np.array([0.99, 0.99, 0.99])\n    alphas2 = np.array([0.99, i/10+0.01, 0.99])\n    print(\n        (alphas1 @ alphas2)\n        / (np.linalg.norm(alphas1) * np.linalg.norm(alphas2))\n    )\n\n0.8205993697788486\n0.8592097001276557\n0.8931041846101601\n0.922001477953326\n0.9458469136837342\n0.9647902152478092\n0.9791448197419074\n0.9893383956554502\n0.9958634776150151\n0.999234605291067\n\n\n\nchange = 1\ntotal = 100\nfor i in range(10):\n    alphas1 = np.array([0.99] * total)\n    alphas2 = np.array([i/10+0.01] * change + [0.99] * (total-change))\n    print(\n        (alphas1 @ alphas2)\n        / (np.linalg.norm(alphas1) * np.linalg.norm(alphas2))\n    )\n\n0.9950884433038646\n0.996042042598324\n0.9968928178048972\n0.9976406076930077\n0.9982853465829943\n0.9988270641922016\n0.9992658852368395\n0.9996020287923161\n0.9998358074159202\n0.9999676260368984"
  },
  {
    "objectID": "000_Blog/011_math/ks-projections.html",
    "href": "000_Blog/011_math/ks-projections.html",
    "title": "Kronecker Sum Projections",
    "section": "",
    "text": "In my work I have to deal with Kronecker Sums a lot; these are ultimately built out of Kronecker Products, which are represented by \\(\\otimes\\) work like this:\n\n\n\nTwo ways of thinking about Kronecker Products\n\n\nKronecker Sums are represented by \\(\\oplus\\) and are defined as \\(A \\oplus B = A \\otimes I + I \\otimes B\\). They have a convenient property that allows us to factor out eigenvectors: \\(A \\oplus B = \\left(V_A \\otimes V_B\\right)\\left(\\Lambda_A \\oplus \\Lambda_B\\right)\\left(V_A^T \\otimes V_B^T\\right)\\), which means that for most things I only need to consider the Kronecker Sum of diagonal matrices.\nIn my work I have two coding problems related to this task:\n\nFinding a fast algorithm for the Kronecker Sum of many diagonal matrices\nFinding a fast algorithm to KS-decompose a diagonal matrix\n\nBy KS-decompose, I mean to represent a matrix as a Kronecker Sum of other matrices of a given size. Most matrices are not perfectly KS-decomposable; specifically we want the decomposition that minimizes the Frobenius Distance between the original matrix and the proposed decomposition. We won’t go over the specifics of Frobenius Distance, but it’s basically the matrix analog for the Euclidean Distance between vectors.\nGreenewald, Zhou, and Hero (2017) have already given an analytic formula for the KS-decomposition, which we’ll use later. But we’ll tackle the Kronecker Sum algorithm first."
  },
  {
    "objectID": "000_Blog/011_math/ks-projections.html#fast-ks-diag",
    "href": "000_Blog/011_math/ks-projections.html#fast-ks-diag",
    "title": "Kronecker Sum Projections",
    "section": "Fast KS-Diag",
    "text": "Fast KS-Diag\n\ndef kronsum_diag(\n    *lams: \"1D vectors to be kronsummed\"\n):\n    # Setup\n    ds = [len(lam) for lam in lams]\n    d_lefts = np.cumprod([1] + ds[:-1]).astype(int)\n    d_rights = np.cumprod([1] + ds[::-1])[-2::-1].astype(int)\n    total = d_rights[0] * ds[0]\n    out = np.zeros(total)\n    \n    # We're gonna be really naughty here and use stride_tricks\n    # This is going to reshape our vector in a way so that the elements\n    # we want to affect are batched by the first two dimensions\n    # \n    # This is because numpy's memory format doesn't directly support\n    # block-strides, i.e. there's no direct way to grab:\n    # [x x x x] x x x x [x x x x] x x x x\n    # You can only either grab blocks:\n    # [x x x x x x x x] x x x x x x x x\n    # or strides:\n    # [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x\n    #\n    # But with stride_tricks we can massage the data\n    # into a block-stride format, at the cost of an extra dimension,\n    # via using a view (NOT a copy), making this quite\n    # an efficient operation!\n    for i in range(len(lams)):\n        sz = lams[i].strides[0]\n        toset = np.lib.stride_tricks.as_strided(\n            out,\n            shape=(\n                d_lefts[i], # The skips\n                d_rights[i], # The blocks\n                ds[i] # What we want\n            ),\n            strides=(\n                sz * ds[i] * d_rights[i],\n                sz * 1,\n                sz * d_rights[i],\n            )\n        )\n        toset += lams[i]\n        \n    return out\n    \n    \n# This is indeed correct!\n# Checked by hand\nprint(kronsum_diag(\n    np.arange(2),\n    np.arange(3),\n    np.arange(4)\n))\n\n[0. 1. 2. 3. 1. 2. 3. 4. 2. 3. 4. 5. 1. 2. 3. 4. 2. 3. 4. 5. 3. 4. 5. 6.]\n\n\n\n# We can see that this operation is just about as fast as possible\n# Time is dominated by initialization and broadcasting\n# And consistently a couple percent of the time is taken by stride tricks\n# which is a basically free operation.\n# Can run 100-200 times a second on 100x100x100 data\n# And same for 1000x1000 data\n# And 500-1000 times a second on 100x100 data\n\nTimer unit: 1e-06 s\n\nTotal time: 0.005829 s\nFile: /var/folders/k0/qy74mdx10qs493700g929k5h0000gn/T/ipykernel_27579/3788467138.py\nFunction: kronsum_diag at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def kronsum_diag(\n     2                                               *lams: \"1D vectors to be kronsummed\"\n     3                                           ):\n     4                                               # Setup\n     5         1         26.0     26.0      0.4      ds = [len(lam) for lam in lams]\n     6         1        132.0    132.0      2.3      d_lefts = np.cumprod([1] + ds[:-1]).astype(int)\n     7         1         87.0     87.0      1.5      d_rights = np.cumprod([1] + ds[::-1])[-2::-1].astype(int)\n     8         1         13.0     13.0      0.2      total = d_rights[0] * ds[0]\n     9         1       1493.0   1493.0     25.6      out = np.zeros(total)\n    10                                               \n    11                                               # We're gonna be really naughty here and use stride_tricks\n    12                                               # This is going to reshape our vector in a way so that the elements\n    13                                               # we want to affect are batched by the first two dimensions\n    14                                               # \n    15                                               # This is because numpy's memory format doesn't directly support\n    16                                               # block-strides, i.e. there's no direct way to grab:\n    17                                               # [x x x x] x x x x [x x x x] x x x x\n    18                                               # You can only either grab blocks:\n    19                                               # [x x x x x x x x] x x x x x x x x\n    20                                               # or strides:\n    21                                               # [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x [x] x\n    22                                               #\n    23                                               # But with stride_tricks we can massage the data\n    24                                               # into a block-stride format, at the cost of an extra dimension,\n    25                                               # via using a view (NOT a copy), making this quite\n    26                                               # an efficient operation!\n    27         4         32.0      8.0      0.5      for i in range(len(lams)):\n    28         3         19.0      6.3      0.3          sz = lams[i].strides[0]\n    29         6        314.0     52.3      5.4          toset = np.lib.stride_tricks.as_strided(\n    30         3          5.0      1.7      0.1              out,\n    31         3          7.0      2.3      0.1              shape=(\n    32         3         31.0     10.3      0.5                  d_lefts[i], # The skips\n    33         3          7.0      2.3      0.1                  d_rights[i], # The blocks\n    34         3          6.0      2.0      0.1                  ds[i] # What we want\n    35                                                       ),\n    36         3          4.0      1.3      0.1              strides=(\n    37         3         20.0      6.7      0.3                  sz * ds[i] * d_rights[i],\n    38         3          5.0      1.7      0.1                  sz * 1,\n    39         3          6.0      2.0      0.1                  sz * d_rights[i],\n    40                                                       )\n    41                                                   )\n    42         3       3620.0   1206.7     62.1          toset += lams[i]\n    43                                                   \n    44         1          2.0      2.0      0.0      return out"
  },
  {
    "objectID": "000_Blog/011_math/ks-projections.html#footnotes",
    "href": "000_Blog/011_math/ks-projections.html#footnotes",
    "title": "Kronecker Sum Projections",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBesides the memory of shape and strides themselves, but these are negligibly sized.↩︎\nI know, I’m really pulling out all the stops in advanced NumPy - if only there was some way to get einsum involved…↩︎\nTo be honest I expect there are some optimizations to be made here since the input list is so small (and the fact that it’s being passed as a list… And that the computation for these could be done jointly quite easily… etc…↩︎\nAnd we should expect much less since the raw amount of additions we have to do is quite large so there’s no way we’d get it down to the cost of an np.prod!↩︎"
  },
  {
    "objectID": "000_Blog/2022tuedec13.html",
    "href": "000_Blog/2022tuedec13.html",
    "title": "Making a blog",
    "section": "",
    "text": "Sam inspired me to make a blog, so here it is. The goal of this blog is to document random problems I encounter over the course of the PhD, since often I end up encountering the same problem multiple times but forget how to solve it.\n\n\nThe first problem I encountered, albeit not a recurring one, is how to make a blog. Sam made me aware of Jekyll but I wanted to find a Jupyter-based solution since I do (almost) all of my work in JupyterLab. This led me to nbdev. nbdev does many things, it seems, one of which is to allow you to compile .ipynb files into a blog.\n\n\nI then created two new conda environments, Jupyter-nbdev and Blog-nbdev: the first environment will be for the nbdev-empowered JupyterLab editor, and the second environment will be the specific kernel I’ll use with the editor.\nSince I’ll have multiple environments open, as well as launching JupyterLab from a terminal and handling GitHub push/pulls to publish the blog, I created a terminal setup that looks like this:\n\n\n\nCustomized Terminal\n\n\nI’ll share the commands I used to set this up, with a comment at the start indicating which terminal tab the commands should be run in.\n# (Jupyter-nbdev)\nconda create -n Jupyter-nbdev\nconda activate Jupyter-nbdev\n\nconda install conda-forge::python=3.9 fastai::nbdev=2.3.9 conda-forge::jupyterlab=3.5.1 conda-forge::nb_conda_kernels=2.3.1\nI chose Python 3.9 because in the past bumping up to 3.10 has caused some packages to not be available. I found the specific versions for other packages by running search commands and picking the most recent version:\n# (Jupyter-nbdev)\nconda search conda-forge::nbdev\nconda search fastai::nbdev\nconda search conda-forge::jupyterlab\nconda search conda-forge::nb_conda_kernels\nI would have preferred to install nbdev from conda-forge (since in the past I’ve had problems from mixing channels) but the only version available was a decent bit behind the fastai channel, and I’m early in the process so it wouldn’t be a big deal if I needed to start over due to channel-mixing woes.\n# (Blog-nbdev)\nconda create -n Blog-nbdev\nconda activate Blog-nbdev\nconda install conda-forge::python=3.9 fastai::nbdev=2.3.9 conda-forge::ipykernel=6.19.2\nipykernel and nb_conda_kernels are used to allow the Jupyter-nbdev JupyterLab to find the Blog-nbdev kernel. From there, I launched JupyterLab:\n# (JupyterLab)\nconda activate Jupyter-nbdev\njupyter lab\n\n\n\nI followed the steps here. In short: create a new public repository with no ReadMe or other file, but with the ‘description’ field filled in. Then, go to the directory that you want to create your repo in in your terminal:\n# (GitHub)\nconda activate Blog-nbdev\ngit clone https://github.com/YOUR_GITHUB_USERNAME/NAME_OF_YOUR_REPO.git\ncd NAME_OF_YOUR_REPO\nnbdev_new\nThe last command tells nbdev to fill out the repository with the template for your blog. I got the following output from nbdev_new:\n# Output of (GitHub)\n/Users/baileyandrew/opt/anaconda3/envs/Blog-nbdev/lib/python3.9/site-packages/ghapi/core.py:101: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\nrepo = Blog # Automatically inferred from git\nbranch = main # Automatically inferred from git\nuser = BaileyAndrew # Automatically inferred from git\nauthor = Bailey Andrew # Automatically inferred from git\nauthor_email = MY_EMAIL@cool-emails.com # Automatically inferred from git\ndescription = A blog. # Automatically inferred from git\nsettings.ini created.\n/bin/sh: quarto: command not found\nThe output looks a little warning-y, but let’s continue for now:\n# (GitHub)\ngit add .\ngit commit -m 'Created Blog'\ngit push\n\n# Output\n! [remote rejected] main -&gt; main (refusing to allow a Personal Access Token to create or update workflow .github/workflows/deploy.yaml without workflow scope)\nThe push command fails; the error message seems related to the warning I got previously. The problem seems to be that I only ever create GitHub tokens with ‘repo’ permissions, not ‘workflow’ permissions, but nbdev uses the workflow feature to prepare the website. This is easy to fix: on GitHub, click on your profile and go to ‘settings’, then ‘developer settings’, ‘personal access tokens’, and finally ‘tokens (classic)’. You can either create a new token, or update an existing one - just make sure to add the ‘workflow’ permissions.\nOnce you’ve pushed, go into settings on GitHub to set the branch to be gh-pages: \nFinally, run the following to allow nbdev to trigger workflows:\n# (GitHub)\nnbdev_install_hooks\nNow, whenever you push to main, a workflow will automatically trigger that will generate the html/css/js for your website and put it in the gh-pages branch. (I do not know if pushing to other branches would trigger this as well).\n\n\n\nWhen you’re ready to start writing blog posts, go into the nbs folder and create a new .ipynb with the kernel Python [conda env:Blog-nbdev]. (If it’s not in the nbs folder, it won’t appear in the blog). There are already two examples, index.ipynb and 00_core.ipynb - index.ipynb probably fills some special role so don’t mess with it, but you can delete 00_core.ipynb.\nImportantly, your first cell should be a markdown cell with just a top-level header:\n# Top-level header\nThe name of the header will be the name of the blog post in the generated website (and the name of the file will be the name of the url to that post). If you have anything else in the first cell, it will probably be cut off. Your blog will likely look something like this initially:\n\n\n\nExample Blog\n\n\nThe last issue I had was getting the images to display, which was overcome by creating an images sub-folder of nbs and storing them there.\nAfter that, it worked like a charm! Running nbdev_preview was quite helpful in debugging the issues, as it previews your website and updates automatically!"
  },
  {
    "objectID": "000_Blog/2022tuedec13.html#how-to-make-a-blog",
    "href": "000_Blog/2022tuedec13.html#how-to-make-a-blog",
    "title": "Making a blog",
    "section": "",
    "text": "The first problem I encountered, albeit not a recurring one, is how to make a blog. Sam made me aware of Jekyll but I wanted to find a Jupyter-based solution since I do (almost) all of my work in JupyterLab. This led me to nbdev. nbdev does many things, it seems, one of which is to allow you to compile .ipynb files into a blog.\n\n\nI then created two new conda environments, Jupyter-nbdev and Blog-nbdev: the first environment will be for the nbdev-empowered JupyterLab editor, and the second environment will be the specific kernel I’ll use with the editor.\nSince I’ll have multiple environments open, as well as launching JupyterLab from a terminal and handling GitHub push/pulls to publish the blog, I created a terminal setup that looks like this:\n\n\n\nCustomized Terminal\n\n\nI’ll share the commands I used to set this up, with a comment at the start indicating which terminal tab the commands should be run in.\n# (Jupyter-nbdev)\nconda create -n Jupyter-nbdev\nconda activate Jupyter-nbdev\n\nconda install conda-forge::python=3.9 fastai::nbdev=2.3.9 conda-forge::jupyterlab=3.5.1 conda-forge::nb_conda_kernels=2.3.1\nI chose Python 3.9 because in the past bumping up to 3.10 has caused some packages to not be available. I found the specific versions for other packages by running search commands and picking the most recent version:\n# (Jupyter-nbdev)\nconda search conda-forge::nbdev\nconda search fastai::nbdev\nconda search conda-forge::jupyterlab\nconda search conda-forge::nb_conda_kernels\nI would have preferred to install nbdev from conda-forge (since in the past I’ve had problems from mixing channels) but the only version available was a decent bit behind the fastai channel, and I’m early in the process so it wouldn’t be a big deal if I needed to start over due to channel-mixing woes.\n# (Blog-nbdev)\nconda create -n Blog-nbdev\nconda activate Blog-nbdev\nconda install conda-forge::python=3.9 fastai::nbdev=2.3.9 conda-forge::ipykernel=6.19.2\nipykernel and nb_conda_kernels are used to allow the Jupyter-nbdev JupyterLab to find the Blog-nbdev kernel. From there, I launched JupyterLab:\n# (JupyterLab)\nconda activate Jupyter-nbdev\njupyter lab\n\n\n\nI followed the steps here. In short: create a new public repository with no ReadMe or other file, but with the ‘description’ field filled in. Then, go to the directory that you want to create your repo in in your terminal:\n# (GitHub)\nconda activate Blog-nbdev\ngit clone https://github.com/YOUR_GITHUB_USERNAME/NAME_OF_YOUR_REPO.git\ncd NAME_OF_YOUR_REPO\nnbdev_new\nThe last command tells nbdev to fill out the repository with the template for your blog. I got the following output from nbdev_new:\n# Output of (GitHub)\n/Users/baileyandrew/opt/anaconda3/envs/Blog-nbdev/lib/python3.9/site-packages/ghapi/core.py:101: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\nrepo = Blog # Automatically inferred from git\nbranch = main # Automatically inferred from git\nuser = BaileyAndrew # Automatically inferred from git\nauthor = Bailey Andrew # Automatically inferred from git\nauthor_email = MY_EMAIL@cool-emails.com # Automatically inferred from git\ndescription = A blog. # Automatically inferred from git\nsettings.ini created.\n/bin/sh: quarto: command not found\nThe output looks a little warning-y, but let’s continue for now:\n# (GitHub)\ngit add .\ngit commit -m 'Created Blog'\ngit push\n\n# Output\n! [remote rejected] main -&gt; main (refusing to allow a Personal Access Token to create or update workflow .github/workflows/deploy.yaml without workflow scope)\nThe push command fails; the error message seems related to the warning I got previously. The problem seems to be that I only ever create GitHub tokens with ‘repo’ permissions, not ‘workflow’ permissions, but nbdev uses the workflow feature to prepare the website. This is easy to fix: on GitHub, click on your profile and go to ‘settings’, then ‘developer settings’, ‘personal access tokens’, and finally ‘tokens (classic)’. You can either create a new token, or update an existing one - just make sure to add the ‘workflow’ permissions.\nOnce you’ve pushed, go into settings on GitHub to set the branch to be gh-pages: \nFinally, run the following to allow nbdev to trigger workflows:\n# (GitHub)\nnbdev_install_hooks\nNow, whenever you push to main, a workflow will automatically trigger that will generate the html/css/js for your website and put it in the gh-pages branch. (I do not know if pushing to other branches would trigger this as well).\n\n\n\nWhen you’re ready to start writing blog posts, go into the nbs folder and create a new .ipynb with the kernel Python [conda env:Blog-nbdev]. (If it’s not in the nbs folder, it won’t appear in the blog). There are already two examples, index.ipynb and 00_core.ipynb - index.ipynb probably fills some special role so don’t mess with it, but you can delete 00_core.ipynb.\nImportantly, your first cell should be a markdown cell with just a top-level header:\n# Top-level header\nThe name of the header will be the name of the blog post in the generated website (and the name of the file will be the name of the url to that post). If you have anything else in the first cell, it will probably be cut off. Your blog will likely look something like this initially:\n\n\n\nExample Blog\n\n\nThe last issue I had was getting the images to display, which was overcome by creating an images sub-folder of nbs and storing them there.\nAfter that, it worked like a charm! Running nbdev_preview was quite helpful in debugging the issues, as it previews your website and updates automatically!"
  },
  {
    "objectID": "staticposts.html",
    "href": "staticposts.html",
    "title": "Static Posts",
    "section": "",
    "text": "All the 4x4 Sudoku Puzzles\n\n\n\nFun\n\n\n\n\nBailey Andrew\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Eventually Comprehensive Guide to Single-Cell Biases\n\n\n\nWork\n\n\nOmics\n\n\nUseful\n\n\nIn Progress\n\n\n\n\nBailey Andrew\n\n\nJan 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoulders in Valleys Tips and Tricks\n\n\n\nFun\n\n\n\n\nBailey Andrew\n\n\nJan 7, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "700_Static/scRNA/sc-biases.html",
    "href": "700_Static/scRNA/sc-biases.html",
    "title": "An Eventually Comprehensive Guide to Single-Cell Biases",
    "section": "",
    "text": "This guide is in progress but will hopefully end up being a fully comprehensive guide to all the biases encountered by the different protocols of scRNA-seq data (with perhaps room to increase scope to all sc-omics data).\nBut that’s in the future - for now, this is empty!"
  },
  {
    "objectID": "slideshowindex.html",
    "href": "slideshowindex.html",
    "title": "Slideshows",
    "section": "",
    "text": "Journal Club\n\n\n\nWork\n\n\nUseful\n\n\n\n\nBailey Andrew\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_005.html",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_005.html",
    "title": "Boulders in Valleys 005",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_005.html#puzzle",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_005.html#puzzle",
    "title": "Boulders in Valleys 005",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_005.html#rules",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_005.html#rules",
    "title": "Boulders in Valleys 005",
    "section": "Rules",
    "text": "Rules\n\nCurve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints\n\n\n\nBoulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve.\n\n\n\nMechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_003.html",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_003.html",
    "title": "Boulders in Valleys 003",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_003.html#puzzle",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_003.html#puzzle",
    "title": "Boulders in Valleys 003",
    "section": "",
    "text": "The Puzzle\n\n\nPlay online with Penpa"
  },
  {
    "objectID": "500_Puzzles/Boulders_in_Valleys/bnv_003.html#rules",
    "href": "500_Puzzles/Boulders_in_Valleys/bnv_003.html#rules",
    "title": "Boulders in Valleys 003",
    "section": "Rules:",
    "text": "Rules:\n\nCurve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints\n\n\n\nBoulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve.\n\n\n\nMechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples"
  },
  {
    "objectID": "500_Puzzles/bnv.html",
    "href": "500_Puzzles/bnv.html",
    "title": "Boulders in Valleys",
    "section": "",
    "text": "Standard Rules\n\n\n\n\nBailey Andrew\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Rules\n\n\n\n\nBailey Andrew\n\n\nJan 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Rules\n\n\nDifficult\n\n\n\n\nBailey Andrew\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Rules\n\n\nDifficult\n\n\n\n\nBailey Andrew\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Rules\n\n\nDifficult\n\n\n\n\nBailey Andrew\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "500_Puzzles/bnv.html#curve-rules",
    "href": "500_Puzzles/bnv.html#curve-rules",
    "title": "Boulders in Valleys",
    "section": "Curve Rules",
    "text": "Curve Rules\n\nThe solution is a single, non-self-intersecting curve that partitions the grid into two zones (whether by looping or starting and ending at the grid boundary)\nThe curve travels along grid edges (like Slitherlink) within the grid boundary\nThere is only one curve that fits the constraints"
  },
  {
    "objectID": "500_Puzzles/bnv.html#boulders-in-valleys-clue-rules",
    "href": "500_Puzzles/bnv.html#boulders-in-valleys-clue-rules",
    "title": "Boulders in Valleys",
    "section": "Boulders in Valleys Clue Rules",
    "text": "Boulders in Valleys Clue Rules\n\nThere is a ball at every number. The value of the number indicates the deepest depth the ball could roll down to. It must roll down to this depth along at least one path, but it does not have to roll down to this depth along every path.\nArrows indicate direction of gravity for the ball.\nGrid boundary stops the ball as if it were part of the curve."
  },
  {
    "objectID": "500_Puzzles/bnv.html#mechanics-of-rolling-balls",
    "href": "500_Puzzles/bnv.html#mechanics-of-rolling-balls",
    "title": "Boulders in Valleys",
    "section": "Mechanics of Rolling Balls",
    "text": "Mechanics of Rolling Balls\n\nA ball can roll “down” any corner, and fall straight “down” if there is no edge below it (“down” defined with respect to gravity).\nA ball cannot roll “up” (against gravity) a wall or along two “horizontal” (perpendicular to gravity) segments in a row (the hill would be too shallow!).\nBalls do not interfere with each other (i.e. they can pass through each other, overlap each other, etc)\n\n\n\n\nExamples\n\n\nIf you want to get better at this puzzle type, check out this post on tips and tricks."
  }
]